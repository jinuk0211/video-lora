{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8zV1ErNM1dC"
   },
   "source": [
    "# prompt: drive.mount\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quF7Md5zLGbi",
    "outputId": "4184351a-ccb1-4c78-ed75-efa487eb7d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "[2025-07-25 11:18:51,610] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 11:18:53,792] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install deepspeed pytorch-optimizer torch-optimi -U -q\n",
    "!pip install wandb -q\n",
    "!pip install tensorboard -q\n",
    "!pip install toml -q\n",
    "import deepspeed.comm.comm as dist\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.7.4.post1+cu128torch2.7.0cxx11abiFALSE) (2.1.5)\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.7.4.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.7.0+cu128 torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install ./flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import flash_attn_2_cuda as flash_attn_gpu\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !pip install typing_extensions -U \n",
    "from typing_extensions import LiteralString, TypeAliasType, TypeIs, deprecated\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'transformers>=4.49.0' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate einops timm transformers ftfy easydict diffusers deepspeed datasets imageio loguru opencv-python gradio toml -q\n",
    "from accelerate import init_empty_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import imageio\n",
    "DTYPE_MAP = {'float32': torch.float32, 'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float8': torch.float8_e4m3fn}\n",
    "VIDEO_EXTENSIONS = set(x.extension for x in imageio.config.video_extensions)\n",
    "AUTOCAST_DTYPE = torch.bfloat16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTYPE_MAP = {'float32': torch.float32, 'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float8': torch.float8_e4m3fn}\n",
    "VIDEO_EXTENSIONS = set(x.extension for x in imageio.config.video_extensions)\n",
    "# AUTOCAST_DTYPE\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install ninja\n",
    "# !export MAX_JOBS=4\n",
    "# !git clone https://github.com/Dao-AILab/flash-attention.git\n",
    "# %cd flash-attention\n",
    "# !pip install . \n",
    "#cuda 11.8일시\n",
    "# !pip install flash-attn --no-build-isolation #--prefer-binary \n",
    "\n",
    "# !pip install flash-attn==2.5.5 --no-build-isolation #--prefer-binary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0iHaDdEN3N9",
    "outputId": "ef652391-941a-48ea-a91a-1773f7d3fe20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Wan2.1'...\n",
      "remote: Enumerating objects: 368, done.\u001b[K\n",
      "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 368 (delta 0), reused 0 (delta 0), pack-reused 366 (from 2)\u001b[K\n",
      "Receiving objects: 100% (368/368), 10.69 MiB | 10.31 MiB/s, done.\n",
      "Resolving deltas: 100% (203/203), done.\n",
      "Updating files: 100% (59/59), done.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Wan-Video/Wan2.1.git\n",
    "import sys\n",
    "# sys.path.insert(0, '/workspace/Wan2.1')\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSfj1C4hRpIi",
    "outputId": "dd6960e4-4f8f-4795-fc0e-63fc1d6c6d1f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wan'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msafetensors\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwan\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanModel\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_module_tensor_to_device\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wan'"
     ]
    }
   ],
   "source": [
    "import safetensors\n",
    "from pathlib import Path\n",
    "from wan.modules.model import WanModel\n",
    "import torch\n",
    "from accelerate.utils import set_module_tensor_to_device\n",
    "\n",
    "KEEP_IN_HIGH_PRECISION = ['norm', 'bias', 'patch_embedding', 'text_embedding', 'time_embedding', 'time_projection', 'head', 'modulation']\n",
    "dtype = torch.bfloat16\n",
    "transformer_dtype=torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aobLNxL9HyIi",
    "outputId": "8b7d5157-b4bc-464f-fc34-5d285c0992d8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (1.1.5)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.50)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.1.31)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: pfzy, InquirerPy\n",
      "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 22 files:   0%|                                 | 0/22 [00:00<?, ?it/s]Downloading '.gitattributes' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.0a1f66aea84e17cbf6a60c51431723062f87df8a.incomplete'\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "\n",
      ".gitattributes: 2.23kB [00:00, 2.22MB/s]\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/.gitattributes\n",
      "Fetching 22 files:   5%|█▏                       | 1/22 [00:00<00:03,  5.41it/s]Downloading 'LICENSE.txt' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/cyBuwAu93UXke23CJCWORBYR70A=.261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64.incomplete'\n",
      "Downloading 'assets/comp_effic.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/X4-4NQEdffK1bUQbDx0NJ4IbcNo=.b0e225caffb4b31295ad150f95ee852e4c3dde4a00ac8f79a2ff500f2ce26b8d.incomplete'\n",
      "Downloading 'assets/data_for_diff_stage.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/UCVN8O248kjCIp8i1TvUXWfm-q4=.59aec08409f2d46b0e640e4e120dc7cca52c08c3de56d026602dbcff1ebf241a.incomplete'\n",
      "Downloading 'assets/i2v_res.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/GL7zbyOeRDu8VADezw2p_Gw3fW4=.6823b3206d8d0cb18d3b5b949dec1217f1178109ba11f14e977b67e1f7b8a248.incomplete'\n",
      "\n",
      "LICENSE.txt: 11.4kB [00:00, 16.2MB/s]\n",
      "Downloading 'assets/logo.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/gvouIpVGo_vjLxWDOGAmKk_SZvY=.96cddc0f667293436d0b9f92a299b6346b65b231d38ee49719a33d46c91fe1e3.incomplete'\n",
      "Downloading 'README.md' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.fbbf7c13900d37f3179c546d09372aeb52e0fae1.incomplete'\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/LICENSE.txt\n",
      "Downloading 'Wan2.1_VAE.pth' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/OZTShqo6Di7rh0SEswnfwSjKa9w=.38071ab59bd94681c686fa51d75a1968f64e470262043be31f7a094e442fd981.incomplete'\n",
      "\n",
      "comp_effic.png:   0%|                               | 0.00/1.79M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "data_for_diff_stage.jpg:   0%|                       | 0.00/528k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "i2v_res.png:   0%|                                   | 0.00/892k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'assets/.DS_Store' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/aHCrtOA_94YSUtnsyba37B_tPq0=.d65165279105ca6773180500688df4bdc69a2c7b771752f0a46ef120b7fd8ec3.incomplete'\n",
      "data_for_diff_stage.jpg: 100%|███████████████| 528k/528k [00:00<00:00, 80.6MB/s]\n",
      "\n",
      "\n",
      "README.md: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "README.md: 16.9kB [00:00, 9.74MB/s]                 | 0.00/56.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "logo.png: 100%|████████████████████████████| 56.3k/56.3k [00:00<00:00, 34.2MB/s]\n",
      "\n",
      "\n",
      "i2v_res.png: 100%|███████████████████████████| 892k/892k [00:00<00:00, 31.4MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/data_for_diff_stage.jpg\n",
      "\n",
      "\n",
      "\n",
      ".DS_Store: 100%|███████████████████████████| 6.15k/6.15k [00:00<00:00, 19.9MB/s]Download complete. Moving file to Wan2.1-T2V-1.3B/README.md\n",
      "\n",
      "comp_effic.png: 100%|██████████████████████| 1.79M/1.79M [00:00<00:00, 43.2MB/s]\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/logo.png\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/i2v_res.png\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/comp_effic.png\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/.DS_Store\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth:   2%|▍                      | 10.5M/508M [00:00<00:05, 95.8MB/s]\u001b[A\u001b[ADownloading 'assets/t2v_res.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/VwwCFkBXvnWpmAWpzHvOZFYnnp8=.91db579092446be2a834bc67721a8e4346936f38c4edb912f459ca3e10f8f439.incomplete'\n",
      "Downloading 'assets/vben_1.3b_vs_sota.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/yhawob5KxOUy3bpsGU-xOWB6FC8=.b7705db79f2e1428ec7a1e6fff8c4fbde062fb95bb233516ddbd04b20007c845.incomplete'\n",
      "\n",
      "t2v_res.jpg: 100%|███████████████████████████| 301k/301k [00:00<00:00, 26.1MB/s]\u001b[A\n",
      "Downloading 'assets/vben_vs_sota.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/p0uIb0aMIBdDB8sg9tk5UvunoB8=.9a0e86ca85046d2675f97984b88b6e74df07bba8a62a31ab8a1aef50d4eda44e.incomplete'\n",
      "\n",
      "vben_1.3b_vs_sota.png:   0%|                         | 0.00/516k [00:00<?, ?B/s]\u001b[ADownloading 'assets/video_dit_arch.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/PbT6aGUrWxKRPnz45UQ5EbqxKYE=.195dceec6570289d8b01cc51d2e28a7786216f19de55b23978a52610d1646a66.incomplete'\n",
      "Downloading 'diffusion_pytorch_model.safetensors' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/4SfAgk9U607e8pVunEB9nSiU10k=.96b6b242ca1c2f24e9d02cd6596066fab6d310e2d7538f33ae267cb18d957e8f.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "vben_vs_sota.png:   0%|                             | 0.00/1.55M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'assets/video_vae_res.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/_r133FrlYqQbngOMpQWXWYhhvVM=.d8f9e7f7353848056a615c8ef35ab86ec22976bb46cb27405008b4089701945c.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/t2v_res.jpg\n",
      "video_dit_arch.jpg:   0%|                            | 0.00/643k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "video_vae_res.jpg: 100%|█████████████████████| 213k/213k [00:00<00:00, 18.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "video_dit_arch.jpg: 100%|████████████████████| 643k/643k [00:00<00:00, 24.7MB/s]\n",
      "vben_1.3b_vs_sota.png: 100%|█████████████████| 516k/516k [00:00<00:00, 12.1MB/s]\n",
      "vben_vs_sota.png: 100%|████████████████████| 1.55M/1.55M [00:00<00:00, 50.2MB/s]\n",
      "Downloading 'config.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.d203bef600b2f3c64fe1f5f53d70a2087f4ccd2f.incomplete'\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/vben_1.3b_vs_sota.png\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/video_vae_res.jpg\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/video_dit_arch.jpg\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/vben_vs_sota.png\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 249/249 [00:00<00:00, 1.07MB/s]\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/config.json\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth:   4%|▉                      | 21.0M/508M [00:00<00:07, 68.1MB/s]\u001b[A\u001b[ADownloading 'examples/i2v_input.JPG' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/examples/GqO1vCXUoNdTZc6wjxo1L7sjRdI=.077e3d965090c9028c69c00931675f42e1acc815c6eb450ab291b3b72d211a8e.incomplete'\n",
      "Downloading 'google/umt5-xxl/spiece.model' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/vj8E1loknrCNPSP8nWJC234Bff4=.e3909a67b780650b35cf529ac782ad2b6b26e6d1f849d3fbb6a872905f452458.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%| | 10.5M/5.68G [00:00<01:22, 68.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "i2v_input.JPG:   0%|                                 | 0.00/251k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading 'google/umt5-xxl/tokenizer.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.6e197b4d3dbd71da14b4eb255f4fa91c9c1f2068b20a2de2472967ca3d22602b.incomplete'\n",
      "spiece.model:   0%|                                 | 0.00/4.55M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'google/umt5-xxl/special_tokens_map.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/ahkChHUJFxEmOdq5GDFEmerRzCY=.14855e7052ffbb595057dfd791d293c1c940db2c.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i2v_input.JPG: 100%|█████████████████████████| 251k/251k [00:00<00:00, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading 'google/umt5-xxl/tokenizer_config.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/vzaExXFZNBay89bvlQv-ZcI6BTg=.4e1cc1cd85599ce0b47fd0a746af188fe4043ff2.incomplete'\n",
      "Downloading 'models_t5_umt5-xxl-enc-bf16.pth' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/7xjTyx9p9-CteGiI3VEINu_Ohx0=.7cace0da2b446bbbbc57d031ab6cf163a3d59b366da94e5afe36745b746fd81d.incomplete'\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/examples/i2v_input.JPG\n",
      "\n",
      "special_tokens_map.json: 6.62kB [00:00, 4.73MB/s]\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|              | 0.00/11.4G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 61.7kB [00:00, 67.6MB/s]A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/special_tokens_map.json\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/tokenizer_config.json\n",
      "spiece.model: 100%|████████████████████████| 4.55M/4.55M [00:00<00:00, 63.5MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%| | 21.0M/5.68G [00:00<01:12, 78.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownload complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/spiece.model\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth:   6%|█▍                     | 31.5M/508M [00:00<00:07, 62.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json:  62%|█████████████▋        | 10.5M/16.8M [00:00<00:00, 70.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizer.json: 100%|██████████████████████| 16.8M/16.8M [00:00<00:00, 67.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%|  | 41.9M/5.68G [00:00<00:54, 103MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownload complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/tokenizer.json\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth:  10%|██▍                    | 52.4M/508M [00:00<00:05, 82.9MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|      | 31.5M/11.4G [00:00<01:47, 105MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%|  | 62.9M/5.68G [00:00<00:44, 125MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  14%|███▍                    | 73.4M/508M [00:00<00:03, 111MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|      | 52.4M/11.4G [00:00<01:22, 137MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%|  | 83.9M/5.68G [00:00<00:38, 147MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  19%|████▍                   | 94.4M/508M [00:00<00:03, 129MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 73.4M/11.4G [00:00<01:16, 148MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%|   | 105M/5.68G [00:00<00:34, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  23%|█████▋                   | 115M/508M [00:01<00:02, 145MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 94.4M/11.4G [00:00<01:10, 159MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%|   | 126M/5.68G [00:00<00:32, 170MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  27%|██████▋                  | 136M/508M [00:01<00:02, 157MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|       | 115M/11.4G [00:00<01:05, 171MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|   | 147M/5.68G [00:01<00:32, 173MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  31%|███████▋                 | 157M/508M [00:01<00:02, 160MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|       | 136M/11.4G [00:00<01:04, 174MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|   | 168M/5.68G [00:01<00:31, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  35%|████████▊                | 178M/508M [00:01<00:01, 167MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|       | 157M/11.4G [00:01<01:04, 173MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|   | 189M/5.68G [00:01<00:30, 178MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  39%|█████████▊               | 199M/508M [00:01<00:01, 173MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|       | 178M/11.4G [00:01<01:02, 180MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|   | 210M/5.68G [00:01<00:31, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  43%|██████████▊              | 220M/508M [00:01<00:01, 174MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|       | 199M/11.4G [00:01<01:02, 180MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|   | 231M/5.68G [00:01<00:33, 163MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  48%|███████████▉             | 241M/508M [00:01<00:01, 155MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏      | 220M/11.4G [00:01<01:13, 152MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|▏  | 252M/5.68G [00:01<00:34, 157MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  52%|████████████▉            | 262M/508M [00:01<00:01, 162MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏      | 241M/11.4G [00:01<01:10, 157MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|▏  | 273M/5.68G [00:01<00:33, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  56%|█████████████▉           | 283M/508M [00:02<00:01, 165MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏      | 262M/11.4G [00:01<01:10, 158MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|▏  | 294M/5.68G [00:01<00:33, 163MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  60%|██████████████▉          | 304M/508M [00:02<00:01, 169MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏      | 283M/11.4G [00:01<01:08, 162MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|▏  | 315M/5.68G [00:02<00:31, 169MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  64%|████████████████         | 325M/508M [00:02<00:01, 170MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|▏  | 336M/5.68G [00:02<00:30, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏      | 304M/11.4G [00:01<01:10, 156MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  68%|█████████████████        | 346M/508M [00:02<00:00, 170MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|▏  | 357M/5.68G [00:02<00:29, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏      | 325M/11.4G [00:02<01:10, 156MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  72%|██████████████████       | 367M/508M [00:02<00:00, 173MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏  | 377M/5.68G [00:02<00:29, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏      | 346M/11.4G [00:02<01:07, 162MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  76%|███████████████████      | 388M/508M [00:02<00:00, 170MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏  | 398M/5.68G [00:02<00:29, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏      | 367M/11.4G [00:02<01:09, 158MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  81%|████████████████████▏    | 409M/508M [00:02<00:00, 158MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏  | 419M/5.68G [00:02<00:28, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏      | 388M/11.4G [00:02<01:12, 152MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  85%|█████████████████████▏   | 430M/508M [00:02<00:00, 161MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏  | 440M/5.68G [00:02<00:29, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎      | 409M/11.4G [00:02<01:12, 151MB/s]\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  89%|██████████████████████▏  | 451M/508M [00:03<00:00, 158MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏  | 472M/5.68G [00:02<00:26, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▎  | 493M/5.68G [00:02<00:27, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  93%|███████████████████████▏ | 472M/508M [00:03<00:00, 155MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎      | 440M/11.4G [00:02<01:05, 166MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▎  | 514M/5.68G [00:03<00:27, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Wan2.1_VAE.pth:  97%|████████████████████████▎| 493M/508M [00:03<00:00, 164MB/s]\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎      | 461M/11.4G [00:02<01:06, 165MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth: 100%|█████████████████████████| 508M/508M [00:03<00:00, 149MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎      | 482M/11.4G [00:03<01:02, 174MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▎  | 556M/5.68G [00:03<00:26, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownload complete. Moving file to Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n",
      "Fetching 22 files:  18%|████▌                    | 4/22 [00:03<00:20,  1.16s/it]\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎      | 514M/11.4G [00:03<00:58, 186MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▎  | 577M/5.68G [00:03<00:27, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎      | 535M/11.4G [00:03<00:57, 188MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▎  | 598M/5.68G [00:03<00:27, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎      | 556M/11.4G [00:03<00:56, 192MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▎  | 619M/5.68G [00:03<00:26, 192MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎      | 577M/11.4G [00:03<00:54, 196MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▎  | 650M/5.68G [00:03<00:24, 204MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎      | 598M/11.4G [00:03<00:55, 195MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▎  | 671M/5.68G [00:03<00:24, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▍      | 619M/11.4G [00:03<00:59, 181MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▎  | 692M/5.68G [00:04<00:26, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍      | 640M/11.4G [00:03<00:57, 188MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▍  | 724M/5.68G [00:04<00:26, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍      | 671M/11.4G [00:03<00:54, 197MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▍  | 744M/5.68G [00:04<00:25, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍      | 692M/11.4G [00:04<00:53, 199MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▍  | 765M/5.68G [00:04<00:25, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍      | 713M/11.4G [00:04<00:52, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▍  | 797M/5.68G [00:04<00:24, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍      | 744M/11.4G [00:04<00:51, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍      | 765M/11.4G [00:04<00:54, 195MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▍  | 828M/5.68G [00:04<00:23, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▍  | 849M/5.68G [00:04<00:23, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍      | 786M/11.4G [00:04<00:56, 188MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍      | 807M/11.4G [00:04<00:55, 191MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▍  | 881M/5.68G [00:04<00:22, 215MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▌      | 839M/11.4G [00:04<00:53, 196MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▍  | 912M/5.68G [00:05<00:23, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌      | 870M/11.4G [00:04<00:50, 207MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▍  | 944M/5.68G [00:05<00:21, 217MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌      | 902M/11.4G [00:05<00:49, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▌  | 975M/5.68G [00:05<00:22, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌      | 923M/11.4G [00:05<00:50, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▎ | 1.01G/5.68G [00:05<00:22, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌      | 944M/11.4G [00:05<00:51, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▎ | 1.04G/5.68G [00:05<00:21, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌      | 975M/11.4G [00:05<00:50, 205MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▍ | 1.07G/5.68G [00:05<00:22, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 1.01G/11.4G [00:05<00:48, 211MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▍ | 1.10G/5.68G [00:05<00:22, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 1.04G/11.4G [00:05<00:51, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 1.06G/11.4G [00:05<00:52, 196MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▍ | 1.13G/5.68G [00:06<00:21, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌     | 1.09G/11.4G [00:06<00:50, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▍ | 1.16G/5.68G [00:06<00:21, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌     | 1.11G/11.4G [00:06<00:51, 199MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▍ | 1.20G/5.68G [00:06<00:21, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌     | 1.13G/11.4G [00:06<00:53, 193MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▍ | 1.22G/5.68G [00:06<00:22, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌     | 1.16G/11.4G [00:06<00:48, 209MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▍ | 1.24G/5.68G [00:06<00:22, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▋     | 1.18G/11.4G [00:06<00:49, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▍ | 1.26G/5.68G [00:06<00:23, 189MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▋     | 1.21G/11.4G [00:06<00:52, 192MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▍ | 1.29G/5.68G [00:06<00:21, 204MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▋     | 1.23G/11.4G [00:06<00:54, 186MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▍ | 1.31G/5.68G [00:07<00:21, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▋     | 1.25G/11.4G [00:06<00:53, 189MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▍ | 1.34G/5.68G [00:07<00:20, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▋     | 1.28G/11.4G [00:07<00:49, 204MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▋     | 1.30G/11.4G [00:07<00:50, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▍ | 1.37G/5.68G [00:07<00:21, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▋     | 1.33G/11.4G [00:07<00:48, 208MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▍ | 1.41G/5.68G [00:07<00:20, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▋     | 1.35G/11.4G [00:07<00:49, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▌ | 1.44G/5.68G [00:07<00:19, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▋     | 1.37G/11.4G [00:07<00:49, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▌ | 1.47G/5.68G [00:07<00:20, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▋     | 1.39G/11.4G [00:07<00:48, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▌ | 1.49G/5.68G [00:07<00:20, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▋     | 1.42G/11.4G [00:07<00:49, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▊     | 1.44G/11.4G [00:07<00:48, 203MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▌ | 1.52G/5.68G [00:08<00:19, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▊     | 1.47G/11.4G [00:07<00:46, 212MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▌ | 1.55G/5.68G [00:08<00:19, 210MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▊     | 1.50G/11.4G [00:08<00:46, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▌ | 1.58G/5.68G [00:08<00:19, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▊     | 1.52G/11.4G [00:08<00:47, 209MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▌ | 1.61G/5.68G [00:08<00:19, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▊     | 1.55G/11.4G [00:08<00:46, 209MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▌ | 1.64G/5.68G [00:08<00:19, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▊     | 1.58G/11.4G [00:08<00:46, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▌ | 1.67G/5.68G [00:08<00:18, 214MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▊     | 1.61G/11.4G [00:08<00:45, 215MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▌ | 1.70G/5.68G [00:08<00:19, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▌ | 1.72G/5.68G [00:08<00:19, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▊     | 1.65G/11.4G [00:08<00:48, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▌ | 1.74G/5.68G [00:09<00:20, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▉     | 1.67G/11.4G [00:08<00:49, 195MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▌ | 1.76G/5.68G [00:09<00:20, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▉     | 1.69G/11.4G [00:09<00:50, 193MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▋ | 1.79G/5.68G [00:09<00:19, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▉     | 1.72G/11.4G [00:09<00:48, 198MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▉     | 1.74G/11.4G [00:09<00:48, 199MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▋ | 1.81G/5.68G [00:09<00:19, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▉     | 1.76G/11.4G [00:09<00:48, 200MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▋ | 1.84G/5.68G [00:09<00:19, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▋ | 1.86G/5.68G [00:09<00:19, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▉     | 1.79G/11.4G [00:09<00:45, 209MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▋ | 1.88G/5.68G [00:09<00:19, 197MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▉     | 1.81G/11.4G [00:09<00:46, 207MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▋ | 1.90G/5.68G [00:09<00:19, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▉     | 1.85G/11.4G [00:09<00:45, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▋ | 1.92G/5.68G [00:10<00:19, 192MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▉     | 1.87G/11.4G [00:09<00:46, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▋ | 1.94G/5.68G [00:10<00:19, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▉     | 1.89G/11.4G [00:09<00:46, 205MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▋ | 1.97G/5.68G [00:10<00:18, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|█     | 1.92G/11.4G [00:10<00:45, 208MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▋ | 1.99G/5.68G [00:10<00:17, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|█     | 1.94G/11.4G [00:10<00:45, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▋ | 2.02G/5.68G [00:10<00:17, 210MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|█     | 1.96G/11.4G [00:10<00:45, 207MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▋ | 2.04G/5.68G [00:10<00:17, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|█     | 1.98G/11.4G [00:10<00:45, 207MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▋ | 2.08G/5.68G [00:10<00:17, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|█     | 2.01G/11.4G [00:10<00:44, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▋ | 2.10G/5.68G [00:10<00:17, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|█     | 2.04G/11.4G [00:10<00:43, 212MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▊ | 2.13G/5.68G [00:11<00:16, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|█     | 2.08G/11.4G [00:10<00:43, 213MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▊ | 2.16G/5.68G [00:11<00:16, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▊ | 2.18G/5.68G [00:11<00:16, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|█     | 2.11G/11.4G [00:11<00:48, 189MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|█     | 2.13G/11.4G [00:11<00:48, 190MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▊ | 2.21G/5.68G [00:11<00:16, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|█▏    | 2.15G/11.4G [00:11<00:48, 189MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▊ | 2.24G/5.68G [00:11<00:16, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|█▏    | 2.17G/11.4G [00:11<00:49, 185MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▊ | 2.26G/5.68G [00:11<00:16, 203MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|█▏    | 2.19G/11.4G [00:11<00:48, 189MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▊ | 2.29G/5.68G [00:11<00:16, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█▏    | 2.22G/11.4G [00:11<00:44, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▊ | 2.32G/5.68G [00:11<00:16, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█▏    | 2.24G/11.4G [00:11<00:45, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▊ | 2.34G/5.68G [00:12<00:16, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█▏    | 2.28G/11.4G [00:11<00:42, 215MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▊ | 2.37G/5.68G [00:12<00:15, 215MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█▏    | 2.31G/11.4G [00:12<00:44, 205MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▊ | 2.40G/5.68G [00:12<00:15, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█▏    | 2.33G/11.4G [00:12<00:44, 203MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▊ | 2.43G/5.68G [00:12<00:15, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█▏    | 2.35G/11.4G [00:12<00:44, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█▎    | 2.37G/11.4G [00:12<00:44, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▊ | 2.46G/5.68G [00:12<00:14, 216MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█▎    | 2.40G/11.4G [00:12<00:42, 209MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▉ | 2.50G/5.68G [00:12<00:15, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▉ | 2.53G/5.68G [00:12<00:14, 215MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█▎    | 2.42G/11.4G [00:12<01:05, 137MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▉ | 2.56G/5.68G [00:13<00:15, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.44G/11.4G [00:12<01:02, 144MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▉ | 2.59G/5.68G [00:13<00:14, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.46G/11.4G [00:13<01:00, 148MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▉ | 2.61G/5.68G [00:13<00:15, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.49G/11.4G [00:13<00:57, 155MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▉ | 2.64G/5.68G [00:13<00:14, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.51G/11.4G [00:13<00:57, 155MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▉ | 2.66G/5.68G [00:13<00:14, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.53G/11.4G [00:13<00:56, 156MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▉ | 2.69G/5.68G [00:13<00:14, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█▎    | 2.55G/11.4G [00:13<00:54, 161MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▉ | 2.72G/5.68G [00:13<00:14, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▎    | 2.57G/11.4G [00:13<00:52, 169MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▉ | 2.75G/5.68G [00:13<00:13, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▎    | 2.59G/11.4G [00:13<00:50, 173MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▍    | 2.61G/11.4G [00:13<00:48, 181MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▉ | 2.78G/5.68G [00:14<00:13, 210MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▍    | 2.64G/11.4G [00:14<00:45, 192MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▉ | 2.81G/5.68G [00:14<00:13, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▍    | 2.66G/11.4G [00:14<00:45, 192MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▉ | 2.83G/5.68G [00:14<00:13, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▍    | 2.68G/11.4G [00:14<00:44, 196MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|█ | 2.85G/5.68G [00:14<00:13, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▍    | 2.71G/11.4G [00:14<00:44, 193MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|█ | 2.87G/5.68G [00:14<00:13, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|█ | 2.89G/5.68G [00:14<00:14, 189MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▍    | 2.74G/11.4G [00:14<00:42, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|█ | 2.92G/5.68G [00:14<00:14, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▍    | 2.76G/11.4G [00:14<00:42, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|█ | 2.94G/5.68G [00:14<00:14, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▍    | 2.78G/11.4G [00:14<00:44, 194MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▍    | 2.80G/11.4G [00:14<00:44, 191MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|█ | 2.96G/5.68G [00:15<00:14, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▍    | 2.82G/11.4G [00:14<00:44, 193MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|█ | 2.98G/5.68G [00:15<00:14, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|█ | 3.00G/5.68G [00:15<00:14, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▌    | 2.85G/11.4G [00:15<00:42, 200MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|█ | 3.02G/5.68G [00:15<00:13, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▌    | 2.88G/11.4G [00:15<00:40, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▌    | 2.90G/11.4G [00:15<00:40, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▌    | 2.94G/11.4G [00:15<00:39, 213MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|█ | 3.05G/5.68G [00:15<00:19, 136MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|█ | 3.07G/5.68G [00:15<00:17, 148MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▌    | 2.97G/11.4G [00:15<00:42, 200MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|█ | 3.09G/5.68G [00:15<00:17, 151MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▌    | 2.99G/11.4G [00:15<00:42, 197MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|█ | 3.12G/5.68G [00:16<00:15, 170MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▌    | 3.01G/11.4G [00:15<00:44, 188MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▌    | 3.03G/11.4G [00:16<00:43, 191MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|█ | 3.16G/5.68G [00:16<00:13, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▌    | 3.05G/11.4G [00:16<00:42, 194MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|█ | 3.19G/5.68G [00:16<00:13, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▌    | 3.07G/11.4G [00:16<00:41, 198MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▋    | 3.09G/11.4G [00:16<00:41, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|█▏| 3.22G/5.68G [00:16<00:12, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|█▏| 3.24G/5.68G [00:16<00:12, 198MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▋    | 3.12G/11.4G [00:16<00:40, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▋    | 3.15G/11.4G [00:16<00:40, 205MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|█▏| 3.27G/5.68G [00:16<00:11, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|█▏| 3.29G/5.68G [00:16<00:11, 204MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▋    | 3.18G/11.4G [00:16<00:39, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▋    | 3.21G/11.4G [00:16<00:38, 212MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|█▏| 3.32G/5.68G [00:17<00:11, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▋    | 3.24G/11.4G [00:16<00:36, 223MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|█▏| 3.36G/5.68G [00:17<00:11, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|█▏| 3.38G/5.68G [00:17<00:11, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▋    | 3.27G/11.4G [00:17<00:39, 205MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|█▏| 3.40G/5.68G [00:17<00:11, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▋    | 3.30G/11.4G [00:17<00:39, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|█▏| 3.42G/5.68G [00:17<00:11, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▊    | 3.32G/11.4G [00:17<00:39, 203MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|█▏| 3.44G/5.68G [00:17<00:11, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▊    | 3.34G/11.4G [00:17<00:40, 196MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|█▏| 3.46G/5.68G [00:17<00:11, 198MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|█▏| 3.48G/5.68G [00:17<00:11, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▊    | 3.37G/11.4G [00:17<00:42, 190MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|█▏| 3.50G/5.68G [00:17<00:11, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▊    | 3.39G/11.4G [00:17<00:42, 188MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▊    | 3.41G/11.4G [00:17<00:43, 185MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|█▏| 3.53G/5.68G [00:18<00:10, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▊    | 3.43G/11.4G [00:18<00:43, 183MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|█▎| 3.55G/5.68G [00:18<00:10, 203MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▊    | 3.45G/11.4G [00:18<00:42, 188MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|█▎| 3.58G/5.68G [00:18<00:10, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|█▎| 3.60G/5.68G [00:18<00:10, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▊    | 3.48G/11.4G [00:18<00:39, 199MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▊    | 3.50G/11.4G [00:18<00:39, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|█▎| 3.63G/5.68G [00:18<00:09, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▊    | 3.52G/11.4G [00:18<00:38, 203MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|█▎| 3.66G/5.68G [00:18<00:08, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▉    | 3.55G/11.4G [00:18<00:36, 214MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|█▎| 3.69G/5.68G [00:18<00:08, 242MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|█▎| 3.72G/5.68G [00:18<00:08, 243MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▉    | 3.59G/11.4G [00:18<00:36, 210MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|█▎| 3.75G/5.68G [00:19<00:07, 245MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▉    | 3.62G/11.4G [00:18<00:38, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|█▎| 3.79G/5.68G [00:19<00:08, 225MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▉    | 3.64G/11.4G [00:19<00:40, 193MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▉    | 3.66G/11.4G [00:19<00:41, 185MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|█▎| 3.82G/5.68G [00:19<00:08, 218MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▉    | 3.68G/11.4G [00:19<00:41, 187MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|█▎| 3.85G/5.68G [00:19<00:09, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▉    | 3.70G/11.4G [00:19<00:42, 178MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|█▎| 3.87G/5.68G [00:19<00:09, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▉    | 3.72G/11.4G [00:19<00:41, 182MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|█▎| 3.89G/5.68G [00:19<00:09, 191MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▉    | 3.74G/11.4G [00:19<00:42, 179MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|█▍| 3.91G/5.68G [00:19<00:09, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▉    | 3.76G/11.4G [00:19<00:43, 176MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|█▍| 3.93G/5.68G [00:20<00:09, 189MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▉    | 3.79G/11.4G [00:19<00:43, 174MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|█▍| 3.95G/5.68G [00:20<00:09, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.81G/11.4G [00:20<00:45, 166MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|█▍| 3.97G/5.68G [00:20<00:09, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|█▍| 4.00G/5.68G [00:20<00:09, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.83G/11.4G [00:20<00:47, 159MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|█▍| 4.02G/5.68G [00:20<00:09, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.85G/11.4G [00:20<00:48, 156MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|█▍| 4.04G/5.68G [00:20<00:08, 184MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.87G/11.4G [00:20<00:47, 157MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|█▍| 4.06G/5.68G [00:20<00:08, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.89G/11.4G [00:20<00:46, 162MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|█▍| 4.08G/5.68G [00:20<00:08, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|██    | 3.91G/11.4G [00:20<00:43, 170MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|█▍| 4.11G/5.68G [00:20<00:07, 204MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|██    | 3.94G/11.4G [00:20<00:39, 187MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|█▍| 4.13G/5.68G [00:21<00:07, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|██    | 3.96G/11.4G [00:20<00:38, 192MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|█▍| 4.16G/5.68G [00:21<00:07, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|██    | 4.00G/11.4G [00:21<00:36, 201MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|█▍| 4.18G/5.68G [00:21<00:07, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|██▏   | 4.03G/11.4G [00:21<00:35, 206MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|█▍| 4.22G/5.68G [00:21<00:07, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|█▍| 4.24G/5.68G [00:21<00:06, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|██▏   | 4.06G/11.4G [00:21<00:35, 208MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|█▌| 4.26G/5.68G [00:21<00:06, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|██▏   | 4.08G/11.4G [00:21<00:35, 207MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|█▌| 4.28G/5.68G [00:21<00:06, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|██▏   | 4.10G/11.4G [00:21<00:35, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|█▌| 4.30G/5.68G [00:21<00:06, 207MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|██▏   | 4.12G/11.4G [00:21<00:35, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|██▏   | 4.14G/11.4G [00:21<00:35, 203MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|█▌| 4.33G/5.68G [00:22<00:06, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|██▏   | 4.16G/11.4G [00:21<00:38, 189MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|█▌| 4.36G/5.68G [00:22<00:06, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|██▏   | 4.18G/11.4G [00:22<00:38, 188MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|█▌| 4.38G/5.68G [00:22<00:06, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|██▏   | 4.20G/11.4G [00:22<00:39, 183MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|█▌| 4.40G/5.68G [00:22<00:06, 189MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|██▏   | 4.23G/11.4G [00:22<00:39, 179MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|█▌| 4.42G/5.68G [00:22<00:06, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|██▏   | 4.25G/11.4G [00:22<00:40, 175MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|█▌| 4.45G/5.68G [00:22<00:06, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.27G/11.4G [00:22<00:42, 168MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|█▌| 4.47G/5.68G [00:22<00:06, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.29G/11.4G [00:22<00:42, 167MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|█▌| 4.49G/5.68G [00:22<00:06, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.31G/11.4G [00:22<00:41, 170MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|█▌| 4.51G/5.68G [00:23<00:06, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.33G/11.4G [00:22<00:43, 163MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|█▌| 4.53G/5.68G [00:23<00:06, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|█▌| 4.55G/5.68G [00:23<00:06, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.35G/11.4G [00:23<00:45, 155MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|█▌| 4.57G/5.68G [00:23<00:06, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|██▎   | 4.37G/11.4G [00:23<00:44, 158MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|█▌| 4.59G/5.68G [00:23<00:05, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|██▎   | 4.39G/11.4G [00:23<00:46, 149MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|█▋| 4.61G/5.68G [00:23<00:05, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|██▎   | 4.41G/11.4G [00:23<00:45, 153MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|█▋| 4.63G/5.68G [00:23<00:05, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|█▋| 4.66G/5.68G [00:23<00:05, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|██▎   | 4.44G/11.4G [00:23<00:46, 150MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|█▋| 4.68G/5.68G [00:23<00:05, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|██▎   | 4.46G/11.4G [00:23<00:46, 150MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|█▋| 4.70G/5.68G [00:24<00:05, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|██▎   | 4.48G/11.4G [00:23<00:44, 156MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|█▋| 4.72G/5.68G [00:24<00:05, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██▍   | 4.50G/11.4G [00:24<00:44, 156MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|█▋| 4.74G/5.68G [00:24<00:05, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██▍   | 4.52G/11.4G [00:24<00:43, 159MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|█▋| 4.76G/5.68G [00:24<00:05, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██▍   | 4.54G/11.4G [00:24<00:41, 165MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|█▋| 4.78G/5.68G [00:24<00:05, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██▍   | 4.56G/11.4G [00:24<00:40, 169MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|█▋| 4.80G/5.68G [00:24<00:04, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██▍   | 4.58G/11.4G [00:24<00:40, 166MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|█▋| 4.82G/5.68G [00:24<00:04, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.60G/11.4G [00:24<00:40, 168MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|█▋| 4.84G/5.68G [00:24<00:04, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.62G/11.4G [00:24<00:39, 171MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|█▋| 4.87G/5.68G [00:24<00:04, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.65G/11.4G [00:24<00:38, 175MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|█▋| 4.89G/5.68G [00:25<00:04, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.67G/11.4G [00:24<00:36, 183MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|█▋| 4.91G/5.68G [00:25<00:04, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|█▋| 4.93G/5.68G [00:25<00:04, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|█▋| 4.95G/5.68G [00:25<00:03, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.69G/11.4G [00:25<00:55, 121MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|█▊| 4.97G/5.68G [00:25<00:03, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██▍   | 4.71G/11.4G [00:25<00:52, 128MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|█▊| 4.99G/5.68G [00:25<00:03, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|█▊| 5.01G/5.68G [00:25<00:03, 186MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██▍   | 4.73G/11.4G [00:25<00:50, 132MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|█▊| 5.03G/5.68G [00:25<00:03, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██▌   | 4.75G/11.4G [00:25<00:48, 135MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|█▊| 5.05G/5.68G [00:25<00:03, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██▌   | 4.77G/11.4G [00:25<00:48, 137MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|█▊| 5.08G/5.68G [00:26<00:03, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██▌   | 4.79G/11.4G [00:25<00:45, 144MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|█▊| 5.10G/5.68G [00:26<00:03, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|█▊| 5.12G/5.68G [00:26<00:03, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██▌   | 4.81G/11.4G [00:26<00:48, 136MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|█▊| 5.14G/5.68G [00:26<00:02, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.83G/11.4G [00:26<00:44, 145MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|█▊| 5.16G/5.68G [00:26<00:02, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.85G/11.4G [00:26<00:44, 145MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|█▊| 5.18G/5.68G [00:26<00:02, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.88G/11.4G [00:26<00:44, 146MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|█▊| 5.20G/5.68G [00:26<00:02, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.90G/11.4G [00:26<00:44, 145MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|█▊| 5.22G/5.68G [00:26<00:02, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|█▊| 5.24G/5.68G [00:27<00:02, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.92G/11.4G [00:26<00:44, 144MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|█▊| 5.26G/5.68G [00:27<00:02, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▌   | 4.94G/11.4G [00:27<00:43, 147MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|█▊| 5.28G/5.68G [00:27<00:02, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▌   | 4.96G/11.4G [00:27<00:44, 143MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|█▊| 5.31G/5.68G [00:27<00:02, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▋   | 4.98G/11.4G [00:27<00:43, 147MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|█▉| 5.33G/5.68G [00:27<00:01, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▋   | 5.00G/11.4G [00:27<00:41, 151MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|█▉| 5.35G/5.68G [00:27<00:01, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▋   | 5.02G/11.4G [00:27<00:41, 154MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|█▉| 5.37G/5.68G [00:27<00:01, 178MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▋   | 5.04G/11.4G [00:27<00:38, 163MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|█▉| 5.39G/5.68G [00:27<00:01, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.06G/11.4G [00:27<00:38, 163MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|█▉| 5.41G/5.68G [00:28<00:01, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|█▉| 5.43G/5.68G [00:28<00:01, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.09G/11.4G [00:27<00:38, 164MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|█▉| 5.45G/5.68G [00:28<00:01, 183MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.11G/11.4G [00:28<00:38, 162MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|█▉| 5.47G/5.68G [00:28<00:01, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.13G/11.4G [00:28<00:37, 164MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|█▉| 5.49G/5.68G [00:28<00:01, 177MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.15G/11.4G [00:28<00:39, 158MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|█▉| 5.52G/5.68G [00:28<00:00, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▋   | 5.17G/11.4G [00:28<00:40, 153MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|█▉| 5.54G/5.68G [00:28<00:00, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▋   | 5.19G/11.4G [00:28<00:40, 152MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▊   | 5.21G/11.4G [00:28<00:40, 152MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|█▉| 5.56G/5.68G [00:29<00:01, 118MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▊   | 5.23G/11.4G [00:28<00:39, 154MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|█▉| 5.58G/5.68G [00:29<00:00, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▊   | 5.25G/11.4G [00:29<00:40, 150MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|█▉| 5.60G/5.68G [00:29<00:00, 142MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|█▉| 5.62G/5.68G [00:29<00:00, 150MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▊   | 5.27G/11.4G [00:29<00:42, 145MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|█▉| 5.64G/5.68G [00:29<00:00, 153MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▊   | 5.30G/11.4G [00:29<00:41, 145MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors: 100%|█▉| 5.66G/5.68G [00:29<00:00, 164MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "diffusion_pytorch_model.safetensors: 100%|██| 5.68G/5.68G [00:29<00:00, 191MB/s]\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\n",
      "\n",
      "Fetching 22 files:  73%|█████████████████▍      | 16/22 [00:30<00:12,  2.04s/it]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▊   | 5.36G/11.4G [00:29<00:37, 162MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▊   | 5.38G/11.4G [00:29<00:35, 168MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▊   | 5.41G/11.4G [00:29<00:32, 183MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▊   | 5.44G/11.4G [00:30<00:30, 192MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▉   | 5.47G/11.4G [00:30<00:29, 197MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▉   | 5.51G/11.4G [00:30<00:28, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▉   | 5.53G/11.4G [00:30<00:28, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▉   | 5.55G/11.4G [00:30<00:29, 197MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▉   | 5.57G/11.4G [00:30<00:29, 193MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▉   | 5.59G/11.4G [00:30<00:29, 195MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▉   | 5.61G/11.4G [00:30<00:29, 198MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▉   | 5.63G/11.4G [00:31<00:29, 194MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▉   | 5.65G/11.4G [00:31<00:31, 183MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▉   | 5.67G/11.4G [00:31<00:31, 181MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|███   | 5.69G/11.4G [00:31<00:32, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|███   | 5.71G/11.4G [00:31<00:32, 174MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|███   | 5.74G/11.4G [00:31<00:33, 169MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|███   | 5.76G/11.4G [00:31<00:32, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|███   | 5.78G/11.4G [00:31<00:33, 167MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|███   | 5.80G/11.4G [00:32<00:32, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|███   | 5.82G/11.4G [00:32<00:32, 169MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|███   | 5.84G/11.4G [00:32<00:32, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|███   | 5.86G/11.4G [00:32<00:32, 168MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|███   | 5.88G/11.4G [00:32<00:32, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|███   | 5.90G/11.4G [00:32<00:30, 177MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|███▏  | 5.92G/11.4G [00:32<00:32, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|███▏  | 5.95G/11.4G [00:32<00:31, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 5.97G/11.4G [00:33<00:31, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 5.99G/11.4G [00:33<00:30, 174MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 6.01G/11.4G [00:33<00:30, 176MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 6.03G/11.4G [00:33<00:31, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 6.05G/11.4G [00:33<00:31, 169MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|███▏  | 6.07G/11.4G [00:33<00:31, 167MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|███▏  | 6.09G/11.4G [00:33<00:30, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|███▏  | 6.11G/11.4G [00:33<00:30, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|███▏  | 6.13G/11.4G [00:34<00:29, 175MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|███▎  | 6.16G/11.4G [00:34<00:30, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|███▎  | 6.18G/11.4G [00:34<00:29, 174MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.20G/11.4G [00:34<00:29, 177MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.22G/11.4G [00:34<00:31, 164MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.24G/11.4G [00:34<00:31, 163MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.26G/11.4G [00:34<00:31, 164MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.28G/11.4G [00:34<00:31, 164MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|███▎  | 6.30G/11.4G [00:35<00:31, 161MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|███▎  | 6.32G/11.4G [00:35<00:30, 166MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|███▎  | 6.34G/11.4G [00:35<00:29, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|███▎  | 6.36G/11.4G [00:35<00:29, 169MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|███▎  | 6.39G/11.4G [00:35<00:28, 173MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|███▍  | 6.41G/11.4G [00:35<00:29, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|███▍  | 6.43G/11.4G [00:35<00:28, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|███▍  | 6.45G/11.4G [00:35<00:27, 176MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|███▍  | 6.47G/11.4G [00:36<00:28, 175MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|███▍  | 6.49G/11.4G [00:36<00:26, 181MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|███▍  | 6.51G/11.4G [00:36<00:25, 188MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|███▍  | 6.54G/11.4G [00:36<00:23, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|███▍  | 6.56G/11.4G [00:36<00:23, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|███▍  | 6.59G/11.4G [00:36<00:24, 199MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|███▍  | 6.62G/11.4G [00:36<00:22, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|███▌  | 6.65G/11.4G [00:36<00:22, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|███▌  | 6.67G/11.4G [00:36<00:22, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|███▌  | 6.69G/11.4G [00:37<00:22, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|███▌  | 6.72G/11.4G [00:37<00:22, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|███▌  | 6.75G/11.4G [00:37<00:21, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███▌  | 6.77G/11.4G [00:37<00:22, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███▌  | 6.81G/11.4G [00:37<00:21, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███▌  | 6.84G/11.4G [00:37<00:21, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███▌  | 6.86G/11.4G [00:37<00:21, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.88G/11.4G [00:38<00:23, 192MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.90G/11.4G [00:38<00:24, 181MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.92G/11.4G [00:38<00:25, 174MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.94G/11.4G [00:38<00:25, 176MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.96G/11.4G [00:38<00:25, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███▋  | 6.98G/11.4G [00:38<00:25, 175MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███▋  | 7.00G/11.4G [00:38<00:26, 164MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███▋  | 7.03G/11.4G [00:38<00:25, 168MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███▋  | 7.05G/11.4G [00:39<00:25, 167MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███▋  | 7.07G/11.4G [00:39<00:25, 171MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███▋  | 7.09G/11.4G [00:39<00:25, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.11G/11.4G [00:39<00:24, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.13G/11.4G [00:39<00:25, 169MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.15G/11.4G [00:39<00:24, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.17G/11.4G [00:39<00:24, 170MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.19G/11.4G [00:39<00:23, 175MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▊  | 7.21G/11.4G [00:39<00:22, 182MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▊  | 7.25G/11.4G [00:40<00:20, 197MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▊  | 7.27G/11.4G [00:40<00:20, 199MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▊  | 7.30G/11.4G [00:40<00:19, 204MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▊  | 7.32G/11.4G [00:40<00:19, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▉  | 7.34G/11.4G [00:40<00:19, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▉  | 7.37G/11.4G [00:40<00:18, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▉  | 7.39G/11.4G [00:40<00:19, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▉  | 7.41G/11.4G [00:40<00:18, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▉  | 7.44G/11.4G [00:41<00:18, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▉  | 7.48G/11.4G [00:41<00:18, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▉  | 7.51G/11.4G [00:41<00:18, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▉  | 7.54G/11.4G [00:41<00:18, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▉  | 7.57G/11.4G [00:41<00:18, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|████  | 7.60G/11.4G [00:41<00:17, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|████  | 7.63G/11.4G [00:41<00:17, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|████  | 7.67G/11.4G [00:42<00:17, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|████  | 7.70G/11.4G [00:42<00:17, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|████  | 7.73G/11.4G [00:42<00:17, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|████  | 7.75G/11.4G [00:42<00:17, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|████  | 7.78G/11.4G [00:42<00:17, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|████▏ | 7.81G/11.4G [00:42<00:16, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|████▏ | 7.84G/11.4G [00:42<00:16, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|████▏ | 7.87G/11.4G [00:43<00:16, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|████▏ | 7.91G/11.4G [00:43<00:16, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|████▏ | 7.93G/11.4G [00:43<00:16, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|████▏ | 7.96G/11.4G [00:43<00:15, 213MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|████▏ | 7.99G/11.4G [00:43<00:16, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|████▏ | 8.02G/11.4G [00:43<00:15, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|████▎ | 8.05G/11.4G [00:43<00:15, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|████▎ | 8.07G/11.4G [00:44<00:15, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|████▎ | 8.10G/11.4G [00:44<00:15, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|████▎ | 8.13G/11.4G [00:44<00:15, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|████▎ | 8.16G/11.4G [00:44<00:15, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|████▎ | 8.19G/11.4G [00:44<00:15, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|████▎ | 8.22G/11.4G [00:44<00:15, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|████▎ | 8.25G/11.4G [00:44<00:14, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|████▎ | 8.27G/11.4G [00:45<00:15, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|████▍ | 8.30G/11.4G [00:45<00:14, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|████▍ | 8.34G/11.4G [00:45<00:14, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|████▍ | 8.37G/11.4G [00:45<00:14, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|████▍ | 8.40G/11.4G [00:45<00:13, 213MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|████▍ | 8.43G/11.4G [00:45<00:14, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|████▍ | 8.45G/11.4G [00:45<00:13, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|████▍ | 8.48G/11.4G [00:45<00:13, 216MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|████▍ | 8.51G/11.4G [00:46<00:13, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|████▌ | 8.55G/11.4G [00:46<00:13, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|████▌ | 8.58G/11.4G [00:46<00:13, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|████▌ | 8.60G/11.4G [00:46<00:13, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|████▌ | 8.62G/11.4G [00:46<00:13, 204MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|████▌ | 8.65G/11.4G [00:46<00:12, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|████▌ | 8.68G/11.4G [00:46<00:12, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|████▌ | 8.71G/11.4G [00:47<00:12, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|████▌ | 8.75G/11.4G [00:47<00:12, 213MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|████▋ | 8.78G/11.4G [00:47<00:12, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|████▋ | 8.81G/11.4G [00:47<00:12, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|████▋ | 8.84G/11.4G [00:47<00:11, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|████▋ | 8.87G/11.4G [00:47<00:11, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|████▋ | 8.90G/11.4G [00:48<00:11, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|████▋ | 8.93G/11.4G [00:48<00:11, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|████▋ | 8.97G/11.4G [00:48<00:11, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|████▋ | 8.99G/11.4G [00:48<00:11, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|████▊ | 9.01G/11.4G [00:48<00:12, 195MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|████▊ | 9.03G/11.4G [00:48<00:12, 192MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████▊ | 9.05G/11.4G [00:48<00:11, 196MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████▊ | 9.07G/11.4G [00:48<00:11, 198MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████▊ | 9.10G/11.4G [00:49<00:10, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████▊ | 9.12G/11.4G [00:49<00:10, 204MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████▊ | 9.15G/11.4G [00:49<00:10, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████▊ | 9.18G/11.4G [00:49<00:10, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████▊ | 9.21G/11.4G [00:49<00:10, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████▊ | 9.23G/11.4G [00:49<00:10, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████▉ | 9.26G/11.4G [00:49<00:10, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████▉ | 9.29G/11.4G [00:49<00:09, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████▉ | 9.31G/11.4G [00:50<00:09, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████▉ | 9.34G/11.4G [00:50<00:09, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▉ | 9.37G/11.4G [00:50<00:09, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▉ | 9.41G/11.4G [00:50<00:09, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▉ | 9.43G/11.4G [00:50<00:09, 206MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▉ | 9.46G/11.4G [00:50<00:09, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|█████ | 9.48G/11.4G [00:50<00:08, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|█████ | 9.51G/11.4G [00:50<00:08, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|█████ | 9.54G/11.4G [00:51<00:08, 213MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|█████ | 9.57G/11.4G [00:51<00:08, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|█████ | 9.60G/11.4G [00:51<00:08, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|█████ | 9.64G/11.4G [00:51<00:08, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|█████ | 9.66G/11.4G [00:51<00:08, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|█████ | 9.68G/11.4G [00:51<00:08, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|█████▏| 9.71G/11.4G [00:51<00:07, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|█████▏| 9.74G/11.4G [00:52<00:07, 214MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|█████▏| 9.77G/11.4G [00:52<00:07, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|█████▏| 9.79G/11.4G [00:52<00:07, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|█████▏| 9.83G/11.4G [00:52<00:07, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|█████▏| 9.86G/11.4G [00:52<00:07, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|█████▏| 9.89G/11.4G [00:52<00:07, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|█████▏| 9.92G/11.4G [00:52<00:06, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|█████▎| 9.95G/11.4G [00:53<00:06, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|█████▎| 9.98G/11.4G [00:53<00:06, 215MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|█████▎| 10.0G/11.4G [00:53<00:06, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|█████▎| 10.0G/11.4G [00:53<00:06, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|█████▎| 10.1G/11.4G [00:53<00:06, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|█████▎| 10.1G/11.4G [00:53<00:05, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|█████▎| 10.1G/11.4G [00:53<00:05, 209MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|█████▎| 10.2G/11.4G [00:54<00:05, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|█████▍| 10.2G/11.4G [00:54<00:06, 189MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|█████▍| 10.2G/11.4G [00:54<00:06, 188MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|█████▍| 10.2G/11.4G [00:54<00:05, 192MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|█████▍| 10.2G/11.4G [00:54<00:06, 184MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|█████▍| 10.3G/11.4G [00:54<00:05, 184MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|█████▍| 10.3G/11.4G [00:54<00:05, 190MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|█████▍| 10.3G/11.4G [00:54<00:05, 187MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|█████▍| 10.3G/11.4G [00:54<00:05, 187MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|█████▍| 10.3G/11.4G [00:55<00:05, 184MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|█████▍| 10.4G/11.4G [00:55<00:05, 189MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|█████▍| 10.4G/11.4G [00:55<00:04, 199MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|█████▌| 10.4G/11.4G [00:55<00:04, 201MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|█████▌| 10.5G/11.4G [00:55<00:04, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|█████▌| 10.5G/11.4G [00:55<00:04, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|█████▌| 10.5G/11.4G [00:55<00:04, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|█████▌| 10.5G/11.4G [00:56<00:03, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|█████▌| 10.6G/11.4G [00:56<00:03, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|█████▌| 10.6G/11.4G [00:56<00:03, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|█████▌| 10.6G/11.4G [00:56<00:03, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|█████▌| 10.6G/11.4G [00:56<00:03, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|█████▋| 10.7G/11.4G [00:56<00:03, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|█████▋| 10.7G/11.4G [00:56<00:03, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|█████▋| 10.7G/11.4G [00:57<00:04, 143MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|█████▋| 10.7G/11.4G [00:57<00:04, 149MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|█████▋| 10.8G/11.4G [00:57<00:03, 155MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|█████▋| 10.8G/11.4G [00:57<00:03, 162MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|█████▋| 10.8G/11.4G [00:57<00:03, 165MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|█████▋| 10.8G/11.4G [00:57<00:03, 172MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|█████▋| 10.9G/11.4G [00:57<00:02, 180MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|█████▋| 10.9G/11.4G [00:57<00:02, 190MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|█████▊| 10.9G/11.4G [00:58<00:02, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|█████▊| 10.9G/11.4G [00:58<00:02, 202MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|█████▊| 11.0G/11.4G [00:58<00:01, 205MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|█████▊| 11.0G/11.4G [00:58<00:01, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|█████▊| 11.0G/11.4G [00:58<00:01, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|█████▊| 11.0G/11.4G [00:58<00:01, 204MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|█████▊| 11.1G/11.4G [00:58<00:01, 212MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|█████▊| 11.1G/11.4G [00:58<00:01, 210MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|█████▉| 11.1G/11.4G [00:59<00:01, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|█████▉| 11.2G/11.4G [00:59<00:00, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|█████▉| 11.2G/11.4G [00:59<00:00, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|█████▉| 11.2G/11.4G [00:59<00:00, 211MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|█████▉| 11.3G/11.4G [00:59<00:00, 208MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|█████▉| 11.3G/11.4G [00:59<00:00, 207MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|█████▉| 11.3G/11.4G [00:59<00:00, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|█████▉| 11.3G/11.4G [01:00<00:00, 203MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|█████▉| 11.3G/11.4G [01:00<00:00, 194MB/s]\u001b[A\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|██████| 11.4G/11.4G [01:00<00:00, 188MB/s]\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n",
      "Fetching 22 files: 100%|████████████████████████| 22/22 [01:01<00:00,  2.79s/it]\n",
      "/workspace/Wan2.1-T2V-1.3B\n"
     ]
    }
   ],
   "source": [
    "!pip install \"huggingface_hub[cli]\"\n",
    "!mkdir Wan2.1-T2V-1.3B\n",
    "!huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./Wan2.1-T2V-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "cziQM03zMHO4",
    "outputId": "33f63a1d-59f5-4a19-ccd1-80175ab4a3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# dataset = '{dataset_path.replace(os.sep, \"/\")}'\n",
    "import os\n",
    "import torch\n",
    "# # use bfloat16 for the entire notebook\n",
    "# torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "# if torch.cuda.get_device_properties(0).major >= 8:\n",
    "#     # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "#     torch.backends.cuda.matmul.allow_tf32 = True\n",
    "#     torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import imageio.v2 as iio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from loguru import logger as guru\n",
    "\n",
    "# from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# Florence model import - required dependency\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "def extract_video_name(path):\n",
    "  \"\"\"Intelligently recognize video name from path\"\"\"\n",
    "  if not path:\n",
    "      return \"sequence\"\n",
    "\n",
    "  # If path contains video_frames_, it means extracted from video\n",
    "  if \"video_frames_\" in path:\n",
    "      # Use timestamp as video name\n",
    "      import re\n",
    "      match = re.search(r'video_frames_(\\d+)', path)\n",
    "      if match:\n",
    "          timestamp = match.group(1)\n",
    "          return f\"video_{timestamp}\"\n",
    "\n",
    "  # Otherwise use directory name\n",
    "  path_obj = Path(path)\n",
    "  dir_name = path_obj.name\n",
    "\n",
    "  # If directory name is empty or special directory, use parent directory name\n",
    "  if not dir_name or dir_name in ['.', '..']:\n",
    "      dir_name = path_obj.parent.name\n",
    "\n",
    "  # Clean directory name, remove possible special characters\n",
    "  import re\n",
    "  clean_name = re.sub(r'[^\\w\\-_]', '_', dir_name)\n",
    "\n",
    "  return clean_name if clean_name else \"sequence\"\n",
    "\n",
    "def isimage(p):\n",
    "    ext = os.path.splitext(p.lower())[-1]\n",
    "    return ext in [\".png\", \".jpg\", \".jpeg\"]\n",
    "def set_img_dir(img_dir: str) -> int:\n",
    "    # self._clear_image()\n",
    "    image = None\n",
    "    frame_index = 0\n",
    "    cur_mask = None\n",
    "    cur_logit = None\n",
    "    masks_all = []\n",
    "    bbox_masks_all = []\n",
    "\n",
    "    guru.info(f\"Scanning directory: {img_dir}\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        guru.error(f\"Directory does not exist: {img_dir}\")\n",
    "        return 0\n",
    "\n",
    "    all_files = os.listdir(img_dir)\n",
    "    guru.info(f\"Total {len(all_files)} files in directory: {all_files}\")\n",
    "\n",
    "    img_paths = [\n",
    "        os.path.abspath(os.path.join(img_dir, p)) for p in sorted(all_files) if isimage(p)\n",
    "    ]\n",
    "\n",
    "    guru.info(f\"Found {len(img_paths)} image files: {[os.path.basename(p) for p in img_paths]}\")\n",
    "\n",
    "    return len(img_paths)\n",
    "\n",
    "\n",
    "def validate_frame_count(frames):\n",
    "    \"\"\"Validate if frame count follows 4N+1 format and is within 5-81 range\"\"\"\n",
    "    if frames < 5 or frames > 81:\n",
    "        return False, f\"Frame count must be between 5-81, current value: {frames}\"\n",
    "\n",
    "    if (frames - 1) % 4 != 0:\n",
    "        # Find closest valid value\n",
    "        valid_values = [4*n + 1 for n in range(1, 21) if 5 <= 4*n + 1 <= 81]\n",
    "        closest = min(valid_values, key=lambda x: abs(x - frames))\n",
    "        return False, f\"Frame count must follow 4N+1 format (N is positive integer), suggested: {closest}\"\n",
    "\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def resize_and_crop_frame(frame, target_width, target_height):\n",
    "    \"\"\"Resize and crop frame, following resize_video.py logic\"\"\"\n",
    "    if target_width <= 0 or target_height <= 0:\n",
    "        return frame\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # If already target size, return directly\n",
    "    if w == target_width and h == target_height:\n",
    "        return frame\n",
    "\n",
    "    # Determine if landscape or portrait\n",
    "    is_landscape = w >= h\n",
    "    target_is_landscape = target_width >= target_height\n",
    "\n",
    "    # Calculate scaling and cropping\n",
    "    if is_landscape == target_is_landscape:\n",
    "        # Same orientation\n",
    "        if w / h > target_width / target_height:\n",
    "            # Width ratio is larger, adjust height first\n",
    "            new_height = target_height\n",
    "            new_width = int(w * (target_height / h))\n",
    "            frame = cv2.resize(frame, (new_width, new_height))\n",
    "            # Crop center part\n",
    "            start_x = (new_width - target_width) // 2\n",
    "            frame = frame[:, start_x:start_x + target_width]\n",
    "        else:\n",
    "            # Height ratio is larger, adjust width first\n",
    "            new_width = target_width\n",
    "            new_height = int(h * (target_width / w))\n",
    "            frame = cv2.resize(frame, (new_width, new_height))\n",
    "            # Crop center part\n",
    "            start_y = (new_height - target_height) // 2\n",
    "            frame = frame[start_y:start_y + target_height, :]\n",
    "    else:\n",
    "        # Different orientation, need rotation or special handling\n",
    "        # Simple handling here: scale and crop according to target ratio\n",
    "        scale_w = target_width / w\n",
    "        scale_h = target_height / h\n",
    "        scale = max(scale_w, scale_h)  # Choose larger scale ratio to ensure coverage\n",
    "\n",
    "        new_width = int(w * scale)\n",
    "        new_height = int(h * scale)\n",
    "        frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "        # Center crop\n",
    "        start_x = max(0, (new_width - target_width) // 2)\n",
    "        start_y = max(0, (new_height - target_height) // 2)\n",
    "        end_x = min(new_width, start_x + target_width)\n",
    "        end_y = min(new_height, start_y + target_height)\n",
    "\n",
    "        frame = frame[start_y:end_y, start_x:end_x]\n",
    "\n",
    "        # If cropped size is insufficient, add padding (black borders)\n",
    "        if frame.shape[:2] != (target_height, target_width):\n",
    "            padded_frame = np.zeros((target_height, target_width, 3), dtype=frame.dtype)\n",
    "            h_offset = (target_height - frame.shape[0]) // 2\n",
    "            w_offset = (target_width - frame.shape[1]) // 2\n",
    "            padded_frame[h_offset:h_offset+frame.shape[0], w_offset:w_offset+frame.shape[1]] = frame\n",
    "            frame = padded_frame\n",
    "\n",
    "    return frame\n",
    "print('done')\n",
    "# extract_button.click(\n",
    "#     actually_extract_frames,\n",
    "#     [input_video_field, video_target_frames, video_target_width, video_target_height],\n",
    "#     [instruction, extracted_dir_field, frame_index, input_image, mask_dir_field]\n",
    "# )\n",
    "def actually_extract_frames(video_file, target_frames, target_width, target_height):\n",
    "  if video_file is None:\n",
    "      error_msg = \"Please upload a video file first\"\n",
    "      return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "  # Validate frame count\n",
    "  is_valid, error_msg = validate_frame_count(int(target_frames))\n",
    "  if not is_valid:\n",
    "      return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "  # Create temporary directory to store extracted frames\n",
    "  temp_dir = os.path.join(\"./tmp\", f\"video_frames_{int(time.time())}\")\n",
    "  os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "  try:\n",
    "      # Get video information\n",
    "      cap = cv2.VideoCapture(video_file)\n",
    "      if not cap.isOpened():\n",
    "          error_msg = \"Cannot open video file\"\n",
    "          return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "      orig_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "      orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "      orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "      # Determine final frame count\n",
    "      final_frames = int(target_frames)\n",
    "      final_width = int(target_width)\n",
    "      final_height = int(target_height)\n",
    "\n",
    "      # Calculate sampling interval\n",
    "      if final_frames >= orig_frame_count:\n",
    "          # Extract all frames\n",
    "          frame_indices = list(range(orig_frame_count))\n",
    "      else:\n",
    "          # Uniform sampling\n",
    "          interval = orig_frame_count / final_frames\n",
    "          frame_indices = [min(int(i * interval), orig_frame_count - 1) for i in range(final_frames)]\n",
    "\n",
    "      # Extract frames\n",
    "      extracted_frames = 0\n",
    "      guru.info(f\"Starting frame extraction, target frame count: {len(frame_indices)}\")\n",
    "      guru.info(f\"Save directory: {temp_dir}\")\n",
    "\n",
    "      for i, frame_idx in enumerate(frame_indices):\n",
    "          cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "          ret, frame = cap.read()\n",
    "          if not ret:\n",
    "              guru.warning(f\"Cannot read frame {frame_idx}\")\n",
    "              break\n",
    "\n",
    "          # Adjust resolution\n",
    "          if orig_width != final_width or orig_height != final_height:\n",
    "              # frame = prompts.resize_and_crop_frame(frame, final_width, final_height) # prompts = PromptGUI(checkpoint_dir, model_cfg)\n",
    "              frame = resize_and_crop_frame(frame, final_width, final_height)\n",
    "\n",
    "          # Save frame\n",
    "          output_path = os.path.join(temp_dir, f\"{i:05d}.jpg\")\n",
    "          try:\n",
    "              success = cv2.imwrite(output_path, frame)\n",
    "              if success:\n",
    "                  extracted_frames += 1\n",
    "                  if extracted_frames <= 3 or extracted_frames % 10 == 0:  # Only log first 3 and every 10th\n",
    "                      guru.info(f\"Successfully saved frame {i}: {output_path}\")\n",
    "              else:\n",
    "                  guru.error(f\"cv2.imwrite returned failure: {output_path}\")\n",
    "          except Exception as e:\n",
    "              guru.error(f\"Exception occurred while saving frame {i}: {e}\")\n",
    "\n",
    "      guru.info(f\"Actually extracted {extracted_frames} frames\")\n",
    "\n",
    "      cap.release()\n",
    "\n",
    "      # Check what files are actually in the directory\n",
    "      actual_files = os.listdir(temp_dir)\n",
    "      guru.info(f\"After extraction, files in directory {temp_dir}: {actual_files}\")\n",
    "\n",
    "      if extracted_frames == 0:\n",
    "          error_msg = \"Failed to extract any frames\"\n",
    "          return error_msg\n",
    "\n",
    "      # Automatically load extracted frames and get SAM features\n",
    "      # Convert to absolute path\n",
    "      abs_temp_dir = os.path.abspath(temp_dir)\n",
    "      guru.info(f\"Using absolute path: {abs_temp_dir}\")\n",
    "      video_name = extract_video_name(abs_temp_dir)\n",
    "      num_imgs = set_img_dir(abs_temp_dir)\n",
    "      if num_imgs == 0:\n",
    "          error_msg = \"No image files found in extracted directory\"\n",
    "          return error_msg, temp_dir, None, \"./processed_data/sequence\"\n",
    "\n",
    "      # slider = gr.Slider(minimum=0, maximum=num_imgs - 1, value=0, step=1)\n",
    "      # first_image = set_input_image(0)\n",
    "      # guru.debug(f\"Setting frame {i} / {len(img_paths)}\")\n",
    "      # if i < 0 or i >= len(self.img_paths):\n",
    "      #     return image\n",
    "      # self.clear_points()\n",
    "      # self.frame_index = i\n",
    "      # image = iio.imread(self.img_paths[i])\n",
    "      # self.image = image\n",
    "\n",
    "      # return image\n",
    "\n",
    "\n",
    "      # Automatically get SAM features\n",
    "      # sam_message, sam_image = get_sam_features()\n",
    "\n",
    "      # Generate processing information\n",
    "      process_info = []\n",
    "      if target_frames != orig_frame_count:\n",
    "          process_info.append(f\"Frame count: {orig_frame_count} -> {extracted_frames}\")\n",
    "      if target_width != orig_width or target_height != orig_height:\n",
    "          process_info.append(f\"Resolution: {orig_width}x{orig_height} -> {final_width}x{final_height}\")\n",
    "\n",
    "      message = f\"Video frames extracted to: {temp_dir}, total {extracted_frames} frames.\"\n",
    "      if process_info:\n",
    "          message += f\" Processing: {', '.join(process_info)}.\"\n",
    "      # message += f\" {sam_message}\"\n",
    "\n",
    "\n",
    "      # default_output_path = get_default_output_path()\n",
    "      \"\"\"Generate default output path\"\"\"\n",
    "      if video_name:\n",
    "          default_output_path = f\"./processed_data/{video_name}\"\n",
    "      else:\n",
    "          default_output_path = \"./processed_data/sequence\"\n",
    "\n",
    "      # return message, temp_dir, sam_image if sam_image is not None else first_image, default_output_path\n",
    "      return message, temp_dir, default_output_path\n",
    "\n",
    "  except Exception as e:\n",
    "      error_msg = f\"Frame extraction failed: {str(e)}\"\n",
    "      return error_msg, \"./processed_data/sequence\"\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eME0if54m0Oe"
   },
   "outputs": [],
   "source": [
    "florence_model = None\n",
    "florence_processor = None\n",
    "\n",
    "def init_florence_model():\n",
    "    global florence_model, florence_processor\n",
    "        \n",
    "    if florence_model is not None and florence_processor is not None:\n",
    "        return True  # Model already loaded, no need to reload\n",
    "    \n",
    "    print(\"Loading Florence model, please wait...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    # Load model and processor\n",
    "    florence_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"multimodalart/Florence-2-large-no-flash-attn\", torch_dtype=torch_dtype, trust_remote_code=True\n",
    "    ).to(device)\n",
    "    \n",
    "    florence_processor = AutoProcessor.from_pretrained(\n",
    "        \"multimodalart/Florence-2-large-no-flash-attn\", trust_remote_code=True\n",
    "    )\n",
    "    print(\"Florence model loaded successfully\")\n",
    "    return True\n",
    "init_florence_model()    \n",
    "def generate_caption(image_path, concept_prefix=\"\"):\n",
    "    \"\"\"Use Florence model to generate caption for image\"\"\"\n",
    "    global florence_model, florence_processor\n",
    "\n",
    "    if florence_model is None or florence_processor is None:\n",
    "        raise RuntimeError(\"Florence model not initialized, please call init_florence_model() first\")\n",
    "\n",
    "    device = next(florence_model.parameters()).device\n",
    "    torch_dtype = next(florence_model.parameters()).dtype\n",
    "\n",
    "    # Read image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = \"<DETAILED_CAPTION>\"\n",
    "\n",
    "    # Construct input\n",
    "    inputs = florence_processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "    # Generate caption\n",
    "    generated_ids = florence_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"], max_new_tokens=1024, num_beams=3\n",
    "    )\n",
    "    generated_text = florence_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Post-processing\n",
    "    parsed_answer = florence_processor.post_process_generation(\n",
    "        generated_text, task=prompt, image_size=(image.width, image.height)\n",
    "    )\n",
    "    caption_text = parsed_answer[\"<DETAILED_CAPTION>\"].replace(\"The image shows \", \"\")\n",
    "\n",
    "    # Add concept prefix\n",
    "    if concept_prefix:\n",
    "        caption_text = f\"{concept_prefix} {caption_text}\"\n",
    "\n",
    "    return caption_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEEMMAvgQNnb",
    "outputId": "a78b3104-3bc6-41d4-986d-0146a62bfc52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-25 11:21:29.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mStarting frame extraction, target frame count: 49\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:29.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mSave directory: ./tmp/video_frames_1753442489\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:29.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 0: ./tmp/video_frames_1753442489/00000.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:29.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 1: ./tmp/video_frames_1753442489/00001.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 2: ./tmp/video_frames_1753442489/00002.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:30.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 9: ./tmp/video_frames_1753442489/00009.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:32.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 19: ./tmp/video_frames_1753442489/00019.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:33.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 29: ./tmp/video_frames_1753442489/00029.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:35.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 39: ./tmp/video_frames_1753442489/00039.jpg\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mActually extracted 49 frames\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1mAfter extraction, files in directory ./tmp/video_frames_1753442489: ['00048.jpg', '00047.jpg', '00046.jpg', '00045.jpg', '00044.jpg', '00043.jpg', '00042.jpg', '00041.jpg', '00040.jpg', '00039.jpg', '00038.jpg', '00037.jpg', '00036.jpg', '00035.jpg', '00034.jpg', '00033.jpg', '00032.jpg', '00031.jpg', '00030.jpg', '00029.jpg', '00028.jpg', '00027.jpg', '00026.jpg', '00025.jpg', '00024.jpg', '00023.jpg', '00022.jpg', '00021.jpg', '00020.jpg', '00019.jpg', '00018.jpg', '00017.jpg', '00016.jpg', '00015.jpg', '00014.jpg', '00013.jpg', '00012.jpg', '00011.jpg', '00010.jpg', '00009.jpg', '00008.jpg', '00007.jpg', '00006.jpg', '00005.jpg', '00004.jpg', '00003.jpg', '00002.jpg', '00001.jpg', '00000.jpg']\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mUsing absolute path: /workspace/tmp/video_frames_1753442489\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mScanning directory: /workspace/tmp/video_frames_1753442489\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTotal 49 files in directory: ['00048.jpg', '00047.jpg', '00046.jpg', '00045.jpg', '00044.jpg', '00043.jpg', '00042.jpg', '00041.jpg', '00040.jpg', '00039.jpg', '00038.jpg', '00037.jpg', '00036.jpg', '00035.jpg', '00034.jpg', '00033.jpg', '00032.jpg', '00031.jpg', '00030.jpg', '00029.jpg', '00028.jpg', '00027.jpg', '00026.jpg', '00025.jpg', '00024.jpg', '00023.jpg', '00022.jpg', '00021.jpg', '00020.jpg', '00019.jpg', '00018.jpg', '00017.jpg', '00016.jpg', '00015.jpg', '00014.jpg', '00013.jpg', '00012.jpg', '00011.jpg', '00010.jpg', '00009.jpg', '00008.jpg', '00007.jpg', '00006.jpg', '00005.jpg', '00004.jpg', '00003.jpg', '00002.jpg', '00001.jpg', '00000.jpg']\u001b[0m\n",
      "\u001b[32m2025-07-25 11:21:36.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mFound 49 image files: ['00000.jpg', '00001.jpg', '00002.jpg', '00003.jpg', '00004.jpg', '00005.jpg', '00006.jpg', '00007.jpg', '00008.jpg', '00009.jpg', '00010.jpg', '00011.jpg', '00012.jpg', '00013.jpg', '00014.jpg', '00015.jpg', '00016.jpg', '00017.jpg', '00018.jpg', '00019.jpg', '00020.jpg', '00021.jpg', '00022.jpg', '00023.jpg', '00024.jpg', '00025.jpg', '00026.jpg', '00027.jpg', '00028.jpg', '00029.jpg', '00030.jpg', '00031.jpg', '00032.jpg', '00033.jpg', '00034.jpg', '00035.jpg', '00036.jpg', '00037.jpg', '00038.jpg', '00039.jpg', '00040.jpg', '00041.jpg', '00042.jpg', '00043.jpg', '00044.jpg', '00045.jpg', '00046.jpg', '00047.jpg', '00048.jpg']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "message, temp_dir, default_output_path = actually_extract_frames('/workspace/vid.mp4',49,832,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4poNbrTRRqnF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/tmp/video_frames_1753442489'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir=temp_dir.replace('.','/workspace')\n",
    "temp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "68rr2xlzSLnd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/processed_data/video_1753442489\n",
      "/workspace/processed_data/video_1753442489/traindata\n"
     ]
    }
   ],
   "source": [
    "default_output_path = default_output_path.replace('.','/workspace')\n",
    "traindata_path = default_output_path + '/traindata'\n",
    "print(default_output_path)\n",
    "print(traindata_path)\n",
    "# traindata_path = '/workspace/processed_data/video_1753339507/traindata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oYMVUOvEWl-2"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"{traindata_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IgU8LlEJRm5P"
   },
   "outputs": [],
   "source": [
    "!mv {temp_dir}/* {traindata_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_text ='Two teenagers quietly gaze out over a vibrant city skyline nestled behind a vast, sunlit forest. Framed by leafy trees and an open balcony, the scene captures a moment of calm and wonder beneath a brilliant summer sky filled with fluffy clouds'\n",
    "video_name =extract_video_name('/content/vid.mp4')\n",
    "txt_filename = video_name.replace('.mp4', '.txt')\n",
    "txt_path = os.path.join(default_output_path, 'traindata', 'vid.txt')\n",
    "\n",
    "with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(caption_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_1wiSe0q67a"
   },
   "outputs": [],
   "source": [
    "video_name =extract_video_name('/content/vid.mp4')\n",
    "txt_filename = video_name.replace('.mp4', '.txt')\n",
    "txt_path = os.path.join(default_output_path, 'traindata', txt_filename)\n",
    "\n",
    "# Generate caption for first frame\n",
    "first_frame_path = os.path.join(default_output_path,'traindata', '00000.jpg')\n",
    "caption_text = generate_caption(first_frame_path)\n",
    "\n",
    "with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(caption_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d-lST9fLXT_T"
   },
   "outputs": [],
   "source": [
    "def create_configs(output_dir: str, ckpt_path: str, learning_rate: float = 1e-3,\n",
    "                  save_every_n_epochs: int = 50, epochs: int = 100, precision: str = \"fp16\"):\n",
    "        \"\"\"Create training configuration files\"\"\"\n",
    "        configs_dir = os.path.join(output_dir, 'configs')\n",
    "        os.makedirs(configs_dir, exist_ok=True)\n",
    "\n",
    "        # Create dataset.toml\n",
    "        dataset_config = f\"\"\"enable_ar_bucket = false\n",
    "\n",
    "[[directory]]\n",
    "path = '{os.path.join(output_dir, \"traindata\").replace(os.sep, \"/\")}'\n",
    "num_repeats = 1\n",
    "\"\"\"\n",
    "\n",
    "        dataset_path = os.path.join(configs_dir, 'dataset.toml')\n",
    "        with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(dataset_config)\n",
    "\n",
    "        # Configure different parameters based on precision\n",
    "        if precision == \"fp16\":\n",
    "            transformer_dtype = 'bfloat16'\n",
    "            optimizer_type = 'AdamW'\n",
    "            stabilize_line = \"\"  # Don't add stabilize parameter\n",
    "        else:  # fp8\n",
    "            transformer_dtype = 'float8'\n",
    "            optimizer_type = 'AdamW8bitKahan'\n",
    "            stabilize_line = \"stabilize = true\"\n",
    "\n",
    "        # Create training.toml\n",
    "        training_config = f\"\"\"output_dir = '{os.path.join(output_dir, \"lora\").replace(os.sep, \"/\")}'\n",
    "dataset = '{dataset_path.replace(os.sep, \"/\")}'\n",
    "\n",
    "epochs = {epochs}\n",
    "micro_batch_size_per_gpu = 1\n",
    "pipeline_stages = 1\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_clipping = 1\n",
    "warmup_steps = 0\n",
    "\n",
    "eval_every_n_epochs = 1000000\n",
    "eval_before_first_step = true\n",
    "eval_micro_batch_size_per_gpu = 1\n",
    "eval_gradient_accumulation_steps = 1\n",
    "\n",
    "save_every_n_epochs = {save_every_n_epochs}\n",
    "checkpoint_every_n_minutes = 1000000000\n",
    "activation_checkpointing = 'unsloth'\n",
    "partition_method = 'parameters'\n",
    "save_dtype = 'bfloat16'\n",
    "caching_batch_size = 1\n",
    "steps_per_print = 1\n",
    "video_clip_mode = 'single_beginning'\n",
    "blocks_to_swap = 32\n",
    "\n",
    "[model]\n",
    "type = 'wan'\n",
    "ckpt_path = '{ckpt_path.replace(os.sep, \"/\")}'\n",
    "dtype = 'bfloat16'\n",
    "transformer_dtype = '{transformer_dtype}'\n",
    "timestep_sample_method = 'uniform'\n",
    "\n",
    "[adapter]\n",
    "type = 'lora'\n",
    "rank = 16\n",
    "dtype = 'bfloat16'\n",
    "exclude_linear_modules = [\"k_img\", \"v_img\"]\n",
    "\n",
    "[optimizer]\n",
    "type = '{optimizer_type}'\n",
    "lr = {learning_rate}\n",
    "betas = [0.9, 0.99]\n",
    "weight_decay = 0.01\n",
    "{stabilize_line}\n",
    "\"\"\"\n",
    "\n",
    "        training_path = os.path.join(configs_dir, 'training.toml')\n",
    "        with open(training_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(training_config)\n",
    "\n",
    "        return configs_dir, dataset_path, training_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "St_g35-WXaAN",
    "outputId": "a0d2ebf6-8647-4271-a96d-93c9cf68e2e3"
   },
   "outputs": [],
   "source": [
    "# create_configs(default_output_path,'Wan2.1-I2V-14B-480P')/content/Wan2.1-T2V-1.3B\n",
    "configs_dir, dataset_path, training_path =create_configs(default_output_path,'/workspace/Wan2.1-T2V-1.3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/processed_data/video_1753442489/configs'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import flash_attn_interface\n",
    "    FLASH_ATTN_3_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_3_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    FLASH_ATTN_2_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_2_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "\n",
    "__all__ = [\n",
    "    'flash_attention',\n",
    "    'attention',\n",
    "]\n",
    "\n",
    "\n",
    "def flash_attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    version=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    q:              [B, Lq, Nq, C1].\n",
    "    k:              [B, Lk, Nk, C1].\n",
    "    v:              [B, Lk, Nk, C2]. Nq must be divisible by Nk.\n",
    "    q_lens:         [B].\n",
    "    k_lens:         [B].\n",
    "    dropout_p:      float. Dropout probability.\n",
    "    softmax_scale:  float. The scaling of QK^T before applying softmax.\n",
    "    causal:         bool. Whether to apply causal attention mask.\n",
    "    window_size:    (left right). If not (-1, -1), apply sliding window local attention.\n",
    "    deterministic:  bool. If True, slightly slower and uses more memory.\n",
    "    dtype:          torch.dtype. Apply when dtype of q/k/v is not float16/bfloat16.\n",
    "    \"\"\"\n",
    "    half_dtypes = (torch.float16, torch.bfloat16)\n",
    "    assert dtype in half_dtypes\n",
    "    assert q.device.type == 'cuda' and q.size(-1) <= 256\n",
    "\n",
    "    # params\n",
    "    b, lq, lk, out_dtype = q.size(0), q.size(1), k.size(1), q.dtype\n",
    "\n",
    "    def half(x):\n",
    "        return x if x.dtype in half_dtypes else x.to(dtype)\n",
    "\n",
    "    # preprocess query\n",
    "    if q_lens is None:\n",
    "        q = half(q.flatten(0, 1))\n",
    "        q_lens = torch.tensor(\n",
    "            [lq] * b, dtype=torch.int32).to(\n",
    "                device=q.device, non_blocking=True)\n",
    "    else:\n",
    "        q = half(torch.cat([u[:v] for u, v in zip(q, q_lens)]))\n",
    "\n",
    "    # preprocess key, value\n",
    "    if k_lens is None:\n",
    "        k = half(k.flatten(0, 1))\n",
    "        v = half(v.flatten(0, 1))\n",
    "        k_lens = torch.tensor(\n",
    "            [lk] * b, dtype=torch.int32).to(\n",
    "                device=k.device, non_blocking=True)\n",
    "    else:\n",
    "        k = half(torch.cat([u[:v] for u, v in zip(k, k_lens)]))\n",
    "        v = half(torch.cat([u[:v] for u, v in zip(v, k_lens)]))\n",
    "\n",
    "    q = q.to(v.dtype)\n",
    "    k = k.to(v.dtype)\n",
    "\n",
    "    if q_scale is not None:\n",
    "        q = q * q_scale\n",
    "\n",
    "    if version is not None and version == 3 and not FLASH_ATTN_3_AVAILABLE:\n",
    "        warnings.warn(\n",
    "            'Flash attention 3 is not available, use flash attention 2 instead.'\n",
    "        )\n",
    "\n",
    "    # apply attention\n",
    "    if (version is None or version == 3) and FLASH_ATTN_3_AVAILABLE:\n",
    "        # Note: dropout_p, window_size are not supported in FA3 now.\n",
    "        x = flash_attn_interface.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            seqused_q=None,\n",
    "            seqused_k=None,\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            deterministic=deterministic)[0].unflatten(0, (b, lq))\n",
    "    else:\n",
    "        assert FLASH_ATTN_2_AVAILABLE\n",
    "        x = flash_attn.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic).unflatten(0, (b, lq))\n",
    "\n",
    "    # output\n",
    "    return x.type(out_dtype)\n",
    "\n",
    "\n",
    "def attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    fa_version=None,\n",
    "):\n",
    "    if FLASH_ATTN_2_AVAILABLE or FLASH_ATTN_3_AVAILABLE:\n",
    "        return flash_attention(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            q_lens=q_lens,\n",
    "            k_lens=k_lens,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            q_scale=q_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic,\n",
    "            dtype=dtype,\n",
    "            version=fa_version,\n",
    "        )\n",
    "    else:\n",
    "        if q_lens is not None or k_lens is not None:\n",
    "            warnings.warn(\n",
    "                'Padding mask is disabled when using scaled_dot_product_attention. It can have a significant impact on performance.'\n",
    "            )\n",
    "        attn_mask = None\n",
    "\n",
    "        q = q.transpose(1, 2).to(dtype)\n",
    "        k = k.transpose(1, 2).to(dtype)\n",
    "        v = v.transpose(1, 2).to(dtype)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=attn_mask, is_causal=causal, dropout_p=dropout_p)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aAX6ufw6eJYB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassRegistry:\n",
    "    def __init__(self):\n",
    "        self.classes = dict()\n",
    "        self.args = dict()\n",
    "        self.arg_keys = None\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.classes[item]\n",
    "\n",
    "    def add_to_registry(self, name):\n",
    "        def add_class_by_name(cls):\n",
    "            self.classes[name] = cls\n",
    "            return cls\n",
    "\n",
    "        return add_class_by_name\n",
    "\n",
    "\n",
    "lora_prosessors = ClassRegistry()\n",
    "lora_linear_layers = ClassRegistry()\n",
    "\n",
    "\n",
    "@lora_linear_layers.add_to_registry(\"lora\")\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4, training=True, sig_type=None,):\n",
    "        super().__init__()\n",
    "\n",
    "        if rank > min(in_features, out_features):\n",
    "            raise ValueError(\n",
    "                f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\"\n",
    "            )\n",
    "        self.rank = rank\n",
    "\n",
    "        self.down = nn.Linear(in_features, rank, bias=False)\n",
    "        self.up = nn.Linear(rank, out_features, bias=False)\n",
    "\n",
    "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, hidden_states, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones((1, self.rank))\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        dtype = self.down.weight.dtype\n",
    "\n",
    "        down_hidden_states = self.down(hidden_states.to(dtype)) * mask.to(hidden_states.device)\n",
    "        up_hidden_states = self.up(down_hidden_states)\n",
    "\n",
    "        return up_hidden_states.to(orig_dtype)\n",
    "\n",
    "\n",
    "class LoRACrossAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 base_cross_attn,\n",
    "                 hidden_size,\n",
    "                 lora_linear_layer=LoRALinearLayer,\n",
    "                 cross_attention_dim=None,\n",
    "                 rank=4,\n",
    "                 alpha=1.0,\n",
    "                 sig_type=None):\n",
    "        super().__init__()\n",
    "        self.base = base_cross_attn\n",
    "\n",
    "        # LoRA layers\n",
    "        self.to_q_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_k_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_v_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_out_lora = lora_linear_layer(\n",
    "            hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # LoRA parameter registration (for easy optimizer filtering)\n",
    "        self.lora_layers = [\n",
    "            self.to_q_lora, self.to_k_lora, self.to_v_lora, self.to_out_lora\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, context, context_lens):\n",
    "        b, _, _ = x.shape\n",
    "        n = self.base.num_heads\n",
    "        d = self.base.head_dim\n",
    "\n",
    "        # compute query, key, value with LoRA\n",
    "        q = self.base.norm_q(\n",
    "            self.base.q(x) + self.scaling * self.to_q_lora(x)\n",
    "        ).view(b, -1, n, d)\n",
    "\n",
    "        k = self.base.norm_k(\n",
    "            self.base.k(context) + self.scaling * self.to_k_lora(context)\n",
    "        ).view(b, -1, n, d)\n",
    "\n",
    "        v = self.base.v(context) + self.scaling * self.to_v_lora(context)\n",
    "        v = v.view(b, -1, n, d)\n",
    "\n",
    "        # compute attention\n",
    "        x = flash_attention(q, k, v, k_lens=context_lens)\n",
    "\n",
    "        # output projection with LoRA\n",
    "        x = x.flatten(2)\n",
    "        x = self.base.o(x) + self.scaling * self.to_out_lora(x)\n",
    "        return x\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Wan2.1/wan/modules/model.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n",
      "/workspace/Wan2.1/wan/modules/model.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "import wan\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "@autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "class alpha():\n",
    "    pass\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "1xEOPeaj-3ps",
    "outputId": "0ee14fea-afdb-4e5d-80a4-9207422b3217",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#wan.py\n",
    "from torch.amp import autocast\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "# sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), '../submodules/Wan2_1'))\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import set_module_tensor_to_device\n",
    "\n",
    "from base import BasePipeline, PreprocessMediaFile, make_contiguous\n",
    "# from common import AUTOCAST_DTYPE #주범 None값\n",
    "AUTOCAST_DTYPE = torch.bfloat16\n",
    "from offloading import ModelOffloader\n",
    "import wan\n",
    "from wan.modules.t5 import T5Encoder, T5Decoder, T5Model\n",
    "from wan.modules.tokenizers import HuggingfaceTokenizer\n",
    "from wan.modules.vae import WanVAE\n",
    "from wan.modules.model import (\n",
    "    WanModel, sinusoidal_embedding_1d, WanLayerNorm, WanSelfAttention, WAN_CROSSATTENTION_CLASSES\n",
    ")\n",
    "from wan.modules.clip import CLIPModel\n",
    "from wan import configs as wan_configs\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "KEEP_IN_HIGH_PRECISION = ['norm', 'bias', 'patch_embedding', 'text_embedding', 'time_embedding', 'time_projection', 'head', 'modulation']\n",
    "\n",
    "\n",
    "class WanModelFromSafetensors(WanModel):\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        weights_file,\n",
    "        config_file,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        transformer_dtype=torch.bfloat16,\n",
    "    ):\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        config.pop(\"_class_name\", None)\n",
    "        config.pop(\"_diffusers_version\", None)\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = cls(**config)\n",
    "\n",
    "        state_dict = load_file(weights_file, device='cpu')\n",
    "        state_dict = {\n",
    "            re.sub(r'^model\\.diffusion_model\\.', '', k): v for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            dtype_to_use = torch_dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "            set_module_tensor_to_device(model, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "\n",
    "        return model\n",
    "\n",
    "def vae_encode(tensor, vae):\n",
    "    return vae.model.encode(tensor, vae.scale)\n",
    "\n",
    "def umt5_keys_mapping_comfy(state_dict):\n",
    "    import re\n",
    "    # define key mappings rule\n",
    "    def execute_mapping(original_key):\n",
    "        # Token embedding mapping\n",
    "        if original_key == \"shared.weight\":\n",
    "            return \"token_embedding.weight\"\n",
    "\n",
    "        # Final layer norm mapping\n",
    "        if original_key == \"encoder.final_layer_norm.weight\":\n",
    "            return \"norm.weight\"\n",
    "\n",
    "        # Block layer mappings\n",
    "        block_match = re.match(r\"encoder\\.block\\.(\\d+)\\.layer\\.(\\d+)\\.(.+)\", original_key)\n",
    "        if block_match:\n",
    "            block_num = block_match.group(1)\n",
    "            layer_type = int(block_match.group(2))\n",
    "            rest = block_match.group(3)\n",
    "\n",
    "            # self-attn layer（layer.0）\n",
    "            if layer_type == 0:\n",
    "                if \"SelfAttention\" in rest:\n",
    "                    attn_part = rest.split(\".\")[1]\n",
    "                    if attn_part in [\"q\", \"k\", \"v\", \"o\"]:\n",
    "                        return f\"blocks.{block_num}.attn.{attn_part}.weight\"\n",
    "                    elif attn_part == \"relative_attention_bias\":\n",
    "                        return f\"blocks.{block_num}.pos_embedding.embedding.weight\"\n",
    "                elif rest == \"layer_norm.weight\":\n",
    "                    return f\"blocks.{block_num}.norm1.weight\"\n",
    "\n",
    "            # FFN Layer（layer.1）\n",
    "            elif layer_type == 1:\n",
    "                if \"DenseReluDense\" in rest:\n",
    "                    parts = rest.split(\".\")\n",
    "                    if parts[1] == \"wi_0\":\n",
    "                        return f\"blocks.{block_num}.ffn.gate.0.weight\"\n",
    "                    elif parts[1] == \"wi_1\":\n",
    "                        return f\"blocks.{block_num}.ffn.fc1.weight\"\n",
    "                    elif parts[1] == \"wo\":\n",
    "                        return f\"blocks.{block_num}.ffn.fc2.weight\"\n",
    "                elif rest == \"layer_norm.weight\":\n",
    "                    return f\"blocks.{block_num}.norm2.weight\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    new_state_dict = {}\n",
    "    unmapped_keys = []\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = execute_mapping(key)\n",
    "        if new_key:\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            unmapped_keys.append(key)\n",
    "\n",
    "    print(f\"Unmapped keys (usually safe to ignore): {unmapped_keys}\")\n",
    "    del state_dict\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def umt5_keys_mapping_kijai(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"attention.\", \"attn.\")\n",
    "        new_key = new_key.replace(\"final_norm.weight\", \"norm.weight\")\n",
    "        new_state_dict[new_key] = value\n",
    "    del state_dict\n",
    "    return new_state_dict\n",
    "\n",
    "def umt5_keys_mapping(state_dict):\n",
    "    if 'blocks.0.attn.k.weight' in state_dict:\n",
    "        print(\"loading kijai warpper umt5 safetensors model...\")\n",
    "        return umt5_keys_mapping_kijai(state_dict)\n",
    "    else:\n",
    "        print(\"loading comfyui repacked umt5 safetensors model...\")\n",
    "        return umt5_keys_mapping_comfy(state_dict)\n",
    "\n",
    "# We can load T5 a lot faster by copying some code so we can construct the model\n",
    "# inside an init_empty_weights() context.\n",
    "\n",
    "def _t5(name,\n",
    "        encoder_only=False,\n",
    "        decoder_only=False,\n",
    "        return_tokenizer=False,\n",
    "        tokenizer_kwargs={},\n",
    "        dtype=torch.float32,\n",
    "        device='cpu',\n",
    "        **kwargs):\n",
    "    # sanity check\n",
    "    assert not (encoder_only and decoder_only)\n",
    "\n",
    "    # params\n",
    "    if encoder_only:\n",
    "        model_cls = T5Encoder\n",
    "        kwargs['vocab'] = kwargs.pop('vocab_size')\n",
    "        kwargs['num_layers'] = kwargs.pop('encoder_layers')\n",
    "        _ = kwargs.pop('decoder_layers')\n",
    "    elif decoder_only:\n",
    "        model_cls = T5Decoder\n",
    "        kwargs['vocab'] = kwargs.pop('vocab_size')\n",
    "        kwargs['num_layers'] = kwargs.pop('decoder_layers')\n",
    "        _ = kwargs.pop('encoder_layers')\n",
    "    else:\n",
    "        model_cls = T5Model\n",
    "\n",
    "    # init model\n",
    "    with torch.device(device):\n",
    "        model = model_cls(**kwargs)\n",
    "\n",
    "    # init tokenizer\n",
    "    if return_tokenizer:\n",
    "        tokenizer = HuggingfaceTokenizer(f'google/{name}', **tokenizer_kwargs)\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def umt5_xxl(**kwargs):\n",
    "    cfg = dict(\n",
    "        vocab_size=256384,\n",
    "        dim=4096,\n",
    "        dim_attn=4096,\n",
    "        dim_ffn=10240,\n",
    "        num_heads=64,\n",
    "        encoder_layers=24,\n",
    "        decoder_layers=24,\n",
    "        num_buckets=32,\n",
    "        shared_pos=False,\n",
    "        dropout=0.1)\n",
    "    cfg.update(**kwargs)\n",
    "    return _t5('umt5-xxl', **cfg)\n",
    "\n",
    "\n",
    "class T5EncoderModel:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_len,\n",
    "        dtype=torch.bfloat16,\n",
    "        device=torch.cuda.current_device(),\n",
    "        checkpoint_path=None,\n",
    "        tokenizer_path=None,\n",
    "        shard_fn=None,\n",
    "    ):\n",
    "        self.text_len = text_len\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "\n",
    "        # init model\n",
    "        with init_empty_weights():\n",
    "            model = umt5_xxl(\n",
    "                encoder_only=True,\n",
    "                return_tokenizer=False,\n",
    "                dtype=dtype,\n",
    "                device=device).eval().requires_grad_(False)\n",
    "\n",
    "        if checkpoint_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(checkpoint_path, device='cpu')\n",
    "            state_dict = umt5_keys_mapping(state_dict)\n",
    "        else:\n",
    "            state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "        model.load_state_dict(state_dict, assign=True)\n",
    "        self.model = model\n",
    "        if shard_fn is not None:\n",
    "            self.model = shard_fn(self.model, sync_module_states=False)\n",
    "        else:\n",
    "            self.model.to(self.device)\n",
    "        # init tokenizer\n",
    "        self.tokenizer = HuggingfaceTokenizer(\n",
    "            name=tokenizer_path, seq_len=text_len, clean='whitespace')\n",
    "\n",
    "    def __call__(self, texts, device):\n",
    "        ids, mask = self.tokenizer(\n",
    "            texts, return_mask=True, add_special_tokens=True)\n",
    "        ids = ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        seq_lens = mask.gt(0).sum(dim=1).long()\n",
    "        context = self.model(ids, mask)\n",
    "        return [u[:v] for u, v in zip(context, seq_lens)]\n",
    "\n",
    "\n",
    "# Wrapper to hold both VAE and CLIP, so we can move both to/from GPU together.\n",
    "class VaeAndClip(nn.Module):\n",
    "    def __init__(self, vae, clip):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.clip = clip\n",
    "\n",
    "\n",
    "class WanAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cross_attn_type,\n",
    "                 dim,\n",
    "                 ffn_dim,\n",
    "                 num_heads,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 cross_attn_norm=False,\n",
    "                 eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.cross_attn_norm = cross_attn_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        self.norm1 = WanLayerNorm(dim, eps)\n",
    "        self.self_attn = WanSelfAttention(dim, num_heads, window_size, qk_norm,\n",
    "                                          eps)\n",
    "        self.norm3 = WanLayerNorm(\n",
    "            dim, eps,\n",
    "            elementwise_affine=True) if cross_attn_norm else nn.Identity()\n",
    "        self.cross_attn = WAN_CROSSATTENTION_CLASSES[cross_attn_type](dim,\n",
    "                                                                      num_heads,\n",
    "                                                                      (-1, -1),\n",
    "                                                                      qk_norm,\n",
    "                                                                      eps)\n",
    "        self.norm2 = WanLayerNorm(dim, eps)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, ffn_dim), nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(ffn_dim, dim))\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        e,\n",
    "        seq_lens,\n",
    "        grid_sizes,\n",
    "        freqs,\n",
    "        context,\n",
    "        context_lens,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, C]\n",
    "            e(Tensor): Shape [B, 6, C]\n",
    "            seq_lens(Tensor): Shape [B], length of each sequence in batch\n",
    "            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)\n",
    "            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]\n",
    "        \"\"\"\n",
    "        e = (self.modulation + e).chunk(6, dim=1)\n",
    "\n",
    "        # self-attention\n",
    "        y = self.self_attn(\n",
    "            self.norm1(x) * (1 + e[1]) + e[0], seq_lens, grid_sizes,\n",
    "            freqs)\n",
    "        x = x + y * e[2]\n",
    "\n",
    "        # cross-attention & ffn function\n",
    "        def cross_attn_ffn(x, context, context_lens, e):\n",
    "            x = x + self.cross_attn(self.norm3(x), context, context_lens)\n",
    "            y = self.ffn(self.norm2(x) * (1 + e[4]) + e[3])\n",
    "            x = x + y * e[5]\n",
    "            return x\n",
    "\n",
    "        x = cross_attn_ffn(x, context, context_lens, e)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, out_dim, patch_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        out_dim = math.prod(patch_size) * out_dim\n",
    "        self.norm = WanLayerNorm(dim, eps)\n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 2, dim) / dim**0.5)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L1, C]\n",
    "            e(Tensor): Shape [B, C]\n",
    "        \"\"\"\n",
    "        with torch.autocast('cuda', dtype=torch.float32):\n",
    "            e = (self.modulation + e.unsqueeze(1)).chunk(2, dim=1)\n",
    "            x = (self.head(self.norm(x) * (1 + e[1]) + e[0]))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Patch these to remove some forced casting to float32, saving memory.\n",
    "wan.modules.model.WanAttentionBlock = WanAttentionBlock\n",
    "wan.modules.model.Head = Head\n",
    "\n",
    "\n",
    "class WanPipeline(BasePipeline):\n",
    "    name = 'wan'\n",
    "    framerate = 16\n",
    "    checkpointable_layers = ['TransformerLayer']\n",
    "    adapter_target_modules = ['WanAttentionBlock']\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_config = self.config['model']\n",
    "        self.offloader = ModelOffloader('dummy', [], 0, 0, True, torch.device('cuda'), False, debug=False)\n",
    "        ckpt_dir = self.model_config['ckpt_path']\n",
    "        dtype = self.model_config['dtype']\n",
    "\n",
    "        # SkyReels V2 uses 24 FPS. There seems to be no better way to autodetect this.\n",
    "        if 'skyreels' in Path(ckpt_dir).name.lower():\n",
    "            skyreels = True\n",
    "            self.framerate = 24\n",
    "            # FPS is different so make sure to use a new cache dir\n",
    "            self.name = 'skyreels_v2'\n",
    "        else:\n",
    "            skyreels = False\n",
    "\n",
    "        self.original_model_config_path = os.path.join(ckpt_dir, 'config.json')\n",
    "        with open(self.original_model_config_path) as f:\n",
    "            json_config = json.load(f)\n",
    "        self.i2v = (json_config['model_type'] == 'i2v')\n",
    "        self.flf2v = (json_config['model_type'] == 'flf2v')\n",
    "        if self.i2v:\n",
    "            if skyreels:\n",
    "                self.name = 'skyreels_v2_i2v'\n",
    "            else:\n",
    "                self.name = 'wan_i2v'\n",
    "        if self.flf2v:\n",
    "            assert not skyreels\n",
    "            self.name = 'wan_flf2v'\n",
    "        model_dim = json_config['dim']\n",
    "        if not self.i2v and model_dim == 1536:\n",
    "            wan_config = wan_configs.t2v_1_3B\n",
    "        elif self.i2v and model_dim == 1536: # There is no official i2v 1.3b model, but there is https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP\n",
    "            # This is a hack,\n",
    "            wan_config = wan_configs.t2v_1_3B\n",
    "            # The following lines are taken from https://github.com/Wan-Video/Wan2.1/blob/main/wan/configs/wan_i2v_14B.py\n",
    "            wan_config.clip_model = 'clip_xlm_roberta_vit_h_14'\n",
    "            wan_config.clip_dtype = torch.float16\n",
    "            wan_config.clip_checkpoint = 'models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth'\n",
    "            wan_config.clip_tokenizer = 'xlm-roberta-large'\n",
    "        elif self.i2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.i2v_14B\n",
    "        elif self.flf2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.flf2v_14B\n",
    "        elif not self.i2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.t2v_14B\n",
    "        else:\n",
    "            raise RuntimeError(f'Could not autodetect model variant. model_dim={model_dim}, i2v={self.i2v}, flf2v={self.flf2v}')\n",
    "\n",
    "        # This is the outermost class, which isn't a nn.Module\n",
    "        t5_model_path = self.model_config['llm_path'] if self.model_config.get('llm_path', None) else os.path.join(ckpt_dir, wan_config.t5_checkpoint)\n",
    "        self.text_encoder = T5EncoderModel(\n",
    "            text_len=wan_config.text_len,\n",
    "            dtype=dtype,\n",
    "            device='cpu',\n",
    "            checkpoint_path=t5_model_path,\n",
    "            tokenizer_path=os.path.join(ckpt_dir, wan_config.t5_tokenizer),\n",
    "            shard_fn=None,\n",
    "        )\n",
    "\n",
    "        # Same here, this isn't a nn.Module.\n",
    "        # TODO: by default the VAE is float32, and therefore so are the latents. Do we want to change that?\n",
    "        self.vae = WanVAE(\n",
    "            vae_pth=os.path.join(ckpt_dir, wan_config.vae_checkpoint),\n",
    "            device='cpu',\n",
    "        )\n",
    "        # These need to be on the device the VAE will be moved to during caching.\n",
    "        self.vae.mean = self.vae.mean.to('cuda')\n",
    "        self.vae.std = self.vae.std.to('cuda')\n",
    "        self.vae.scale = [self.vae.mean, 1.0 / self.vae.std]\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            self.clip = CLIPModel(\n",
    "                dtype=dtype,\n",
    "                device='cpu',\n",
    "                checkpoint_path=os.path.join(ckpt_dir, wan_config.clip_checkpoint),\n",
    "                tokenizer_path=os.path.join(ckpt_dir, wan_config.clip_tokenizer)\n",
    "            )\n",
    "\n",
    "    # delay loading transformer to save RAM\n",
    "    def load_diffusion_model(self):\n",
    "        dtype = self.model_config['dtype']\n",
    "        transformer_dtype = self.model_config.get('transformer_dtype', dtype)\n",
    "\n",
    "        if transformer_path := self.model_config.get('transformer_path', None):\n",
    "            self.transformer = WanModelFromSafetensors.from_pretrained(\n",
    "                transformer_path,\n",
    "                self.original_model_config_path,\n",
    "                torch_dtype=dtype,\n",
    "                transformer_dtype=transformer_dtype,\n",
    "            )\n",
    "        else:\n",
    "            ckpt_path = Path(self.model_config['ckpt_path'])\n",
    "            with init_empty_weights():\n",
    "                self.transformer = WanModel.from_config(ckpt_path / 'config.json')\n",
    "            state_dict = {}\n",
    "            for shard in ckpt_path.glob('*.safetensors'):\n",
    "                with safetensors.safe_open(shard, framework=\"pt\", device=\"cpu\") as f:\n",
    "                    for key in f.keys():\n",
    "                        state_dict[key] = f.get_tensor(key)\n",
    "            for name, param in self.transformer.named_parameters():\n",
    "                dtype_to_use = dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "                set_module_tensor_to_device(self.transformer, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "#-------------------------------------------------------------------\n",
    "        #lora 파인튜닝\n",
    "        for i,block in enumerate(self.transformer.blocks):\n",
    "            cross_attention_dim = block.cross_attn.dim\n",
    "            attnprocessor = LoRACrossAttention(base_cross_attn=block.cross_attn,hidden_size = cross_attention_dim,cross_attention_dim=cross_attention_dim)\n",
    "            block.cross_attn = attnprocessor\n",
    "            for param in block.cross_attn.base.parameters():\n",
    "              param.requires_grad = False\n",
    "\n",
    "            # Unfreeze LoRA layers\n",
    "            for layer in block.cross_attn.lora_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "#-------------------------------------------------------------------\n",
    "        self.transformer.train()\n",
    "        # We'll need the original parameter name for saving, and the name changes once we wrap modules for pipeline parallelism,\n",
    "        # so store it in an attribute here. Same thing below if we're training a lora and creating lora weights.\n",
    "        for name, p in self.transformer.named_parameters():\n",
    "            p.original_name = name\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.diffusers_pipeline, name)\n",
    "\n",
    "    def get_vae(self):\n",
    "        vae = self.vae.model\n",
    "        clip = self.clip.model if self.i2v or self.flf2v else None\n",
    "        return VaeAndClip(vae, clip)\n",
    "\n",
    "    def get_text_encoders(self):\n",
    "        # Return the inner nn.Module\n",
    "        return [self.text_encoder.model]\n",
    "\n",
    "    def save_adapter(self, save_dir, peft_state_dict):\n",
    "        self.peft_config.save_pretrained(save_dir)\n",
    "        # ComfyUI format.\n",
    "        peft_state_dict = {'diffusion_model.'+k: v for k, v in peft_state_dict.items()}\n",
    "        safetensors.torch.save_file(peft_state_dict, save_dir / 'adapter_model.safetensors', metadata={'format': 'pt'})\n",
    "\n",
    "    def save_model(self, save_dir, diffusers_sd):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_preprocess_media_file_fn(self):\n",
    "        return PreprocessMediaFile(\n",
    "            self.config,\n",
    "            support_video=True,\n",
    "            framerate=self.framerate,\n",
    "            round_height=8,\n",
    "            round_width=8,\n",
    "            round_frames=4,\n",
    "        )\n",
    "\n",
    "    def get_call_vae_fn(self, vae_and_clip):\n",
    "        def fn(tensor):\n",
    "            vae = vae_and_clip.vae\n",
    "            p = next(vae.parameters())\n",
    "            tensor = tensor.to(p.device, p.dtype)\n",
    "\n",
    "            clip = vae_and_clip.clip\n",
    "            if clip is not None:\n",
    "                assert tensor.ndim == 5, f'i2v/flf2v must train on videos, got tensor with shape {tensor.shape}'\n",
    "\n",
    "                # Get video frame count and split points\n",
    "                bs, c, total_frames, h, w = tensor.shape\n",
    "                assert (total_frames - 1) % 3 == 0, f'Video must have 1+3K frames, got {total_frames} frames'\n",
    "                K = (total_frames - 1) // 3\n",
    "\n",
    "                # Frame 1: for CLIP encoding\n",
    "                first_frame = tensor[:, :, 0:1, ...].clone()\n",
    "                clip_context = self.clip.visual(first_frame.to(p.device, p.dtype))\n",
    "\n",
    "                if self.flf2v:\n",
    "                    # For FLF2V, also need last frame CLIP features\n",
    "                    last_frame = tensor[:, :, -1:, ...].clone()\n",
    "                    clip_context = torch.cat([clip_context, self.clip.visual(last_frame.to(p.device, p.dtype))], dim=1)\n",
    "\n",
    "                # Build tensor for VAE encoding\n",
    "                # Frames 2 to K+1: pseudo video condition (keep original content)\n",
    "                condition_frames = tensor[:, :, 1:K+1, ...].clone()\n",
    "\n",
    "                # Frames K+2 to 2K+1: training target\n",
    "                target_frames = tensor[:, :, K+1:2*K+1, ...].clone()\n",
    "\n",
    "                # Directly encode condition frames\n",
    "                y = vae_encode(condition_frames, self.vae)\n",
    "\n",
    "                # Directly encode training target frames\n",
    "                latents = vae_encode(target_frames, self.vae)\n",
    "\n",
    "                ret = {'latents': latents, 'y': y, 'clip_context': clip_context}\n",
    "\n",
    "                # Process mask video (frames 2K+2 to 3K+1)\n",
    "                mask_frames = tensor[:, :, 2*K+1:, ...].clone()\n",
    "                ret['mask_frames'] = mask_frames\n",
    "\n",
    "            else:\n",
    "                latents = vae_encode(tensor, self.vae)\n",
    "                ret = {'latents': latents}\n",
    "\n",
    "            return ret\n",
    "        return fn\n",
    "\n",
    "    def get_call_text_encoder_fn(self, text_encoder):\n",
    "        def fn(caption, is_video):\n",
    "            # Args are lists\n",
    "            p = next(text_encoder.parameters())\n",
    "            ids, mask = self.text_encoder.tokenizer(caption, return_mask=True, add_special_tokens=True)\n",
    "            ids = ids.to(p.device)\n",
    "            mask = mask.to(p.device)\n",
    "            seq_lens = mask.gt(0).sum(dim=1).long()\n",
    "            with torch.autocast(device_type=p.device.type, dtype=p.dtype):\n",
    "                text_embeddings = text_encoder(ids, mask)\n",
    "                return {'text_embeddings': text_embeddings, 'seq_lens': seq_lens}\n",
    "        return fn\n",
    "\n",
    "    def prepare_inputs(self, inputs, timestep_quantile=None):\n",
    "        latents = inputs['latents'].float()\n",
    "        # TODO: why does text_embeddings become float32 here? It's bfloat16 coming out of the text encoder.\n",
    "        text_embeddings = inputs['text_embeddings']\n",
    "        seq_lens = inputs['seq_lens']\n",
    "        mask = inputs['mask']\n",
    "        y = inputs['y'] if self.i2v or self.flf2v else None\n",
    "        clip_context = inputs['clip_context'] if self.i2v or self.flf2v else None\n",
    "        mask_frames = inputs.get('mask_frames', None) if self.i2v or self.flf2v else None\n",
    "\n",
    "        bs, channels, num_frames, h, w = latents.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # make mask (bs, 1, img_h, img_w)\n",
    "            mask = F.interpolate(mask, size=(h, w), mode='nearest-exact')  # resize to latent spatial dimension\n",
    "            mask = mask.unsqueeze(2)  # make mask same number of dims as target\n",
    "\n",
    "        timestep_sample_method = self.model_config.get('timestep_sample_method', 'logit_normal')\n",
    "\n",
    "        if timestep_sample_method == 'logit_normal':\n",
    "            dist = torch.distributions.normal.Normal(0, 1)\n",
    "        elif timestep_sample_method == 'uniform':\n",
    "            dist = torch.distributions.uniform.Uniform(0, 1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if timestep_quantile is not None:\n",
    "            t = dist.icdf(torch.full((bs,), timestep_quantile, device=latents.device))\n",
    "        else:\n",
    "            t = dist.sample((bs,)).to(latents.device)\n",
    "\n",
    "        if timestep_sample_method == 'logit_normal':\n",
    "            sigmoid_scale = self.model_config.get('sigmoid_scale', 1.0)\n",
    "            t = t * sigmoid_scale\n",
    "            t = torch.sigmoid(t)\n",
    "\n",
    "        if shift := self.model_config.get('shift', None):\n",
    "            t = (t * shift) / (1 + (shift - 1) * t)\n",
    "\n",
    "        x_1 = latents\n",
    "        x_0 = torch.randn_like(x_1)\n",
    "        t_expanded = t.view(-1, 1, 1, 1, 1)\n",
    "        x_t = (1 - t_expanded) * x_1 + t_expanded * x_0\n",
    "        target = x_0 - x_1\n",
    "\n",
    "        # timestep input to model needs to be in range [0, 1000]\n",
    "        t = t * 1000\n",
    "\n",
    "        return (\n",
    "            (x_t, y, t, text_embeddings, seq_lens, clip_context, mask_frames),\n",
    "            (target, mask),\n",
    "        )\n",
    "\n",
    "    def to_layers(self):\n",
    "        transformer = self.transformer\n",
    "        layers = [InitialLayer(transformer)]\n",
    "        for i, block in enumerate(transformer.blocks):\n",
    "            layers.append(TransformerLayer(block, i, self.offloader))\n",
    "        layers.append(FinalLayer(transformer))\n",
    "        return layers\n",
    "\n",
    "    def enable_block_swap(self, blocks_to_swap):\n",
    "        transformer = self.transformer\n",
    "        blocks = transformer.blocks\n",
    "        num_blocks = len(blocks)\n",
    "        assert (\n",
    "            blocks_to_swap <= num_blocks - 2\n",
    "        ), f'Cannot swap more than {num_blocks - 2} blocks. Requested {blocks_to_swap} blocks to swap.'\n",
    "        self.offloader = ModelOffloader(\n",
    "            'TransformerBlock', blocks, num_blocks, blocks_to_swap, True, torch.device('cuda'), self.config['reentrant_activation_checkpointing']\n",
    "        )\n",
    "        transformer.blocks = None\n",
    "        transformer.to('cuda')\n",
    "        transformer.blocks = blocks\n",
    "        self.prepare_block_swap_training()\n",
    "        print(f'Block swap enabled. Swapping {blocks_to_swap} blocks out of {num_blocks} blocks.')\n",
    "\n",
    "    def prepare_block_swap_training(self):\n",
    "        self.offloader.enable_block_swap()\n",
    "        self.offloader.set_forward_only(False)\n",
    "        self.offloader.prepare_block_devices_before_forward()\n",
    "\n",
    "    def prepare_block_swap_inference(self, disable_block_swap=False):\n",
    "        if disable_block_swap:\n",
    "            self.offloader.disable_block_swap()\n",
    "        self.offloader.set_forward_only(True)\n",
    "        self.offloader.prepare_block_devices_before_forward()\n",
    "\n",
    "\n",
    "class InitialLayer(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = model.patch_embedding\n",
    "        self.time_embedding = model.time_embedding\n",
    "        self.text_embedding = model.text_embedding\n",
    "        self.time_projection = model.time_projection\n",
    "        self.i2v = (model.model_type == 'i2v')\n",
    "        self.flf2v = (model.model_type == 'flf2v')\n",
    "        if self.i2v or self.flf2v:\n",
    "            self.img_emb = model.img_emb\n",
    "        self.model = [model]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.model[0], name)\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        for item in inputs:\n",
    "            if torch.is_floating_point(item):\n",
    "                item.requires_grad_(True)\n",
    "\n",
    "        x, y, t, context, text_seq_lens, clip_fea, mask_frames = inputs\n",
    "        bs, channels, f, h, w = x.shape\n",
    "        if clip_fea.numel() == 0:\n",
    "            clip_fea = None\n",
    "        context = [emb[:length] for emb, length in zip(context, text_seq_lens)]\n",
    "\n",
    "        device = self.patch_embedding.weight.device\n",
    "        if self.freqs.device != device:\n",
    "            self.freqs = self.freqs.to(device)\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            # Build mask based on mask_frames\n",
    "            if mask_frames is not None:\n",
    "                # Process mask_frames following reference logic\n",
    "                # 1. Convert to single channel (calculate mean)\n",
    "                mask = mask_frames.mean(dim=1, keepdim=False)  # (bs, f, h_orig, w_orig)\n",
    "\n",
    "                # 2. Interpolate to latent space resolution\n",
    "                mask = F.interpolate(\n",
    "                    mask,\n",
    "                    size=(h, w),\n",
    "                    mode='nearest'\n",
    "                )\n",
    "\n",
    "                # 3. Binarize: black (low values) corresponds to 1, white (high values) corresponds to 0\n",
    "                # Assuming input is in [-1,1] range, first normalize to [0,1]\n",
    "                mask = (mask + 1) / 2\n",
    "                mask = (mask < 0.5).float()  # Black regions as 1, white regions as 0\n",
    "\n",
    "                mask = torch.concat([torch.repeat_interleave(mask[:, 0:1], repeats=4, dim=1), mask[:, 1:]], dim=1)\n",
    "                mask = mask.view(mask.shape[0], mask.shape[1] // 4, 4, mask.shape[2], mask.shape[3])\n",
    "                mask = mask.transpose(1, 2)\n",
    "\n",
    "            else:\n",
    "                # If no mask_frames, use original logic\n",
    "                mask = torch.zeros((bs, 4, f, h, w), device=x.device, dtype=x.dtype)\n",
    "                mask[:, :, 0, ...] = 1\n",
    "                if self.flf2v:\n",
    "                    mask[:, :, -1, ...] = 1\n",
    "\n",
    "            y = torch.cat([mask, y], dim=1)\n",
    "            x = [torch.cat([u, v], dim=0) for u, v in zip(x, y)]\n",
    "\n",
    "        # embeddings\n",
    "        x = [self.patch_embedding(u.unsqueeze(0)) for u in x]\n",
    "        grid_sizes = torch.stack(\n",
    "            [torch.tensor(u.shape[2:], dtype=torch.long) for u in x])\n",
    "        x = [u.flatten(2).transpose(1, 2) for u in x]\n",
    "        seq_lens = torch.tensor([u.size(1) for u in x], dtype=torch.long)\n",
    "        seq_len = seq_lens.max()\n",
    "        x = torch.cat([\n",
    "            torch.cat([u, u.new_zeros(1, seq_len - u.size(1), u.size(2))],\n",
    "                      dim=1) for u in x\n",
    "        ])\n",
    "\n",
    "        # time embeddings\n",
    "        e = self.time_embedding(sinusoidal_embedding_1d(self.freq_dim, t).to(x.device, torch.float32))\n",
    "        e0 = self.time_projection(e).unflatten(1, (6, self.dim))\n",
    "\n",
    "        # context\n",
    "        context_lens = None\n",
    "        context = self.text_embedding(\n",
    "            torch.stack([\n",
    "                torch.cat(\n",
    "                    [u, u.new_zeros(self.text_len - u.size(0), u.size(1))])\n",
    "                for u in context\n",
    "            ]))\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            assert clip_fea is not None\n",
    "            if self.flf2v:\n",
    "                self.img_emb.emb_pos.data = self.img_emb.emb_pos.data.to(clip_fea.device, torch.float32)\n",
    "                clip_fea = clip_fea.view(-1, 257, 1280)\n",
    "            context_clip = self.img_emb(clip_fea)  # bs x 257 (x2) x dim\n",
    "            context = torch.concat([context_clip, context], dim=1)\n",
    "\n",
    "        # pipeline parallelism needs everything on the GPU\n",
    "        seq_lens = seq_lens.to(x.device)\n",
    "        grid_sizes = grid_sizes.to(x.device)\n",
    "\n",
    "        return make_contiguous(x, e, e0, seq_lens, grid_sizes, self.freqs, context)\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, block, block_idx, offloader):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.block_idx = block_idx\n",
    "        self.offloader = offloader\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        x, e, e0, seq_lens, grid_sizes, freqs, context = inputs\n",
    "\n",
    "        self.offloader.wait_for_block(self.block_idx)\n",
    "        x = self.block(x, e0, seq_lens, grid_sizes, freqs, context, None)\n",
    "        self.offloader.submit_move_blocks_forward(self.block_idx)\n",
    "\n",
    "        return make_contiguous(x, e, e0, seq_lens, grid_sizes, freqs, context)\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.head = model.head\n",
    "        self.model = [model]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.model[0], name)\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        x, e, e0, seq_lens, grid_sizes, freqs, context = inputs\n",
    "        x = self.head(x, e)\n",
    "        x = self.unpatchify(x, grid_sizes)\n",
    "        return torch.stack(x, dim=0)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_mask_construction()\n",
    "#     test_new_frame_structure()\n",
    "\n",
    "\n",
    "# ckpt_path = Path('Wan2.1-T2V-1.3B')\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     transformer = WanModel.from_config(ckpt_path / 'config.json')\n",
    "# state_dict = {}\n",
    "# for shard in ckpt_path.glob('*.safetensors'):\n",
    "#     with safetensors.safe_open(shard, framework=\"pt\", device=\"cpu\") as f:\n",
    "#         for key in f.keys():\n",
    "#             state_dict[key] = f.get_tensor(key)\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     dtype_to_use = dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "#     set_module_tensor_to_device(transformer, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !pip install deepspeed easydict -qq\n",
    "import deepspeed\n",
    "import easydict as edict\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "from easydict import EasyDict as edict\n",
    "from pathlib import Path\n",
    "import deepspeed\n",
    "\n",
    "def set_config_defaults(config):\n",
    "    # Force the user to set this. If we made it a default of 1, it might use a lot of disk space.\n",
    "    assert 'save_every_n_epochs' in config\n",
    "\n",
    "    config.setdefault('pipeline_stages', 1)\n",
    "    config.setdefault('activation_checkpointing', False)\n",
    "    config['reentrant_activation_checkpointing'] = (config['activation_checkpointing'] == 'unsloth')\n",
    "    config.setdefault('warmup_steps', 0)\n",
    "    if 'save_dtype' in config:\n",
    "        config['save_dtype'] = DTYPE_MAP[config['save_dtype']]\n",
    "\n",
    "    model_config = config['model']\n",
    "    model_dtype_str = model_config['dtype']\n",
    "    model_config['dtype'] = DTYPE_MAP[model_dtype_str]\n",
    "    if transformer_dtype := model_config.get('transformer_dtype', None):\n",
    "        model_config['transformer_dtype'] = DTYPE_MAP.get(transformer_dtype, transformer_dtype)\n",
    "    model_config.setdefault('guidance', 1.0)\n",
    "\n",
    "    if 'adapter' in config:\n",
    "        adapter_config = config['adapter']\n",
    "        adapter_type = adapter_config['type']\n",
    "        if adapter_config['type'] == 'lora':\n",
    "            if 'alpha' in adapter_config:\n",
    "                raise NotImplementedError(\n",
    "                    'This script forces alpha=rank to make the saved LoRA format simpler and more predictable with downstream inference programs. Please remove alpha from the config.'\n",
    "                )\n",
    "            adapter_config['alpha'] = adapter_config['rank']\n",
    "            adapter_config.setdefault('dropout', 0.0)\n",
    "            adapter_config.setdefault('dtype', model_dtype_str)\n",
    "            adapter_config['dtype'] = DTYPE_MAP[adapter_config['dtype']]\n",
    "        else:\n",
    "            raise NotImplementedError(f'Adapter type {adapter_type} is not implemented')\n",
    "\n",
    "    config.setdefault('logging_steps', 1)\n",
    "    config.setdefault('eval_datasets', [])\n",
    "    config.setdefault('eval_gradient_accumulation_steps', 1)\n",
    "    config.setdefault('eval_every_n_steps', None)\n",
    "    config.setdefault('eval_every_n_epochs', None)\n",
    "    config.setdefault('eval_before_first_step', True)\n",
    "\n",
    "def get_config():\n",
    "    # 기본 설정\n",
    "    config = {\n",
    "        \"config\": training_path,  # Path to TOML configuration file\n",
    "        \"local_rank\": -1,  # Local rank passed from distributed launcher\n",
    "        \"resume_from_checkpoint\": None,  # None or True or folder path\n",
    "        \"regenerate_cache\": False,  # Force regenerate cache\n",
    "        \"cache_only\": False,  # Cache model inputs then exit\n",
    "        \"i_know_what_i_am_doing\": False,  # Skip certain checks\n",
    "        \"master_port\": 29500,  # Master port for distributed training\n",
    "        \"dump_dataset\": None,  # Path to dump decoded dataset\n",
    "    }\n",
    "\n",
    "    # DeepSpeed config arguments 추가\n",
    "    ds_parser = deepspeed.add_config_arguments(parser)\n",
    "    ds_defaults = ds_parser.parse_args([])  # 기본값만 추출\n",
    "    ds_config = vars(ds_defaults)  # Namespace → dict\n",
    "\n",
    "    # 합치기\n",
    "    config.update(ds_config)\n",
    "\n",
    "    return edict(config)\n",
    "args = get_config()\n",
    "print('done')\n",
    "# --deepspeed --config ./processed_data/your_sequence/configs/training.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/processed_data/video_1753439446/configs/training.toml'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.config = args.config +=/\n",
    "workspace/processed_data/video_1753439446/configs/training.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOKP8Zi2Wy_J",
    "outputId": "5e57cae1-f282-48e2-af15-fc503d1a594d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WanAttentionBlock(\n",
       "  (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "  (self_attn): WanSelfAttention(\n",
       "    (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (norm_q): WanRMSNorm()\n",
       "    (norm_k): WanRMSNorm()\n",
       "  )\n",
       "  (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "  (cross_attn): WanT2VCrossAttention(\n",
       "    (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (norm_q): WanRMSNorm()\n",
       "    (norm_k): WanRMSNorm()\n",
       "  )\n",
       "  (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "  (ffn): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "    (1): GELU(approximate='tanh')\n",
       "    (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BCDc05mCVcPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n",
      "Device ID: 0\n",
      "  Device Name: NVIDIA RTX 4000 Ada Generation\n",
      "  Compute Capability: 8.9\n",
      "\n",
      "Using device_id: 0\n"
     ]
    }
   ],
   "source": [
    "# prompt: cuda device 이거 0인지 1인지 device_id확인\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "  # Get the number of available CUDA devices\n",
    "  num_cuda_devices = torch.cuda.device_count()\n",
    "  print(f\"Number of CUDA devices available: {num_cuda_devices}\")\n",
    "\n",
    "  # Iterate through available devices and print their properties\n",
    "  for i in range(num_cuda_devices):\n",
    "    device_name = torch.cuda.get_device_name(i)\n",
    "    device_capability = torch.cuda.get_device_capability(i)\n",
    "    print(f\"Device ID: {i}\")\n",
    "    print(f\"  Device Name: {device_name}\")\n",
    "    print(f\"  Compute Capability: {device_capability[0]}.{device_capability[1]}\")\n",
    "\n",
    "  # You can set the device_id based on your preference or configuration\n",
    "  # For example, to use the first device (usually device 0):\n",
    "  device_id = 0\n",
    "  print(f\"\\nUsing device_id: {device_id}\")\n",
    "\n",
    "else:\n",
    "  print(\"CUDA is not available. Please check your GPU and driver installation.\")\n",
    "  # Handle the case where CUDA is not available, e.g., set device_id to None or 'cpu'\n",
    "  device_id = None # Or 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "11gYBaksgz4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import imageio\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime, timezone\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import toml\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "from deepspeed.runtime.pipe import module as ds_pipe_module\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "\n",
    "import dataset as dataset_util\n",
    "import common\n",
    "from common import empty_cuda_cache #is_main_process, DTYPE_MAP, get_rank\n",
    "import saver\n",
    "from isolate_rng import isolate_rng\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    return dist.get_rank()\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "    \n",
    "from typing import Optional\n",
    "import sys\n",
    "import os.path\n",
    "import torch\n",
    "from torch import nn\n",
    "# import peft\n",
    "# from peft.tuners._buffer_dict import BufferDict\n",
    "from transformers import CLIPTextModel, AutoModel\n",
    "import deepspeed\n",
    "from deepspeed.runtime.pipe.schedule import (\n",
    "    SendGrad, RecvActivation, SendActivation, RecvGrad, LoadMicroBatch, ForwardPass, BackwardPass,\n",
    "    ReduceTiedGrads, ReduceGrads, OptimizerStep,\n",
    ")\n",
    "from deepspeed import comm as dist\n",
    "from deepspeed.utils import groups\n",
    "try:\n",
    "    from torch._six import inf\n",
    "except ModuleNotFoundError:\n",
    "    from torch import inf\n",
    "from deepspeed.accelerator import get_accelerator\n",
    "\n",
    "def train_schedule_steps(self):\n",
    "    prev_micro_batch_id = -1\n",
    "    total_steps = 2 * (self.micro_batches + self.stages - 1)\n",
    "    for step_id in range(total_steps):\n",
    "        # Map the step of the pipeline to the micro-batch id and also whether it is a\n",
    "        # forward or backward pass step.\n",
    "        micro_batch_id, is_forward = self._step_to_micro_batch(step_id)\n",
    "\n",
    "        if self._valid_micro_batch(prev_micro_batch_id):\n",
    "            prev_buffer = self._buffer_idx(prev_micro_batch_id)\n",
    "        if self._valid_micro_batch(micro_batch_id):\n",
    "            curr_buffer = self._buffer_idx(micro_batch_id)\n",
    "\n",
    "        cmds = []\n",
    "\n",
    "        # First/last stage loads\n",
    "        if self.stage_id == 0 or self.stage_id == self.stages - 1:\n",
    "            if is_forward and self._valid_micro_batch(micro_batch_id):\n",
    "                cmds.append(LoadMicroBatch(curr_buffer))\n",
    "\n",
    "        # Exchange activations\n",
    "        if is_forward:\n",
    "            if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                cmds.append(SendGrad(prev_buffer))\n",
    "            if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                cmds.append(RecvActivation(curr_buffer))\n",
    "        else:\n",
    "            if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                cmds.append(RecvGrad(curr_buffer))\n",
    "            if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                cmds.append(SendActivation(prev_buffer))\n",
    "\n",
    "        # Computation\n",
    "        if self._valid_micro_batch(micro_batch_id):\n",
    "            if is_forward:\n",
    "                cmds.append(ForwardPass(curr_buffer))\n",
    "            else:\n",
    "                cmds.append(BackwardPass(curr_buffer))\n",
    "\n",
    "        # Model step at the end of the batch\n",
    "        if step_id == total_steps - 1:\n",
    "            cmds.append(ReduceTiedGrads())\n",
    "            cmds.append(ReduceGrads())\n",
    "            cmds.append(OptimizerStep())\n",
    "\n",
    "        # Prepare state for next time\n",
    "        prev_micro_batch_id = micro_batch_id\n",
    "        yield cmds\n",
    "def broadcast_model(self):\n",
    "    for n, p in self.module.named_parameters():\n",
    "        if torch.is_tensor(p) and p.requires_grad:\n",
    "            orig_device = p.device\n",
    "            move_to_gpu = (orig_device != self.device)\n",
    "            if move_to_gpu:\n",
    "                p.data = p.data.to(self.device)\n",
    "            dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)\n",
    "            if move_to_gpu:\n",
    "                p.data = p.data.to(orig_device)\n",
    "\n",
    "\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2, mpu=None):\n",
    "    \"\"\"Clips gradient norm of an iterable of parameters.\n",
    "\n",
    "    This has been adapted from Nvidia megatron. We add norm averaging\n",
    "    to consider MoE params when calculating norm as they will result\n",
    "    in different norms across different ranks.\n",
    "\n",
    "    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n",
    "    added functionality to handle model parallel parameters. Note that\n",
    "    the gradients are modified in place.\n",
    "\n",
    "    Arguments:\n",
    "        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
    "            single Tensor that will have gradients normalized\n",
    "        max_norm (float or int): max norm of the gradients\n",
    "        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n",
    "            infinity norm.\n",
    "\n",
    "    Returns:\n",
    "        Total norm of the parameters (viewed as a single vector).\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    all_norms = []\n",
    "    if norm_type == inf:\n",
    "        for p in parameters:\n",
    "            all_norms.append(p.grad.data.abs().max().float())\n",
    "        total_norm = torch.stack(all_norms).max()\n",
    "        total_norm = total_norm.to(get_accelerator().current_device_name())\n",
    "        # Take max across all GPUs.\n",
    "        if mpu is not None:\n",
    "            dist.all_reduce(total_norm, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n",
    "    else:\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            if mpu is not None:\n",
    "                if (mpu.get_model_parallel_rank() == 0) or deepspeed.runtime.utils.is_model_parallel_parameter(p):\n",
    "                    param_norm = p.grad.data.detach().float().norm(norm_type)\n",
    "                    all_norms.append(param_norm)\n",
    "            else:\n",
    "                param_norm = p.grad.data.detach().float().norm(norm_type)\n",
    "                all_norms.append(param_norm)\n",
    "        if len(all_norms) > 0:\n",
    "            total_norm = torch.stack(all_norms).square().sum().float()\n",
    "        else:\n",
    "            total_norm = get_accelerator().FloatTensor([0.0])\n",
    "        total_norm = total_norm.to(get_accelerator().current_device_name())\n",
    "        # Sum across all model parallel GPUs.\n",
    "        if mpu is not None:\n",
    "            dist.all_reduce(total_norm, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n",
    "        total_norm = total_norm.pow(1. / norm_type)\n",
    "\n",
    "    # Need to average total_norm across different GPUs due to the presence of moe params\n",
    "    pg = groups._get_data_parallel_group()\n",
    "    scaled_norm = total_norm * 1.0 / float(dist.get_world_size(group=pg))\n",
    "    scaled_norm_tensor = scaled_norm\n",
    "\n",
    "    dist.all_reduce(scaled_norm_tensor, group=pg)\n",
    "    total_norm = scaled_norm_tensor\n",
    "    # Change this from the original Deepspeed code.\n",
    "    if len(parameters) > 0:\n",
    "        total_norm = total_norm.to(parameters[0].device)\n",
    "\n",
    "    max_norm = torch.tensor([float(max_norm)], device=total_norm.device)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    tmp_tensor = torch.tensor([1.0], device=clip_coef.device)\n",
    "    clip_coef = torch.min(tmp_tensor, clip_coef)\n",
    "    for p in parameters:\n",
    "        p.grad.data.mul_(clip_coef)\n",
    "    return total_norm\n",
    "    \n",
    "def apply_patches():\n",
    "    # Prevent PEFT from downcasting LoRA weights to fp8 only for this script to upcast them again.\n",
    "    # TODO: probably should send a PR to PEFT. Default behavior looks like a mistake to me.\n",
    "    # peft.tuners.tuners_utils.BaseTunerLayer._move_adapter_to_device_of_base_layer = _move_adapter_to_device_of_base_layer\n",
    "\n",
    "    # Use torch_dtype to avoid needlessly loading the text encoder in float32, only to cast it right after.\n",
    "    # hyvideo.text_encoder.load_text_encoder = load_text_encoder\n",
    "\n",
    "    # LoadMicroBatch before sending / receiving activations so we can avoid a deadlock and broadcast the target\n",
    "    # from the first stage to the last stage. InferenceSchedule already has the commands in the right order\n",
    "    # and doesn't need this.\n",
    "    deepspeed.runtime.pipe.schedule.TrainSchedule.steps = train_schedule_steps\n",
    "\n",
    "    # This does two things:\n",
    "    # 1. For block swapping, some parameters will be on CPU when the DeepSpeedEngine is constructed. So we patch this to\n",
    "    #    first move those parameters to GPU, then back again when broadcasting the model weights from rank 0.\n",
    "    # 2. We skip broadcasting for parameters that don't require grad. These weights are static and always the same because\n",
    "    #    they were loaded from disk, so we can safely skip broadcasting and it's faster.\n",
    "    deepspeed.runtime.engine.DeepSpeedEngine._broadcast_model = broadcast_model\n",
    "\n",
    "# ... later in your code ...\n",
    "\n",
    "    # Don't fail if there are no trainable parameters on a stage.\n",
    "    deepspeed.runtime.engine.DeepSpeedEngine.clip_fp32_gradients = lambda self: clip_grad_norm_(parameters=self.module.parameters(), max_norm=self.gradient_clipping(), mpu=self.mpu)\n",
    "    from unsloth_utils import unsloth_checkpoint\n",
    "\n",
    "from pipeline import ManualPipelineModule\n",
    "\n",
    "wandb_enable = False\n",
    "\n",
    "TIMESTEP_QUANTILES_FOR_EVAL = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = True):\n",
    "        pass\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Monkeypatch this so it counts all layer parameters, not just trainable parameters.\n",
    "# This helps it divide the layers between GPUs more evenly when training a LoRA.\n",
    "def _count_all_layer_params(self):\n",
    "    param_counts = [0] * len(self._layer_specs)\n",
    "    for idx, layer in enumerate(self._layer_specs):\n",
    "        if isinstance(layer, ds_pipe_module.LayerSpec):\n",
    "            l = layer.build()\n",
    "            param_counts[idx] = sum(p.numel() for p in l.parameters())\n",
    "        elif isinstance(layer, nn.Module):\n",
    "            param_counts[idx] = sum(p.numel() for p in layer.parameters())\n",
    "    return param_counts\n",
    "ds_pipe_module.PipelineModule._count_layer_params = _count_all_layer_params\n",
    "\n",
    "def get_most_recent_run_dir(output_dir):\n",
    "    return list(sorted(glob.glob(os.path.join(output_dir, '*'))))[-1]\n",
    "\n",
    "\n",
    "def print_model_info(model):\n",
    "    if not is_main_process():\n",
    "        return\n",
    "    print(model)\n",
    "    for name, module in model.named_modules():\n",
    "        print(f'{type(module)}: {name}')\n",
    "        for pname, p in module.named_parameters(recurse=False):\n",
    "            print(pname)\n",
    "            print(p.dtype)\n",
    "            print(p.device)\n",
    "            print(p.requires_grad)\n",
    "            print()\n",
    "\n",
    "\n",
    "# Need to preload all micro batches since pulling from the dataloader does IPC between the\n",
    "# first and last stage. Can't do that during the train or inference pipeline schedule execution\n",
    "# because it conflicts with the send / recv steps.\n",
    "def get_data_iterator_for_step(dataloader, engine, num_micro_batches=None):\n",
    "    num_micro_batches = num_micro_batches or engine.micro_batches\n",
    "    if not (engine.is_first_stage() or engine.is_last_stage()):\n",
    "        return None\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    items = [next(dataloader_iter) for _ in range(num_micro_batches)]\n",
    "    return iter(items)\n",
    "\n",
    "\n",
    "def evaluate_single(model_engine, eval_dataloader, eval_gradient_accumulation_steps, quantile, pbar=None):\n",
    "    eval_dataloader.set_eval_quantile(quantile)\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        model_engine.reset_activation_shape()\n",
    "        iterator = get_data_iterator_for_step(eval_dataloader, model_engine, num_micro_batches=eval_gradient_accumulation_steps)\n",
    "        loss = model_engine.eval_batch(iterator, num_micro_batches=eval_gradient_accumulation_steps).item()\n",
    "        eval_dataloader.sync_epoch()\n",
    "        if pbar:\n",
    "            pbar.update(1)\n",
    "        total_loss += loss\n",
    "        count += 1\n",
    "        if eval_dataloader.epoch == 2:\n",
    "            break\n",
    "\n",
    "    eval_dataloader.reset()\n",
    "    return total_loss / count\n",
    "\n",
    "\n",
    "def _evaluate(model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps):\n",
    "    pbar_total = 0\n",
    "    for eval_dataloader in eval_dataloaders.values():\n",
    "        pbar_total += len(eval_dataloader) * len(TIMESTEP_QUANTILES_FOR_EVAL) // eval_gradient_accumulation_steps\n",
    "    if is_main_process():\n",
    "        print('Running eval')\n",
    "        pbar = tqdm(total=pbar_total)\n",
    "    else:\n",
    "        pbar = None\n",
    "\n",
    "    start = time.time()\n",
    "    for name, eval_dataloader in eval_dataloaders.items():\n",
    "        losses = []\n",
    "        for quantile in TIMESTEP_QUANTILES_FOR_EVAL:\n",
    "            loss = evaluate_single(model_engine, eval_dataloader, eval_gradient_accumulation_steps, quantile, pbar=pbar)\n",
    "            losses.append(loss)\n",
    "            if is_main_process():\n",
    "                tb_writer.add_scalar(f'{name}/loss_quantile_{quantile:.2f}', loss, step)\n",
    "                if wandb_enable:\n",
    "                    wandb.log({f'{name}/loss_quantile_{quantile:.2f}': loss, 'step': step})\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        if is_main_process():\n",
    "            tb_writer.add_scalar(f'{name}/loss', avg_loss, step)\n",
    "            if wandb_enable:\n",
    "                wandb.log({f'{name}/loss': avg_loss, 'step': step})\n",
    "\n",
    "    duration = time.time() - start\n",
    "    if is_main_process():\n",
    "        tb_writer.add_scalar('eval/eval_time_sec', duration, step)\n",
    "        if wandb_enable:\n",
    "            wandb.log({'eval/eval_time_sec': duration, 'step': step})\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "def evaluate(model, model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps, disable_block_swap):\n",
    "    if len(eval_dataloaders) == 0:\n",
    "        return\n",
    "    empty_cuda_cache()\n",
    "    model.prepare_block_swap_inference(disable_block_swap=disable_block_swap)\n",
    "    with torch.no_grad(), isolate_rng():\n",
    "        seed = get_rank()\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        _evaluate(model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps)\n",
    "    empty_cuda_cache()\n",
    "    model.prepare_block_swap_training()\n",
    "\n",
    "\n",
    "def distributed_init(args):\n",
    "    \"\"\"Initialize distributed training environment.\"\"\"\n",
    "    world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "    rank = int(os.getenv('RANK', '0'))\n",
    "    local_rank = args.local_rank\n",
    "    # GPU 4개로 학습 시 world_size = 4\n",
    "    # GPU 0번 → rank=0, GPU 3번 → rank=3 cuda:3\n",
    "    # 하나의 머신에 GPU 4개: local_rank는 0,1,2,3\n",
    "    # 여러 머신이면 각 머신 안의 로컬 GPU 번호\n",
    "    # Set environment variables for distributed training\n",
    "    os.environ['MASTER_ADDR'] = os.getenv('MASTER_ADDR', 'localhost')\n",
    "    os.environ['MASTER_PORT'] = str(args.master_port)\n",
    "\n",
    "    return world_size, rank, local_rank\n",
    "\n",
    "\n",
    "def get_prodigy_d(optimizer):\n",
    "    d = 0\n",
    "    for group in optimizer.param_groups:\n",
    "        d += group['d']\n",
    "    return d / len(optimizer.param_groups)\n",
    "\n",
    "\n",
    "def _get_automagic_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            state = optimizer.state[p]\n",
    "            lr = optimizer._get_lr(group, state)\n",
    "            lrs.append(lr)\n",
    "    lrs = torch.stack(lrs)\n",
    "    return lrs, lrs.mean()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SKbdVqo6HyI4",
    "outputId": "e8484718-3a35-4c89-c48b-5ff6e5202824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 11:24:09,401] [INFO] [comm.py:676:init_distributed] cdb=None\n",
      "[2025-07-25 11:24:09,402] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# output_dir = '/content/processed_data/video_1753273997/lora'\n",
    "# dataset = '/content/processed_data/video_1753273997/configs/dataset.toml'\n",
    "# from deepspeed import comm as dist\n",
    "import torch.distributed as dist\n",
    "\n",
    "apply_patches()\n",
    "\n",
    "# needed for broadcasting Queue in dataset.py\n",
    "mp.current_process().authkey = b'afsaskgfdjh4'\n",
    "\n",
    "with open(args.config) as f:\n",
    "    config = json.loads(json.dumps(toml.load(f)))\n",
    "\n",
    "set_config_defaults(config)\n",
    "common.AUTOCAST_DTYPE = config['model']['dtype']\n",
    "\n",
    "# Initialize distributed environment before deepspeed -> # dist.init_process_group()\n",
    "# world_size, rank, local_rank = distributed_init(args)\n",
    "local_rank = args.local_rank\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "rank = int(os.getenv('RANK', '0'))\n",
    "os.environ['MASTER_ADDR'] = os.getenv('MASTER_ADDR', 'localhost')\n",
    "os.environ['MASTER_PORT'] = str(args.master_port)\n",
    "\n",
    "# Now initialize deepspeed\n",
    "deepspeed.init_distributed()\n",
    "# deepspeed.init_distributed(dist_init_required=True)\n",
    "\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "# 수동으로 분산 초기화\n",
    "# dist.init_process_group(backend='nccl')\n",
    "\n",
    "# needed for broadcasting Queue in dataset.py\n",
    "torch.cuda.set_device(dist.get_rank())\n",
    "\n",
    "resume_from_checkpoint = (\n",
    "    args.resume_from_checkpoint if args.resume_from_checkpoint is not None\n",
    "    else config.get('resume_from_checkpoint', False)\n",
    ")\n",
    "regenerate_cache = (\n",
    "    args.regenerate_cache if args.regenerate_cache is not None\n",
    "    else config.get('regenerate_cache', False)\n",
    ")\n",
    "\n",
    "model_type = config['model']['type'] #type = 'wan'\n",
    "\n",
    "if model_type == 'flux':\n",
    "    from models import flux\n",
    "    model = flux.FluxPipeline(config)\n",
    "elif model_type == 'ltx-video':\n",
    "    from models import ltx_video\n",
    "    model = ltx_video.LTXVideoPipeline(config)\n",
    "elif model_type == 'hunyuan-video':\n",
    "    from models import hunyuan_video\n",
    "    model = hunyuan_video.HunyuanVideoPipeline(config)\n",
    "elif model_type == 'sdxl':\n",
    "    from models import sdxl\n",
    "    model = sdxl.SDXLPipeline(config)\n",
    "elif model_type == 'cosmos':\n",
    "    from models import cosmos\n",
    "    model = cosmos.CosmosPipeline(config)\n",
    "elif model_type == 'lumina_2':\n",
    "    from models import lumina_2\n",
    "    model = lumina_2.Lumina2Pipeline(config)\n",
    "elif model_type == 'wan':\n",
    "    # from models import wan\n",
    "    # model = wan.WanPipeline(config)\n",
    "    model = WanPipeline(config)\n",
    "elif model_type == 'chroma':\n",
    "    from models import chroma\n",
    "    model = chroma.ChromaPipeline(config)\n",
    "elif model_type == 'hidream':\n",
    "    from models import hidream\n",
    "    model = hidream.HiDreamPipeline(config)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {model_type} is not implemented')\n",
    "\n",
    "\n",
    "with open(config['dataset']) as f:\n",
    "    dataset_config = toml.load(f)\n",
    "gradient_release = config['optimizer'].get('gradient_release', False)\n",
    "ds_config = {\n",
    "    'train_micro_batch_size_per_gpu': config.get('micro_batch_size_per_gpu', 1),\n",
    "    'gradient_accumulation_steps': config.get('gradient_accumulation_steps', 1),\n",
    "    # Can't do gradient clipping with gradient release, since there are no grads at the end of the step anymore.\n",
    "    'gradient_clipping': 0. if gradient_release else config.get('gradient_clipping', 1.0),\n",
    "    'steps_per_print': config.get('steps_per_print', 1),\n",
    "}\n",
    "caching_batch_size = config.get('caching_batch_size', 1)\n",
    "dataset_manager = dataset_util.DatasetManager(model, regenerate_cache=regenerate_cache, caching_batch_size=caching_batch_size)\n",
    "\n",
    "train_data = dataset_util.Dataset(dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "dataset_manager.register(train_data)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': '/workspace/processed_data/video_1753442489/lora',\n",
       " 'dataset': '/workspace/processed_data/video_1753442489/configs/dataset.toml',\n",
       " 'epochs': 100,\n",
       " 'micro_batch_size_per_gpu': 1,\n",
       " 'pipeline_stages': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'gradient_clipping': 1,\n",
       " 'warmup_steps': 0,\n",
       " 'eval_every_n_epochs': 1000000,\n",
       " 'eval_before_first_step': True,\n",
       " 'eval_micro_batch_size_per_gpu': 1,\n",
       " 'eval_gradient_accumulation_steps': 1,\n",
       " 'save_every_n_epochs': 50,\n",
       " 'checkpoint_every_n_minutes': 1000000000,\n",
       " 'activation_checkpointing': 'unsloth',\n",
       " 'partition_method': 'parameters',\n",
       " 'save_dtype': torch.bfloat16,\n",
       " 'caching_batch_size': 1,\n",
       " 'steps_per_print': 1,\n",
       " 'video_clip_mode': 'single_beginning',\n",
       " 'blocks_to_swap': 32,\n",
       " 'model': {'type': 'wan',\n",
       "  'ckpt_path': '/workspace/Wan2.1-T2V-1.3B',\n",
       "  'dtype': torch.bfloat16,\n",
       "  'transformer_dtype': torch.bfloat16,\n",
       "  'timestep_sample_method': 'uniform',\n",
       "  'guidance': 1.0},\n",
       " 'adapter': {'type': 'lora',\n",
       "  'rank': 16,\n",
       "  'dtype': torch.bfloat16,\n",
       "  'exclude_linear_modules': ['k_img', 'v_img'],\n",
       "  'alpha': 16,\n",
       "  'dropout': 0.0},\n",
       " 'optimizer': {'type': 'AdamW',\n",
       "  'lr': 0.001,\n",
       "  'betas': [0.9, 0.99],\n",
       "  'weight_decay': 0.01},\n",
       " 'reentrant_activation_checkpointing': True,\n",
       " 'logging_steps': 1,\n",
       " 'eval_datasets': [],\n",
       " 'eval_every_n_steps': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_data_map = {}\n",
    "#if eval_list = [], for 문 passed\n",
    "for i, eval_dataset in enumerate(config['eval_datasets']):\n",
    "    if type(eval_dataset) == str:\n",
    "        name = f'eval{i}'\n",
    "        config_path = eval_dataset\n",
    "    else:\n",
    "        name = eval_dataset['name']\n",
    "        config_path = eval_dataset['config']\n",
    "    with open(config_path) as f:\n",
    "        eval_dataset_config = toml.load(f)\n",
    "    eval_data_map[name] = dataset_util.Dataset(eval_dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "    dataset_manager.register(eval_data_map[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using framerate=16\n",
      "caching metadata\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afde27dd9dd4433b909031fef4af294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 11:25:44,270] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00012.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,275] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00029.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,285] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00031.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,295] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00045.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,301] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00017.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,310] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00028.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,319] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00036.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,318] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00006.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,323] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00025.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,326] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00034.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,331] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00010.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,333] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00030.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,335] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00015.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,337] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00039.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,343] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00035.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,343] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00048.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,345] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00009.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,347] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00043.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,351] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00044.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,352] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00005.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,353] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00032.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,354] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00023.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,360] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00011.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,360] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00004.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,360] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00000.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,361] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00019.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,364] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00018.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,366] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00016.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,368] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00002.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,369] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00020.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,371] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00041.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,373] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00042.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,375] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00001.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,376] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00040.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,377] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00003.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,380] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00047.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,381] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00007.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,386] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00027.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,386] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00038.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,388] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00033.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,389] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00014.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,389] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00013.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,393] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00024.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,396] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00037.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,397] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00021.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,397] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00046.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,402] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00026.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,420] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00022.jpg. Using empty caption.\n",
      "[2025-07-25 11:25:44,421] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753442489/traindata/00008.jpg. Using empty caption.\n",
      "caching latents: /workspace/processed_data/video_1753442489/traindata\n",
      "caching latents: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512e2cde63534ce6b12485826f385e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching text embeddings: /workspace/processed_data/video_1753442489/traindata\n",
      "caching text embeddings: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bf406871524809b9c9e7984c8d62a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb8666395934f3783b9e89ee661c88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching metadata\n",
      "caching latents: /workspace/processed_data/video_1753442489/traindata\n",
      "caching latents: (None, None, None)\n",
      "caching text embeddings: /workspace/processed_data/video_1753442489/traindata\n",
      "caching text embeddings: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9743a973f6ad4ab2943c7d4f0618ba36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:248: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'wan.modules.model.WanModel'>.load_config(...) followed by <class 'wan.modules.model.WanModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(pipe=0, data=0): 0}\n",
      "[2025-07-25 11:26:41,702] [INFO] [module.py:398:_partition_layers] Partitioning pipeline stages with method parameters\n",
      "stage=0 layers=32\n",
      "     0: InitialLayer\n",
      "     1: TransformerLayer\n",
      "     2: TransformerLayer\n",
      "     3: TransformerLayer\n",
      "     4: TransformerLayer\n",
      "     5: TransformerLayer\n",
      "     6: TransformerLayer\n",
      "     7: TransformerLayer\n",
      "     8: TransformerLayer\n",
      "     9: TransformerLayer\n",
      "    10: TransformerLayer\n",
      "    11: TransformerLayer\n",
      "    12: TransformerLayer\n",
      "    13: TransformerLayer\n",
      "    14: TransformerLayer\n",
      "    15: TransformerLayer\n",
      "    16: TransformerLayer\n",
      "    17: TransformerLayer\n",
      "    18: TransformerLayer\n",
      "    19: TransformerLayer\n",
      "    20: TransformerLayer\n",
      "    21: TransformerLayer\n",
      "    22: TransformerLayer\n",
      "    23: TransformerLayer\n",
      "    24: TransformerLayer\n",
      "    25: TransformerLayer\n",
      "    26: TransformerLayer\n",
      "    27: TransformerLayer\n",
      "    28: TransformerLayer\n",
      "    29: TransformerLayer\n",
      "    30: TransformerLayer\n",
      "    31: FinalLayer\n",
      "  loss: loss_fn\n",
      "[2025-07-25 11:26:42,698] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-07-25 11:26:42,699] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n",
      "[2025-07-25 11:26:42,723] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********\n",
      "\t self.dp_world_size=1\n",
      "\t self.mp_world_size=1\n",
      "\t self.seq_dp_world_size=1\n",
      "\t self.sequence_parallel_size=1\n",
      "***********************************************\n",
      "[2025-07-25 11:26:42,793] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-07-25 11:26:42,796] [INFO] [logging.py:107:log_dist] [Rank 0] Using client callable to create basic optimizer\n",
      "[2025-07-25 11:26:42,797] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-07-25 11:26:42,869] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-07-25 11:26:42,870] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2025-07-25 11:26:42,870] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-07-25 11:26:42,871] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-07-25 11:26:42,872] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "[2025-07-25 11:26:42,875] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\n",
      "[2025-07-25 11:26:42,875] [INFO] [config.py:954:print] DeepSpeedEngine configuration:\n",
      "[2025-07-25 11:26:42,876] [INFO] [config.py:958:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-07-25 11:26:42,877] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-07-25 11:26:42,878] [INFO] [config.py:958:print]   amp_enabled .................. False\n",
      "[2025-07-25 11:26:42,878] [INFO] [config.py:958:print]   amp_params ................... False\n",
      "[2025-07-25 11:26:42,879] [INFO] [config.py:958:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-07-25 11:26:42,880] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False\n",
      "[2025-07-25 11:26:42,881] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\n",
      "[2025-07-25 11:26:42,881] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-07-25 11:26:42,882] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-07-25 11:26:42,883] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-07-25 11:26:42,883] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7407f551e3d0>\n",
      "[2025-07-25 11:26:42,884] [INFO] [config.py:958:print]   communication_data_type ...... None\n",
      "[2025-07-25 11:26:42,885] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False\n",
      "[2025-07-25 11:26:42,885] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-07-25 11:26:42,886] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False\n",
      "[2025-07-25 11:26:42,887] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False\n",
      "[2025-07-25 11:26:42,887] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-07-25 11:26:42,890] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False\n",
      "[2025-07-25 11:26:42,891] [INFO] [config.py:958:print]   dataloader_drop_last ......... False\n",
      "[2025-07-25 11:26:42,891] [INFO] [config.py:958:print]   disable_allgather ............ False\n",
      "[2025-07-25 11:26:42,892] [INFO] [config.py:958:print]   dump_state ................... False\n",
      "[2025-07-25 11:26:42,893] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False\n",
      "[2025-07-25 11:26:42,893] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-07-25 11:26:42,894] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-07-25 11:26:42,895] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-07-25 11:26:42,895] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-07-25 11:26:42,896] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-07-25 11:26:42,897] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-07-25 11:26:42,897] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False\n",
      "[2025-07-25 11:26:42,898] [INFO] [config.py:958:print]   elasticity_enabled ........... False\n",
      "[2025-07-25 11:26:42,899] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\n",
      "[2025-07-25 11:26:42,899] [INFO] [config.py:958:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-07-25 11:26:42,900] [INFO] [config.py:958:print]   global_rank .................. 0\n",
      "[2025-07-25 11:26:42,901] [INFO] [config.py:958:print]   grad_accum_dtype ............. None\n",
      "[2025-07-25 11:26:42,901] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 1\n",
      "[2025-07-25 11:26:42,902] [INFO] [config.py:958:print]   gradient_clipping ............ 1\n",
      "[2025-07-25 11:26:42,903] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-07-25 11:26:42,903] [INFO] [config.py:958:print]   graph_harvesting ............. False\n",
      "[2025-07-25 11:26:42,904] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-07-25 11:26:42,905] [INFO] [config.py:958:print]   load_universal_checkpoint .... False\n",
      "[2025-07-25 11:26:42,905] [INFO] [config.py:958:print]   memory_breakdown ............. False\n",
      "[2025-07-25 11:26:42,906] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False\n",
      "[2025-07-25 11:26:42,907] [INFO] [config.py:958:print]   mics_shard_size .............. -1\n",
      "[2025-07-25 11:26:42,908] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-07-25 11:26:42,908] [INFO] [config.py:958:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-07-25 11:26:42,909] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-07-25 11:26:42,910] [INFO] [config.py:958:print]   optimizer_name ............... None\n",
      "[2025-07-25 11:26:42,910] [INFO] [config.py:958:print]   optimizer_params ............. None\n",
      "[2025-07-25 11:26:42,911] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-07-25 11:26:42,921] [INFO] [config.py:958:print]   pld_enabled .................. False\n",
      "[2025-07-25 11:26:42,922] [INFO] [config.py:958:print]   pld_params ................... False\n",
      "[2025-07-25 11:26:42,923] [INFO] [config.py:958:print]   prescale_gradients ........... False\n",
      "[2025-07-25 11:26:42,924] [INFO] [config.py:958:print]   scheduler_name ............... None\n",
      "[2025-07-25 11:26:42,925] [INFO] [config.py:958:print]   scheduler_params ............. None\n",
      "[2025-07-25 11:26:42,926] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-07-25 11:26:42,926] [INFO] [config.py:958:print]   sparse_attention ............. None\n",
      "[2025-07-25 11:26:42,927] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False\n",
      "[2025-07-25 11:26:42,928] [INFO] [config.py:958:print]   steps_per_print .............. 1\n",
      "[2025-07-25 11:26:42,928] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-07-25 11:26:42,929] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-07-25 11:26:42,930] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None\n",
      "[2025-07-25 11:26:42,932] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False\n",
      "[2025-07-25 11:26:42,933] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None\n",
      "[2025-07-25 11:26:42,933] [INFO] [config.py:958:print]   train_batch_size ............. 1\n",
      "[2025-07-25 11:26:42,934] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-07-25 11:26:42,935] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False\n",
      "[2025-07-25 11:26:42,935] [INFO] [config.py:958:print]   use_node_local_storage ....... False\n",
      "[2025-07-25 11:26:42,936] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False\n",
      "[2025-07-25 11:26:42,938] [INFO] [config.py:958:print]   weight_quantization_config ... None\n",
      "[2025-07-25 11:26:42,939] [INFO] [config.py:958:print]   world_size ................... 1\n",
      "[2025-07-25 11:26:42,939] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False\n",
      "[2025-07-25 11:26:42,940] [INFO] [config.py:958:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-07-25 11:26:42,941] [INFO] [config.py:958:print]   zero_enabled ................. False\n",
      "[2025-07-25 11:26:42,943] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-07-25 11:26:42,943] [INFO] [config.py:958:print]   zero_optimization_stage ...... 0\n",
      "[2025-07-25 11:26:42,944] [INFO] [config.py:944:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1, \n",
      "    \"steps_per_print\": 1\n",
      "}\n",
      "[2025-07-25 11:26:42,945] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=1 micro_batch_size=1\n",
      "[2025-07-25 11:26:42,946] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-07-25 11:26:43,001] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=32 [0, 32) STAGE_PARAMS=1137079360 (1137.079M) TOTAL_PARAMS=1137079360 (1137.079M) UNIQUE_PARAMS=1137079360 (1137.079M)\n"
     ]
    }
   ],
   "source": [
    "from unsloth_utils import unsloth_checkpoint\n",
    "# if args.dump_dataset:\n",
    "#     # only works for flux 코드 있는데 지움\n",
    "\n",
    "\n",
    "dataset_manager.cache()\n",
    "if args.cache_only:\n",
    "    quit()\n",
    "\n",
    "model.load_diffusion_model()\n",
    "\n",
    "# [adapter]\n",
    "# type = 'lora'\n",
    "# rank = 16\n",
    "# dtype = 'bfloat16'\n",
    "# exclude_linear_modules = [\"k_img\", \"v_img\"]\n",
    "\n",
    "# if adapter_config := config.get('adapter', None):\n",
    "#     model.configure_adapter(adapter_config)\n",
    "#     is_adapter = True\n",
    "#     if init_from_existing := adapter_config.get('init_from_existing', None):\n",
    "#         model.load_adapter_weights(init_from_existing)\n",
    "# else:\n",
    "#     is_adapter = False\n",
    "\n",
    "# if this is a new run, create a new dir for it\n",
    "if not resume_from_checkpoint and is_main_process():\n",
    "    run_dir = os.path.join(config['output_dir'], datetime.now(timezone.utc).strftime('%Y%m%d_%H-%M-%S'))\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    shutil.copy(args.config, run_dir)\n",
    "    shutil.copy(config['dataset'], run_dir)\n",
    "    for eval_dataset in config['eval_datasets']:\n",
    "        shutil.copy(eval_dataset['config'], run_dir)\n",
    "# wait for all processes then get the most recent dir (may have just been created)\n",
    "dist.barrier()\n",
    "if resume_from_checkpoint is True:  # No specific folder provided, use most recent\n",
    "    run_dir = get_most_recent_run_dir(config['output_dir'])\n",
    "elif isinstance(resume_from_checkpoint, str):  # Specific folder provided\n",
    "    run_dir = os.path.join(config['output_dir'], resume_from_checkpoint)\n",
    "    if not os.path.exists(run_dir):\n",
    "        raise ValueError(f\"Checkpoint directory {run_dir} does not exist\")\n",
    "else:  # Not resuming, use most recent (newly created) dir\n",
    "    run_dir = get_most_recent_run_dir(config['output_dir'])\n",
    "\n",
    "# WandB logging\n",
    "wandb_enable = config.get('monitoring', {}).get('enable_wandb', False)\n",
    "if wandb_enable:\n",
    "    wandb_api_key     = config['monitoring']['wandb_api_key']\n",
    "    wandb_tracker     = config['monitoring']['wandb_tracker_name']\n",
    "    wandb_run_name    = config['monitoring']['wandb_run_name']\n",
    "    logging_dir       = run_dir\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    wandb.init(\n",
    "        project=wandb_tracker,\n",
    "        name=wandb_run_name,\n",
    "        config=config,\n",
    "        dir=logging_dir\n",
    "    )\n",
    "\n",
    "# Block swapping\n",
    "# if blocks_to_swap := config.get('blocks_to_swap', 0):\n",
    "#     assert config['pipeline_stages'] == 1, 'Block swapping only works with pipeline_stages=1'\n",
    "#     assert 'adapter' in config, 'Block swapping only works when training LoRA'\n",
    "#     # Don't automatically move to GPU, we'll do that ourselves.\n",
    "#     def to(self, *args, **kwargs):\n",
    "#         pass\n",
    "#     deepspeed.pipe.PipelineModule.to = to\n",
    "#     model.enable_block_swap(blocks_to_swap)\n",
    "\n",
    "layers = model.to_layers()\n",
    "additional_pipeline_module_kwargs = {}\n",
    "activation_checkpointing = config['activation_checkpointing']\n",
    "if activation_checkpointing:\n",
    "    if activation_checkpointing == True:\n",
    "        # TODO: block swapping doesn't work with Deepspeed non-reentrant checkpoint, but PyTorch native one is fine. Some\n",
    "        # weights end up on CPU where they shouldn't. Why? Are we giving anything up by not using the Deepspeed implementation?\n",
    "        #checkpoint_func = deepspeed.checkpointing.non_reentrant_checkpoint\n",
    "        from functools import partial\n",
    "        checkpoint_func = partial(torch.utils.checkpoint.checkpoint, use_reentrant=False)\n",
    "    elif activation_checkpointing == 'unsloth':\n",
    "        checkpoint_func = unsloth_checkpoint\n",
    "    else:\n",
    "        raise NotImplementedError(f'activation_checkpointing={activation_checkpointing} is not implemented')\n",
    "    additional_pipeline_module_kwargs.update({\n",
    "        'activation_checkpoint_interval': 1,\n",
    "        'checkpointable_layers': model.checkpointable_layers,\n",
    "        'activation_checkpoint_func': checkpoint_func,\n",
    "    })\n",
    "\n",
    "num_stages = config.get('pipeline_stages', 1)\n",
    "partition_method=config.get('partition_method', 'parameters')\n",
    "partition_split = config.get('partition_split',[len(layers) / num_stages])\n",
    "pipeline_model = ManualPipelineModule(\n",
    "    layers=layers,\n",
    "    num_stages=num_stages,\n",
    "    partition_method=partition_method,\n",
    "    manual_partition_split=partition_split,\n",
    "    loss_fn=model.get_loss_fn(),\n",
    "    **additional_pipeline_module_kwargs\n",
    ")\n",
    "parameters_to_train = [p for p in pipeline_model.parameters() if p.requires_grad]\n",
    "\n",
    "def get_optimizer(model_parameters):\n",
    "    if len(model_parameters) == 0:\n",
    "        return DummyOptimizer()\n",
    "# [optimizer]\n",
    "# type = 'AdamW8bitKahan'\n",
    "# lr = 0.001\n",
    "# betas = [0.9, 0.99]\n",
    "# weight_decay = 0.01\n",
    "# stabilize = true\n",
    "\n",
    "    optim_config = config['optimizer']\n",
    "    optim_type = optim_config['type']\n",
    "    optim_type_lower = optim_type.lower()\n",
    "\n",
    "    args = []\n",
    "    kwargs = {k: v for k, v in optim_config.items() if k not in ['type', 'gradient_release']}\n",
    "\n",
    "    if optim_type_lower == 'adamw':\n",
    "        # TODO: fix this. I'm getting \"fatal error: cuda_runtime.h: No such file or directory\"\n",
    "        # when Deepspeed tries to build the fused Adam extension.\n",
    "        # klass = deepspeed.ops.adam.FusedAdam\n",
    "        klass = torch.optim.AdamW\n",
    "    elif optim_type_lower == 'adamw8bit':\n",
    "        import bitsandbytes\n",
    "        klass = bitsandbytes.optim.AdamW8bit\n",
    "    elif optim_type_lower == 'adamw_optimi':\n",
    "        import optimi\n",
    "        klass = optimi.AdamW\n",
    "    elif optim_type_lower == 'stableadamw':\n",
    "        import optimi\n",
    "        klass = optimi.StableAdamW\n",
    "    elif optim_type_lower == 'sgd':\n",
    "        klass = torch.optim.SGD\n",
    "    elif optim_type_lower == 'adamw8bitkahan':\n",
    "        import adamw_8bit\n",
    "        klass = adamw_8bit.AdamW8bitKahan\n",
    "    elif optim_type_lower == 'offload':\n",
    "        from torchao.prototype.low_bit_optim import CPUOffloadOptimizer\n",
    "        klass = CPUOffloadOptimizer\n",
    "        args.append(torch.optim.AdamW)\n",
    "        kwargs['fused'] = True\n",
    "    elif optim_type_lower == 'automagic':\n",
    "        from optimizers import automagic\n",
    "        klass = automagic.Automagic\n",
    "    else:\n",
    "        import pytorch_optimizer\n",
    "        klass = getattr(pytorch_optimizer, optim_type)\n",
    "\n",
    "    if optim_config.get('gradient_release', False):\n",
    "        # Prevent deepspeed from logging every single param group lr\n",
    "        def _report_progress(self, step):\n",
    "            lr = self.get_lr()\n",
    "            mom = self.get_mom()\n",
    "            deepspeed.utils.logging.log_dist(f\"step={step}, skipped={self.skipped_steps}, lr={lr[0]}, mom={mom[0]}\", ranks=[0])\n",
    "        deepspeed.runtime.engine.DeepSpeedEngine._report_progress = _report_progress\n",
    "\n",
    "        # Deepspeed executes all the code to reduce grads across data parallel ranks even if the DP world size is 1.\n",
    "        # As part of this, any grads that are None are set to zeros. We're doing gradient release to save memory,\n",
    "        # so we have to avoid this.\n",
    "        def _exec_reduce_grads(self):\n",
    "            assert self.mpu.get_data_parallel_world_size() == 1, 'When using gradient release, data parallel world size must be 1. Make sure pipeline_stages = num_gpus.'\n",
    "            return\n",
    "        deepspeed.runtime.pipe.engine.PipelineEngine._INSTRUCTION_MAP[deepspeed.runtime.pipe.schedule.ReduceGrads] = _exec_reduce_grads\n",
    "\n",
    "        # When pipelining multiple forward and backward passes, normally updating the parameter in-place causes an error when calling\n",
    "        # backward() on future micro-batches. But we can modify .data directly so the autograd engine doesn't detect in-place modifications.\n",
    "        # TODO: this is unbelievably hacky and not mathematically sound, I'm just seeing if it works at all.\n",
    "        def add_(self, *args, **kwargs):\n",
    "            self.data.add_(*args, **kwargs)\n",
    "        for p in model_parameters:\n",
    "            p.add_ = add_.__get__(p)\n",
    "\n",
    "        if 'foreach' in inspect.signature(klass).parameters:\n",
    "            kwargs['foreach'] = False\n",
    "\n",
    "        # We're doing an optimizer step for each micro-batch. Scale momentum and EMA betas so that the contribution\n",
    "        # decays at the same rate it would if we were doing one step per batch like normal.\n",
    "        # Reference: https://alexeytochin.github.io/posts/batch_size_vs_momentum/batch_size_vs_momentum.html\n",
    "        gas = ds_config['gradient_accumulation_steps']\n",
    "        if 'betas' in kwargs:\n",
    "            for i in range(len(kwargs['betas'])):\n",
    "                kwargs['betas'][i] = kwargs['betas'][i] ** (1/gas)\n",
    "        if 'momentum' in kwargs:\n",
    "            kwargs['momentum'] = kwargs['momentum'] ** (1/gas)\n",
    "\n",
    "        optimizer_dict = {}\n",
    "        for pg in model.get_param_groups(model_parameters):\n",
    "            param_kwargs = kwargs.copy()\n",
    "            if isinstance(pg, dict):\n",
    "                # param group\n",
    "                for p in pg['params']:\n",
    "                    param_kwargs['lr'] = pg['lr']\n",
    "                    optimizer_dict[p] = klass([p], **param_kwargs)\n",
    "            else:\n",
    "                # param\n",
    "                optimizer_dict[pg] = klass([pg], **param_kwargs)\n",
    "\n",
    "        def optimizer_hook(p):\n",
    "            optimizer_dict[p].step()\n",
    "            optimizer_dict[p].zero_grad()\n",
    "\n",
    "        for p in model_parameters:\n",
    "            p.register_post_accumulate_grad_hook(optimizer_hook)\n",
    "\n",
    "        from optimizers import gradient_release\n",
    "        return gradient_release.GradientReleaseOptimizerWrapper(list(optimizer_dict.values()))\n",
    "    else:\n",
    "        model_parameters = model.get_param_groups(model_parameters)\n",
    "        return klass(model_parameters, *args, **kwargs)\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    args=args,\n",
    "    model=pipeline_model,\n",
    "    model_parameters=parameters_to_train,\n",
    "    optimizer=get_optimizer,\n",
    "    config=ds_config,\n",
    ")\n",
    "model.model_engine = model_engine\n",
    "if model_engine.is_pipe_parallel:\n",
    "      grid = model_engine.grid\n",
    "      model_engine.first_last_stage_group = dist.new_group(ranks=[grid.pp_group[0], grid.pp_group[-1]])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
    "if config['warmup_steps'] > 0:\n",
    "    warmup_steps = config['warmup_steps']\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1/warmup_steps, total_iters=warmup_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, lr_scheduler], milestones=[warmup_steps])\n",
    "model_engine.lr_scheduler = lr_scheduler\n",
    "\n",
    "train_data.post_init(\n",
    "    model_engine.grid.get_data_parallel_rank(),\n",
    "    model_engine.grid.get_data_parallel_world_size(),\n",
    "    model_engine.train_micro_batch_size_per_gpu(),\n",
    "    model_engine.gradient_accumulation_steps(),\n",
    "    config.get('image_micro_batch_size_per_gpu', model_engine.train_micro_batch_size_per_gpu()),\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)['latents'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 11:27:41,424] [INFO] [logging.py:107:log_dist] [Rank 0] step=1, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 1 loss: 0.2073 iter time (s): 1.239 samples/sec: 0.807\n",
      "[2025-07-25 11:27:42,240] [INFO] [logging.py:107:log_dist] [Rank 0] step=2, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 2 loss: nan iter time (s): 0.804 samples/sec: 1.244\n",
      "[2025-07-25 11:27:43,449] [INFO] [logging.py:107:log_dist] [Rank 0] step=3, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 3 loss: nan iter time (s): 0.679 samples/sec: 1.473\n",
      "[2025-07-25 11:27:44,205] [INFO] [logging.py:107:log_dist] [Rank 0] step=4, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 4 loss: nan iter time (s): 0.743 samples/sec: 1.347\n",
      "[2025-07-25 11:27:44,912] [INFO] [logging.py:107:log_dist] [Rank 0] step=5, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 5 loss: nan iter time (s): 0.701 samples/sec: 1.427\n",
      "[2025-07-25 11:27:45,606] [INFO] [logging.py:107:log_dist] [Rank 0] step=6, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 6 loss: nan iter time (s): 0.680 samples/sec: 1.470\n",
      "[2025-07-25 11:27:46,305] [INFO] [logging.py:107:log_dist] [Rank 0] step=7, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 7 loss: nan iter time (s): 0.686 samples/sec: 1.457\n",
      "[2025-07-25 11:27:47,054] [INFO] [logging.py:107:log_dist] [Rank 0] step=8, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 8 loss: nan iter time (s): 0.737 samples/sec: 1.358\n",
      "[2025-07-25 11:27:47,705] [INFO] [logging.py:107:log_dist] [Rank 0] step=9, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 9 loss: nan iter time (s): 0.646 samples/sec: 1.549\n",
      "[2025-07-25 11:27:48,389] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 10 loss: nan iter time (s): 0.677 samples/sec: 1.476\n",
      "[2025-07-25 11:27:49,088] [INFO] [logging.py:107:log_dist] [Rank 0] step=11, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 11 loss: nan iter time (s): 0.690 samples/sec: 1.448\n",
      "[2025-07-25 11:27:49,788] [INFO] [logging.py:107:log_dist] [Rank 0] step=12, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 12 loss: nan iter time (s): 0.693 samples/sec: 1.443\n",
      "[2025-07-25 11:27:50,486] [INFO] [logging.py:107:log_dist] [Rank 0] step=13, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 13 loss: nan iter time (s): 0.689 samples/sec: 1.451\n",
      "[2025-07-25 11:27:51,189] [INFO] [logging.py:107:log_dist] [Rank 0] step=14, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 14 loss: nan iter time (s): 0.694 samples/sec: 1.441\n",
      "[2025-07-25 11:27:51,890] [INFO] [logging.py:107:log_dist] [Rank 0] step=15, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 15 loss: nan iter time (s): 0.689 samples/sec: 1.451\n",
      "[2025-07-25 11:27:52,587] [INFO] [logging.py:107:log_dist] [Rank 0] step=16, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 16 loss: nan iter time (s): 0.689 samples/sec: 1.451\n",
      "[2025-07-25 11:27:53,280] [INFO] [logging.py:107:log_dist] [Rank 0] step=17, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 17 loss: nan iter time (s): 0.684 samples/sec: 1.461\n",
      "[2025-07-25 11:27:53,988] [INFO] [logging.py:107:log_dist] [Rank 0] step=18, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 18 loss: nan iter time (s): 0.700 samples/sec: 1.429\n",
      "[2025-07-25 11:27:54,688] [INFO] [logging.py:107:log_dist] [Rank 0] step=19, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 19 loss: nan iter time (s): 0.691 samples/sec: 1.447\n",
      "[2025-07-25 11:27:55,390] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 20 loss: nan iter time (s): 0.693 samples/sec: 1.442\n",
      "[2025-07-25 11:27:56,084] [INFO] [logging.py:107:log_dist] [Rank 0] step=21, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 21 loss: nan iter time (s): 0.687 samples/sec: 1.455\n",
      "[2025-07-25 11:27:56,788] [INFO] [logging.py:107:log_dist] [Rank 0] step=22, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 22 loss: nan iter time (s): 0.695 samples/sec: 1.439\n",
      "[2025-07-25 11:27:57,489] [INFO] [logging.py:107:log_dist] [Rank 0] step=23, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 23 loss: nan iter time (s): 0.693 samples/sec: 1.442\n",
      "[2025-07-25 11:27:58,189] [INFO] [logging.py:107:log_dist] [Rank 0] step=24, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 24 loss: nan iter time (s): 0.692 samples/sec: 1.445\n",
      "[2025-07-25 11:27:58,896] [INFO] [logging.py:107:log_dist] [Rank 0] step=25, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 25 loss: nan iter time (s): 0.696 samples/sec: 1.438\n",
      "[2025-07-25 11:27:59,601] [INFO] [logging.py:107:log_dist] [Rank 0] step=26, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 26 loss: nan iter time (s): 0.697 samples/sec: 1.436\n",
      "[2025-07-25 11:28:00,305] [INFO] [logging.py:107:log_dist] [Rank 0] step=27, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 27 loss: nan iter time (s): 0.694 samples/sec: 1.440\n",
      "[2025-07-25 11:28:00,989] [INFO] [logging.py:107:log_dist] [Rank 0] step=28, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 28 loss: nan iter time (s): 0.679 samples/sec: 1.473\n",
      "[2025-07-25 11:28:01,690] [INFO] [logging.py:107:log_dist] [Rank 0] step=29, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 29 loss: nan iter time (s): 0.692 samples/sec: 1.444\n",
      "[2025-07-25 11:28:02,388] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 30 loss: nan iter time (s): 0.691 samples/sec: 1.448\n",
      "[2025-07-25 11:28:03,085] [INFO] [logging.py:107:log_dist] [Rank 0] step=31, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 31 loss: nan iter time (s): 0.688 samples/sec: 1.454\n",
      "[2025-07-25 11:28:03,788] [INFO] [logging.py:107:log_dist] [Rank 0] step=32, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 32 loss: nan iter time (s): 0.696 samples/sec: 1.437\n",
      "[2025-07-25 11:28:04,485] [INFO] [logging.py:107:log_dist] [Rank 0] step=33, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 33 loss: nan iter time (s): 0.688 samples/sec: 1.453\n",
      "[2025-07-25 11:28:05,187] [INFO] [logging.py:107:log_dist] [Rank 0] step=34, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 34 loss: nan iter time (s): 0.693 samples/sec: 1.442\n",
      "[2025-07-25 11:28:05,889] [INFO] [logging.py:107:log_dist] [Rank 0] step=35, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 35 loss: nan iter time (s): 0.694 samples/sec: 1.442\n",
      "[2025-07-25 11:28:06,585] [INFO] [logging.py:107:log_dist] [Rank 0] step=36, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 36 loss: nan iter time (s): 0.688 samples/sec: 1.453\n",
      "[2025-07-25 11:28:07,293] [INFO] [logging.py:107:log_dist] [Rank 0] step=37, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 37 loss: nan iter time (s): 0.700 samples/sec: 1.429\n",
      "[2025-07-25 11:28:07,986] [INFO] [logging.py:107:log_dist] [Rank 0] step=38, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 38 loss: nan iter time (s): 0.686 samples/sec: 1.458\n",
      "[2025-07-25 11:28:08,688] [INFO] [logging.py:107:log_dist] [Rank 0] step=39, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 39 loss: nan iter time (s): 0.692 samples/sec: 1.445\n",
      "[2025-07-25 11:28:09,387] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 40 loss: nan iter time (s): 0.693 samples/sec: 1.443\n",
      "[2025-07-25 11:28:10,086] [INFO] [logging.py:107:log_dist] [Rank 0] step=41, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 41 loss: nan iter time (s): 0.691 samples/sec: 1.447\n",
      "[2025-07-25 11:28:10,788] [INFO] [logging.py:107:log_dist] [Rank 0] step=42, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 42 loss: nan iter time (s): 0.695 samples/sec: 1.439\n",
      "[2025-07-25 11:28:11,486] [INFO] [logging.py:107:log_dist] [Rank 0] step=43, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 43 loss: nan iter time (s): 0.690 samples/sec: 1.449\n",
      "[2025-07-25 11:28:12,187] [INFO] [logging.py:107:log_dist] [Rank 0] step=44, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 44 loss: nan iter time (s): 0.695 samples/sec: 1.439\n",
      "[2025-07-25 11:28:12,883] [INFO] [logging.py:107:log_dist] [Rank 0] step=45, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 45 loss: nan iter time (s): 0.688 samples/sec: 1.454\n",
      "[2025-07-25 11:28:13,573] [INFO] [logging.py:107:log_dist] [Rank 0] step=46, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 46 loss: nan iter time (s): 0.683 samples/sec: 1.464\n",
      "[2025-07-25 11:28:14,276] [INFO] [logging.py:107:log_dist] [Rank 0] step=47, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 47 loss: nan iter time (s): 0.694 samples/sec: 1.440\n",
      "[2025-07-25 11:28:14,974] [INFO] [logging.py:107:log_dist] [Rank 0] step=48, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 48 loss: nan iter time (s): 0.691 samples/sec: 1.446\n",
      "[2025-07-25 11:28:15,670] [INFO] [logging.py:107:log_dist] [Rank 0] step=49, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 49 loss: nan iter time (s): 0.690 samples/sec: 1.449\n",
      "Started new epoch: 2\n",
      "[2025-07-25 11:28:16,497] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 50 loss: nan iter time (s): 0.719 samples/sec: 1.390\n",
      "[2025-07-25 11:28:17,178] [INFO] [logging.py:107:log_dist] [Rank 0] step=51, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 51 loss: nan iter time (s): 0.674 samples/sec: 1.484\n",
      "[2025-07-25 11:28:17,877] [INFO] [logging.py:107:log_dist] [Rank 0] step=52, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 52 loss: nan iter time (s): 0.692 samples/sec: 1.446\n",
      "[2025-07-25 11:28:18,577] [INFO] [logging.py:107:log_dist] [Rank 0] step=53, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 53 loss: nan iter time (s): 0.689 samples/sec: 1.452\n",
      "[2025-07-25 11:28:19,275] [INFO] [logging.py:107:log_dist] [Rank 0] step=54, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 54 loss: nan iter time (s): 0.692 samples/sec: 1.445\n",
      "[2025-07-25 11:28:19,987] [INFO] [logging.py:107:log_dist] [Rank 0] step=55, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 55 loss: nan iter time (s): 0.702 samples/sec: 1.425\n",
      "[2025-07-25 11:28:20,679] [INFO] [logging.py:107:log_dist] [Rank 0] step=56, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 56 loss: nan iter time (s): 0.688 samples/sec: 1.454\n",
      "[2025-07-25 11:28:21,400] [INFO] [logging.py:107:log_dist] [Rank 0] step=57, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 57 loss: nan iter time (s): 0.708 samples/sec: 1.412\n",
      "[2025-07-25 11:28:22,099] [INFO] [logging.py:107:log_dist] [Rank 0] step=58, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 58 loss: nan iter time (s): 0.691 samples/sec: 1.446\n",
      "[2025-07-25 11:28:22,798] [INFO] [logging.py:107:log_dist] [Rank 0] step=59, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 59 loss: nan iter time (s): 0.690 samples/sec: 1.449\n",
      "[2025-07-25 11:28:23,499] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 60 loss: nan iter time (s): 0.693 samples/sec: 1.443\n",
      "[2025-07-25 11:28:24,191] [INFO] [logging.py:107:log_dist] [Rank 0] step=61, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 61 loss: nan iter time (s): 0.685 samples/sec: 1.459\n",
      "[2025-07-25 11:28:24,885] [INFO] [logging.py:107:log_dist] [Rank 0] step=62, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 62 loss: nan iter time (s): 0.687 samples/sec: 1.456\n",
      "[2025-07-25 11:28:25,587] [INFO] [logging.py:107:log_dist] [Rank 0] step=63, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 63 loss: nan iter time (s): 0.694 samples/sec: 1.441\n",
      "[2025-07-25 11:28:26,285] [INFO] [logging.py:107:log_dist] [Rank 0] step=64, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 64 loss: nan iter time (s): 0.690 samples/sec: 1.448\n",
      "[2025-07-25 11:28:27,000] [INFO] [logging.py:107:log_dist] [Rank 0] step=65, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 65 loss: nan iter time (s): 0.703 samples/sec: 1.422\n",
      "[2025-07-25 11:28:27,698] [INFO] [logging.py:107:log_dist] [Rank 0] step=66, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 66 loss: nan iter time (s): 0.691 samples/sec: 1.447\n",
      "[2025-07-25 11:28:28,403] [INFO] [logging.py:107:log_dist] [Rank 0] step=67, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 67 loss: nan iter time (s): 0.697 samples/sec: 1.435\n",
      "[2025-07-25 11:28:29,100] [INFO] [logging.py:107:log_dist] [Rank 0] step=68, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 68 loss: nan iter time (s): 0.690 samples/sec: 1.450\n",
      "[2025-07-25 11:28:29,803] [INFO] [logging.py:107:log_dist] [Rank 0] step=69, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 69 loss: nan iter time (s): 0.693 samples/sec: 1.442\n",
      "[2025-07-25 11:28:30,503] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 70 loss: nan iter time (s): 0.692 samples/sec: 1.446\n",
      "[2025-07-25 11:28:31,190] [INFO] [logging.py:107:log_dist] [Rank 0] step=71, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 71 loss: nan iter time (s): 0.683 samples/sec: 1.465\n",
      "[2025-07-25 11:28:31,885] [INFO] [logging.py:107:log_dist] [Rank 0] step=72, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 72 loss: nan iter time (s): 0.688 samples/sec: 1.453\n",
      "[2025-07-25 11:28:32,590] [INFO] [logging.py:107:log_dist] [Rank 0] step=73, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 73 loss: nan iter time (s): 0.697 samples/sec: 1.435\n",
      "[2025-07-25 11:28:33,290] [INFO] [logging.py:107:log_dist] [Rank 0] step=74, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 74 loss: nan iter time (s): 0.693 samples/sec: 1.442\n",
      "[2025-07-25 11:28:33,986] [INFO] [logging.py:107:log_dist] [Rank 0] step=75, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 75 loss: nan iter time (s): 0.688 samples/sec: 1.453\n",
      "[2025-07-25 11:28:34,676] [INFO] [logging.py:107:log_dist] [Rank 0] step=76, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n",
      "steps: 76 loss: nan iter time (s): 0.683 samples/sec: 1.464\n",
      "[2025-07-25 11:28:35,379] [INFO] [logging.py:107:log_dist] [Rank 0] step=77, skipped=0, lr=[0.001], mom=[[0.9, 0.99]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Might be useful because we set things in fp16 / bf16 without explicitly enabling Deepspeed fp16 mode.\n",
    "# Unsure if really needed.\n",
    "communication_data_type = config['lora']['dtype'] if 'lora' in config else config['model']['dtype']\n",
    "model_engine.communication_data_type = communication_data_type\n",
    "\n",
    "train_dataloader = dataset_util.PipelineDataLoader(train_data, model_engine, model_engine.gradient_accumulation_steps(), model)\n",
    "\n",
    "step = 1\n",
    "# make sure to do this before calling model_engine.set_dataloader(), as that method creates an iterator\n",
    "# which starts creating dataloader internal state\n",
    "if resume_from_checkpoint:\n",
    "    load_path, client_state = model_engine.load_checkpoint(\n",
    "        run_dir,\n",
    "        load_module_strict=False,\n",
    "        load_lr_scheduler_states='force_constant_lr' not in config,\n",
    "    )\n",
    "    dist.barrier()  # just so the print below doesn't get swamped\n",
    "    assert load_path is not None\n",
    "    train_dataloader.load_state_dict(client_state['custom_loader'])\n",
    "    step = client_state['step'] + 1\n",
    "    del client_state\n",
    "    if is_main_process():\n",
    "        print(f'Resuming training from checkpoint. Resuming at epoch: {train_dataloader.epoch}, step: {step}')\n",
    "\n",
    "if 'force_constant_lr' in config:\n",
    "    model_engine.lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = config['force_constant_lr']\n",
    "\n",
    "steps_per_epoch = len(train_dataloader) // model_engine.gradient_accumulation_steps()\n",
    "model_engine.total_steps = steps_per_epoch * config['epochs']\n",
    "\n",
    "eval_dataloaders = {\n",
    "    # Set num_dataloader_workers=0 so dataset iteration is completely deterministic.\n",
    "    # We want the exact same noise for each image, each time, for a stable validation loss.\n",
    "    name: dataset_util.PipelineDataLoader(eval_data, model_engine, config['eval_gradient_accumulation_steps'], model, num_dataloader_workers=0)\n",
    "    for name, eval_data in eval_data_map.items()\n",
    "}\n",
    "from saver import Saver\n",
    "is_adapter = False\n",
    "epoch = train_dataloader.epoch\n",
    "tb_writer = SummaryWriter(log_dir=run_dir) if is_main_process() else None\n",
    "saver = Saver(args, config, is_adapter, run_dir, model, train_dataloader, model_engine, pipeline_model)\n",
    "\n",
    "disable_block_swap_for_eval = config.get('disable_block_swap_for_eval', False)\n",
    "if config['eval_before_first_step'] and not resume_from_checkpoint:\n",
    "    evaluate(model, model_engine, eval_dataloaders, tb_writer, 0, config['eval_gradient_accumulation_steps'], disable_block_swap_for_eval)\n",
    "\n",
    "# TODO: this is state we need to save and resume when resuming from checkpoint. It only affects logging.\n",
    "epoch_loss = 0\n",
    "num_steps = 0\n",
    "while True:\n",
    "    #empty_cuda_cache()\n",
    "    model_engine.reset_activation_shape()\n",
    "    iterator = get_data_iterator_for_step(train_dataloader, model_engine)\n",
    "    loss = model_engine.train_batch(iterator).item()\n",
    "    epoch_loss += loss\n",
    "    num_steps += 1\n",
    "    train_dataloader.sync_epoch()\n",
    "\n",
    "    new_epoch, checkpointed, saved = saver.process_epoch(epoch, step)\n",
    "    finished_epoch = True if new_epoch != epoch else False\n",
    "\n",
    "    if is_main_process() and step % config['logging_steps'] == 0:\n",
    "        tb_writer.add_scalar(f'train/loss', loss, step)\n",
    "        if wandb_enable:\n",
    "            wandb.log({'train/loss': loss, 'step': step})\n",
    "        if optimizer.__class__.__name__ == 'Prodigy':\n",
    "            prodigy_d = get_prodigy_d(optimizer)\n",
    "            tb_writer.add_scalar(f'train/prodigy_d', prodigy_d, step)\n",
    "        if optimizer.__class__.__name__ == 'Automagic':\n",
    "            lrs, avg_lr = _get_automagic_lrs(optimizer)\n",
    "            tb_writer.add_histogram(f'train/automagic_lrs', lrs, step)\n",
    "            tb_writer.add_scalar(f'train/automagic_avg_lr', avg_lr, step)\n",
    "\n",
    "    if (config['eval_every_n_steps'] and step % config['eval_every_n_steps'] == 0) or (finished_epoch and config['eval_every_n_epochs'] and epoch % config['eval_every_n_epochs'] == 0):\n",
    "        evaluate(model, model_engine, eval_dataloaders, tb_writer, step, config['eval_gradient_accumulation_steps'], disable_block_swap_for_eval)\n",
    "\n",
    "    if finished_epoch:\n",
    "        if is_main_process():\n",
    "            tb_writer.add_scalar(f'train/epoch_loss', epoch_loss/num_steps, epoch)\n",
    "            if wandb_enable:\n",
    "                wandb.log({'train/epoch_loss': epoch_loss/num_steps, 'epoch': epoch})\n",
    "        epoch_loss = 0\n",
    "        num_steps = 0\n",
    "        epoch = new_epoch\n",
    "        if epoch is None:\n",
    "            break\n",
    "\n",
    "    saver.process_step(step)\n",
    "    step += 1\n",
    "\n",
    "# Save final training state checkpoint and model, unless we just saved them.\n",
    "if not checkpointed:\n",
    "    saver.save_checkpoint(step)\n",
    "if not saved:\n",
    "    saver.save_model(f'epoch{epoch}')\n",
    "\n",
    "if is_main_process():\n",
    "    print('TRAINING COMPLETE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart\n"
     ]
    }
   ],
   "source": [
    "print('restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import dataset as dataset_util\n",
    "with open(config['dataset']) as f:\n",
    "    dataset_config = toml.load(f)\n",
    "radient_release = config['optimizer'].get('gradient_release', False)\n",
    "ds_config = {\n",
    "    'train_micro_batch_size_per_gpu': config.get('micro_batch_size_per_gpu', 1),\n",
    "    'gradient_accumulation_steps': config.get('gradient_accumulation_steps', 1),\n",
    "    # Can't do gradient clipping with gradient release, since there are no grads at the end of the step anymore.\n",
    "    # 'gradient_clipping': 0. if gradient_release else config.get('gradient_clipping', 1.0),\n",
    "    'steps_per_print': config.get('steps_per_print', 1),\n",
    "}\n",
    "caching_batch_size = config.get('caching_batch_size', 1)\n",
    "dataset_manager = dataset_util.DatasetManager(model, regenerate_cache=regenerate_cache, caching_batch_size=caching_batch_size)\n",
    "train_data = dataset_util.Dataset(dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
