{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8zV1ErNM1dC"
   },
   "source": [
    "# prompt: drive.mount\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 26 05:25:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4500               On  |   00000000:C2:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             25W /  200W |       2MiB /  20470MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quF7Md5zLGbi",
    "outputId": "4184351a-ccb1-4c78-ed75-efa487eb7d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "[2025-07-26 05:52:55,992] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 05:52:59,806] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install deepspeed pytorch-optimizer torch-optimi -U -q\n",
    "!pip install wandb -q\n",
    "!pip install tensorboard -q\n",
    "!pip install toml -q\n",
    "import deepspeed.comm.comm as dist\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !pip install typing_extensions -U \n",
    "from typing_extensions import LiteralString, TypeAliasType, TypeIs, deprecated\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'transformers>=4.49.0' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate einops timm transformers ftfy easydict diffusers deepspeed datasets imageio loguru opencv-python gradio toml -q\n",
    "from accelerate import init_empty_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import imageio\n",
    "DTYPE_MAP = {'float32': torch.float32, 'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float8': torch.float8_e4m3fn}\n",
    "VIDEO_EXTENSIONS = set(x.extension for x in imageio.config.video_extensions)\n",
    "AUTOCAST_DTYPE = torch.bfloat16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AUTOCAST_DTYPE\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install ninja\n",
    "# !export MAX_JOBS=4\n",
    "# !git clone https://github.com/Dao-AILab/flash-attention.git\n",
    "# %cd flash-attention\n",
    "# !pip install . \n",
    "#cuda 11.8일시\n",
    "# !pip install flash-attn --no-build-isolation #--prefer-binary \n",
    "\n",
    "# !pip install flash-attn==2.5.5 --no-build-isolation #--prefer-binary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0iHaDdEN3N9",
    "outputId": "ef652391-941a-48ea-a91a-1773f7d3fe20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Wan2.1'...\n",
      "remote: Enumerating objects: 368, done.\u001b[K\n",
      "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 368 (delta 0), reused 0 (delta 0), pack-reused 366 (from 2)\u001b[K\n",
      "Receiving objects: 100% (368/368), 10.69 MiB | 13.45 MiB/s, done.\n",
      "Resolving deltas: 100% (202/202), done.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Wan-Video/Wan2.1.git\n",
    "import sys\n",
    "# sys.path.insert(0, '/workspace/Wan2.1')\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aobLNxL9HyIi",
    "outputId": "8b7d5157-b4bc-464f-fc34-5d285c0992d8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (1.1.5)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.50)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.1.31)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: pfzy, InquirerPy\n",
      "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 22 files:   0%|                                 | 0/22 [00:00<?, ?it/s]Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Wan2.1-T2V-1.3B/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Downloading 'Wan2.1_VAE.pth' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/OZTShqo6Di7rh0SEswnfwSjKa9w=.38071ab59bd94681c686fa51d75a1968f64e470262043be31f7a094e442fd981.incomplete'\n",
      "\n",
      "Wan2.1_VAE.pth:   0%|                                | 0.00/508M [00:00<?, ?B/s]\u001b[ADownloading 'assets/data_for_diff_stage.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/UCVN8O248kjCIp8i1TvUXWfm-q4=.59aec08409f2d46b0e640e4e120dc7cca52c08c3de56d026602dbcff1ebf241a.incomplete'\n",
      "Downloading 'assets/comp_effic.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/X4-4NQEdffK1bUQbDx0NJ4IbcNo=.b0e225caffb4b31295ad150f95ee852e4c3dde4a00ac8f79a2ff500f2ce26b8d.incomplete'\n",
      "Downloading 'LICENSE.txt' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/cyBuwAu93UXke23CJCWORBYR70A=.261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64.incomplete'\n",
      "Downloading 'assets/i2v_res.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/GL7zbyOeRDu8VADezw2p_Gw3fW4=.6823b3206d8d0cb18d3b5b949dec1217f1178109ba11f14e977b67e1f7b8a248.incomplete'\n",
      "Downloading 'assets/.DS_Store' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/aHCrtOA_94YSUtnsyba37B_tPq0=.d65165279105ca6773180500688df4bdc69a2c7b771752f0a46ef120b7fd8ec3.incomplete'\n",
      "Downloading '.gitattributes' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.0a1f66aea84e17cbf6a60c51431723062f87df8a.incomplete'\n",
      "\n",
      "\n",
      "LICENSE.txt: 11.4kB [00:00, 20.6MB/s]A\n",
      "\n",
      "\n",
      ".gitattributes: 0.00B [00:00, ?B/s]\u001b[A\u001b[ADownload complete. Moving file to Wan2.1-T2V-1.3B/LICENSE.txt\n",
      ".gitattributes: 2.23kB [00:00, 3.99MB/s]\n",
      "Downloading 'README.md' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.fbbf7c13900d37f3179c546d09372aeb52e0fae1.incomplete'\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/.gitattributes\n",
      "\n",
      "Wan2.1_VAE.pth:   2%|▍                       | 10.5M/508M [00:00<00:04, 102MB/s]\u001b[A\n",
      "\n",
      "comp_effic.png:   0%|                               | 0.00/1.79M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "README.md: 16.9kB [00:00, 22.8MB/s]A\u001b[A\n",
      "Fetching 22 files:   5%|█▏                       | 1/22 [00:00<00:09,  2.24it/s]Download complete. Moving file to Wan2.1-T2V-1.3B/README.md\n",
      "comp_effic.png: 100%|██████████████████████| 1.79M/1.79M [00:00<00:00, 36.9MB/s]\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/comp_effic.png\n",
      "\n",
      "\n",
      ".DS_Store: 100%|███████████████████████████| 6.15k/6.15k [00:00<00:00, 34.7MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/.DS_Store\n",
      "\n",
      "Wan2.1_VAE.pth:   6%|█▍                      | 31.5M/508M [00:00<00:03, 120MB/s]\u001b[A\n",
      "\n",
      "data_for_diff_stage.jpg:   0%|                       | 0.00/528k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "i2v_res.png:   0%|                                   | 0.00/892k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'assets/logo.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/gvouIpVGo_vjLxWDOGAmKk_SZvY=.96cddc0f667293436d0b9f92a299b6346b65b231d38ee49719a33d46c91fe1e3.incomplete'\n",
      "Downloading 'assets/t2v_res.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/VwwCFkBXvnWpmAWpzHvOZFYnnp8=.91db579092446be2a834bc67721a8e4346936f38c4edb912f459ca3e10f8f439.incomplete'\n",
      "Downloading 'assets/vben_1.3b_vs_sota.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/yhawob5KxOUy3bpsGU-xOWB6FC8=.b7705db79f2e1428ec7a1e6fff8c4fbde062fb95bb233516ddbd04b20007c845.incomplete'\n",
      "\n",
      "Wan2.1_VAE.pth:  10%|██▍                     | 52.4M/508M [00:00<00:03, 149MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "t2v_res.jpg:   0%|                                   | 0.00/301k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'assets/vben_vs_sota.png' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/p0uIb0aMIBdDB8sg9tk5UvunoB8=.9a0e86ca85046d2675f97984b88b6e74df07bba8a62a31ab8a1aef50d4eda44e.incomplete'\n",
      "t2v_res.jpg: 100%|███████████████████████████| 301k/301k [00:00<00:00, 10.1MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vben_vs_sota.png: 100%|█████████████████████| 1.55M/1.55M [00:00<00:00, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/t2v_res.jpg\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/vben_vs_sota.png\n",
      "Downloading 'assets/video_dit_arch.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/PbT6aGUrWxKRPnz45UQ5EbqxKYE=.195dceec6570289d8b01cc51d2e28a7786216f19de55b23978a52610d1646a66.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "logo.png:   0%|                                     | 0.00/56.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  14%|███▍                    | 73.4M/508M [00:00<00:02, 160MB/s]\u001b[A\n",
      "\n",
      "data_for_diff_stage.jpg: 100%|███████████████| 528k/528k [00:00<00:00, 2.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "logo.png: 100%|████████████████████████████| 56.3k/56.3k [00:00<00:00, 1.04MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/data_for_diff_stage.jpg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "video_dit_arch.jpg:   0%|                            | 0.00/643k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownload complete. Moving file to Wan2.1-T2V-1.3B/assets/logo.png\n",
      "\n",
      "\n",
      "\n",
      "i2v_res.png: 100%|███████████████████████████| 892k/892k [00:00<00:00, 3.05MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/i2v_res.png\n",
      "\n",
      "Wan2.1_VAE.pth:  19%|████▍                   | 94.4M/508M [00:00<00:02, 170MB/s]\u001b[ADownloading 'assets/video_vae_res.jpg' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/assets/_r133FrlYqQbngOMpQWXWYhhvVM=.d8f9e7f7353848056a615c8ef35ab86ec22976bb46cb27405008b4089701945c.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "video_vae_res.jpg: 100%|█████████████████████| 213k/213k [00:00<00:00, 59.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading 'config.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.d203bef600b2f3c64fe1f5f53d70a2087f4ccd2f.incomplete'\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/video_vae_res.jpg\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 249/249 [00:00<00:00, 1.66MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/config.json\n",
      "\n",
      "Wan2.1_VAE.pth:  23%|█████▋                   | 115M/508M [00:00<00:02, 170MB/s]\u001b[ADownloading 'diffusion_pytorch_model.safetensors' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/4SfAgk9U607e8pVunEB9nSiU10k=.96b6b242ca1c2f24e9d02cd6596066fab6d310e2d7538f33ae267cb18d957e8f.incomplete'\n",
      "Downloading 'examples/i2v_input.JPG' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/examples/GqO1vCXUoNdTZc6wjxo1L7sjRdI=.077e3d965090c9028c69c00931675f42e1acc815c6eb450ab291b3b72d211a8e.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "video_dit_arch.jpg: 100%|████████████████████| 643k/643k [00:00<00:00, 2.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "video_dit_arch.jpg: 100%|████████████████████| 643k/643k [00:00<00:00, 2.92MB/s]\u001b[A\u001b[A\n",
      "vben_1.3b_vs_sota.png: 100%|█████████████████| 516k/516k [00:00<00:00, 2.31MB/s]\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/vben_1.3b_vs_sota.png\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/assets/video_dit_arch.jpg\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'google/umt5-xxl/special_tokens_map.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/ahkChHUJFxEmOdq5GDFEmerRzCY=.14855e7052ffbb595057dfd791d293c1c940db2c.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "special_tokens_map.json: 6.62kB [00:00, 25.6MB/s]A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/special_tokens_map.json\n",
      "\n",
      "\n",
      "\n",
      "i2v_input.JPG:   0%|                                 | 0.00/251k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'google/umt5-xxl/spiece.model' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/vj8E1loknrCNPSP8nWJC234Bff4=.e3909a67b780650b35cf529ac782ad2b6b26e6d1f849d3fbb6a872905f452458.incomplete'\n",
      "Downloading 'google/umt5-xxl/tokenizer.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.6e197b4d3dbd71da14b4eb255f4fa91c9c1f2068b20a2de2472967ca3d22602b.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spiece.model:   0%|                                 | 0.00/4.55M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  29%|███████▏                 | 147M/508M [00:00<00:01, 181MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i2v_input.JPG: 100%|█████████████████████████| 251k/251k [00:00<00:00, 2.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "spiece.model: 100%|████████████████████████| 4.55M/4.55M [00:00<00:00, 81.3MB/s]\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/examples/i2v_input.JPG\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/spiece.model\n",
      "Downloading 'models_t5_umt5-xxl-enc-bf16.pth' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/7xjTyx9p9-CteGiI3VEINu_Ohx0=.7cace0da2b446bbbbc57d031ab6cf163a3d59b366da94e5afe36745b746fd81d.incomplete'\n",
      "Downloading 'google/umt5-xxl/tokenizer_config.json' to 'Wan2.1-T2V-1.3B/.cache/huggingface/download/google/umt5-xxl/vzaExXFZNBay89bvlQv-ZcI6BTg=.4e1cc1cd85599ce0b47fd0a746af188fe4043ff2.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json: 100%|███████████████████████| 16.8M/16.8M [00:00<00:00, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|              | 0.00/11.4G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "tokenizer.json: 100%|███████████████████████| 16.8M/16.8M [00:00<00:00, 113MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 61.7kB [00:00, 115MB/s][A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/tokenizer_config.json\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/google/umt5-xxl/tokenizer.json\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%| | 10.5M/5.68G [00:00<02:44, 34.4MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  37%|█████████▎               | 189M/508M [00:01<00:01, 170MB/s]\u001b[A\n",
      "Wan2.1_VAE.pth:  41%|██████████▎              | 210M/508M [00:01<00:01, 175MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|     | 10.5M/11.4G [00:00<04:52, 38.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   0%| | 21.0M/5.68G [00:00<02:18, 40.9MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  45%|███████████▎             | 231M/508M [00:01<00:01, 170MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|     | 21.0M/11.4G [00:00<04:06, 46.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 31.5M/5.68G [00:00<02:02, 46.0MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  50%|████████████▍            | 252M/508M [00:01<00:01, 169MB/s]\u001b[A\n",
      "Wan2.1_VAE.pth:  54%|█████████████▍           | 273M/508M [00:01<00:01, 177MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|     | 31.5M/11.4G [00:00<03:47, 49.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 41.9M/5.68G [00:00<01:56, 48.3MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  58%|██████████████▍          | 294M/508M [00:01<00:01, 182MB/s]\u001b[A\n",
      "Wan2.1_VAE.pth:  62%|███████████████▍         | 315M/508M [00:01<00:01, 183MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|     | 41.9M/11.4G [00:00<03:37, 52.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 52.4M/5.68G [00:01<01:53, 49.7MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  66%|████████████████▌        | 336M/508M [00:01<00:00, 183MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   0%|     | 52.4M/11.4G [00:01<03:30, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  70%|█████████████████▌       | 357M/508M [00:02<00:00, 188MB/s]\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 62.9M/5.68G [00:01<01:50, 50.7MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  76%|███████████████████      | 388M/508M [00:02<00:00, 202MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|     | 62.9M/11.4G [00:01<03:25, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 73.4M/5.68G [00:01<01:49, 51.2MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  83%|████████████████████▋    | 419M/508M [00:02<00:00, 204MB/s]\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|     | 73.4M/11.4G [00:01<03:22, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  89%|██████████████████████▏  | 451M/508M [00:02<00:00, 211MB/s]\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   1%| | 83.9M/5.68G [00:01<01:48, 51.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|     | 83.9M/11.4G [00:01<03:20, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  95%|███████████████████████▊ | 482M/508M [00:02<00:00, 204MB/s]\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%| | 94.4M/5.68G [00:01<01:49, 51.1MB/s]\u001b[A\u001b[A\n",
      "Wan2.1_VAE.pth:  99%|████████████████████████▊| 503M/508M [00:02<00:00, 200MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Wan2.1_VAE.pth: 100%|█████████████████████████| 508M/508M [00:02<00:00, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n",
      "\n",
      "\n",
      "Fetching 22 files:  18%|████▌                    | 4/22 [00:03<00:15,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 105M/11.4G [00:01<03:20, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%|  | 115M/5.68G [00:02<01:47, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 115M/11.4G [00:02<03:21, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%|  | 126M/5.68G [00:02<01:44, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 126M/11.4G [00:02<03:19, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   2%|  | 136M/5.68G [00:02<01:45, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 136M/11.4G [00:02<03:19, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|  | 147M/5.68G [00:02<01:45, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 147M/11.4G [00:02<03:18, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|  | 157M/5.68G [00:03<01:44, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 157M/11.4G [00:02<03:17, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   1%|      | 168M/11.4G [00:03<03:16, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|  | 168M/5.68G [00:03<01:44, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 178M/11.4G [00:03<03:18, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|  | 178M/5.68G [00:03<01:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 189M/11.4G [00:03<03:19, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   3%|  | 189M/5.68G [00:03<01:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 199M/11.4G [00:03<03:17, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 199M/5.68G [00:03<01:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 210M/11.4G [00:03<03:16, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 210M/5.68G [00:04<01:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 220M/11.4G [00:03<03:15, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 220M/5.68G [00:04<01:43, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|      | 231M/11.4G [00:04<03:14, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 231M/5.68G [00:04<01:43, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏     | 241M/11.4G [00:04<03:16, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 241M/5.68G [00:04<01:43, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏     | 252M/11.4G [00:04<03:17, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   4%|  | 252M/5.68G [00:04<01:43, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏     | 262M/11.4G [00:04<03:18, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|  | 262M/5.68G [00:05<01:43, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏     | 273M/11.4G [00:04<03:16, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|  | 273M/5.68G [00:05<01:43, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   2%|▏     | 283M/11.4G [00:05<03:15, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|  | 283M/5.68G [00:05<01:43, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 294M/11.4G [00:05<03:14, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 304M/11.4G [00:05<03:13, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|  | 294M/5.68G [00:05<01:42, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 315M/11.4G [00:05<03:15, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   5%|  | 304M/5.68G [00:05<01:42, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 325M/11.4G [00:05<03:16, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|  | 315M/5.68G [00:06<01:42, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 336M/11.4G [00:06<03:14, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|  | 325M/5.68G [00:06<01:42, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 346M/11.4G [00:06<03:14, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|  | 336M/5.68G [00:06<01:41, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 357M/11.4G [00:06<03:14, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|  | 346M/5.68G [00:06<01:41, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 367M/11.4G [00:06<03:14, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|▏ | 357M/5.68G [00:06<01:41, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 377M/11.4G [00:06<03:13, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   6%|▏ | 367M/5.68G [00:07<01:41, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   3%|▏     | 388M/11.4G [00:06<03:14, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏ | 377M/5.68G [00:07<01:40, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 398M/11.4G [00:07<03:15, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏ | 388M/5.68G [00:07<01:40, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 409M/11.4G [00:07<03:14, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏ | 398M/5.68G [00:07<01:40, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 419M/11.4G [00:07<03:13, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏ | 409M/5.68G [00:07<01:40, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 430M/11.4G [00:07<03:12, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   7%|▏ | 419M/5.68G [00:08<01:39, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 440M/11.4G [00:07<03:11, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 451M/11.4G [00:08<03:11, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 430M/5.68G [00:08<01:39, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 461M/11.4G [00:08<03:13, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 440M/5.68G [00:08<01:39, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▏     | 472M/11.4G [00:08<03:14, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 451M/5.68G [00:08<01:39, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎     | 482M/11.4G [00:08<03:13, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 461M/5.68G [00:08<01:39, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎     | 493M/11.4G [00:08<03:12, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 472M/5.68G [00:09<01:39, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   4%|▎     | 503M/11.4G [00:08<03:11, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   8%|▏ | 482M/5.68G [00:09<01:39, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 514M/11.4G [00:09<03:10, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▏ | 493M/5.68G [00:09<01:38, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 524M/11.4G [00:09<03:10, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▏ | 503M/5.68G [00:09<01:38, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 535M/11.4G [00:09<03:13, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▏ | 514M/5.68G [00:09<01:38, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▏ | 524M/5.68G [00:10<01:38, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 545M/11.4G [00:10<04:45, 37.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:   9%|▏ | 535M/5.68G [00:10<01:38, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 556M/11.4G [00:10<04:16, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▏ | 545M/5.68G [00:10<01:37, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 566M/11.4G [00:10<03:56, 45.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▏ | 556M/5.68G [00:10<01:37, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 577M/11.4G [00:10<03:42, 48.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▏ | 566M/5.68G [00:10<01:37, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 587M/11.4G [00:10<03:31, 50.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▏ | 577M/5.68G [00:11<01:37, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 598M/11.4G [00:10<03:25, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  10%|▏ | 587M/5.68G [00:11<01:37, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 608M/11.4G [00:11<03:22, 53.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 598M/5.68G [00:11<01:37, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   5%|▎     | 619M/11.4G [00:11<03:17, 54.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 608M/5.68G [00:11<01:36, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 629M/11.4G [00:11<03:12, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 619M/5.68G [00:11<01:35, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 640M/11.4G [00:11<03:08, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 629M/5.68G [00:12<01:35, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 650M/11.4G [00:11<03:06, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 661M/11.4G [00:12<03:08, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 640M/5.68G [00:12<01:35, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 671M/11.4G [00:12<03:08, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  11%|▏ | 650M/5.68G [00:12<01:35, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 682M/11.4G [00:12<03:06, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▏ | 661M/5.68G [00:12<01:35, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 692M/11.4G [00:12<03:04, 57.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▏ | 671M/5.68G [00:12<01:35, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▎     | 703M/11.4G [00:12<03:03, 58.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▏ | 682M/5.68G [00:13<01:35, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍     | 713M/11.4G [00:12<03:06, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▏ | 692M/5.68G [00:13<01:34, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍     | 724M/11.4G [00:13<03:08, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  12%|▏ | 703M/5.68G [00:13<01:34, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   6%|▍     | 734M/11.4G [00:13<03:09, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 713M/5.68G [00:13<01:34, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 744M/11.4G [00:13<03:10, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 724M/5.68G [00:14<01:53, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 755M/11.4G [00:13<03:38, 48.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 734M/5.68G [00:14<01:47, 46.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 765M/11.4G [00:14<03:27, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 776M/11.4G [00:14<03:19, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 744M/5.68G [00:14<01:42, 48.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 786M/11.4G [00:14<03:15, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 755M/5.68G [00:14<01:39, 49.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 797M/11.4G [00:14<03:13, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  13%|▎ | 765M/5.68G [00:14<01:36, 50.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 807M/11.4G [00:14<03:12, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▎ | 776M/5.68G [00:15<01:35, 51.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 818M/11.4G [00:14<03:09, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▎ | 786M/5.68G [00:15<01:34, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 828M/11.4G [00:15<03:07, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▎ | 797M/5.68G [00:15<01:33, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 839M/11.4G [00:15<03:05, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▎ | 807M/5.68G [00:15<01:32, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   7%|▍     | 849M/11.4G [00:15<03:04, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  14%|▎ | 818M/5.68G [00:15<01:32, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 860M/11.4G [00:15<03:05, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▎ | 828M/5.68G [00:16<01:31, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 870M/11.4G [00:15<03:06, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▎ | 839M/5.68G [00:16<01:31, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 881M/11.4G [00:16<03:06, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▎ | 849M/5.68G [00:16<01:31, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 891M/11.4G [00:16<03:05, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▎ | 860M/5.68G [00:16<01:31, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 902M/11.4G [00:16<03:04, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  15%|▎ | 870M/5.68G [00:16<01:31, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 912M/11.4G [00:16<03:03, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 923M/11.4G [00:16<03:03, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 881M/5.68G [00:17<01:31, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 933M/11.4G [00:16<03:05, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 891M/5.68G [00:17<01:30, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▍     | 944M/11.4G [00:17<03:05, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 902M/5.68G [00:17<01:31, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌     | 954M/11.4G [00:17<03:06, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 912M/5.68G [00:17<01:30, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   8%|▌     | 965M/11.4G [00:17<03:05, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 923M/5.68G [00:17<01:30, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 975M/11.4G [00:17<03:04, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  16%|▎ | 933M/5.68G [00:18<01:30, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 986M/11.4G [00:17<03:03, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▎ | 944M/5.68G [00:18<01:29, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▌     | 996M/11.4G [00:18<03:03, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▎ | 954M/5.68G [00:18<01:29, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.01G/11.4G [00:18<03:02, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▎ | 965M/5.68G [00:18<01:29, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.02G/11.4G [00:18<03:03, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▎ | 975M/5.68G [00:18<01:29, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.03G/11.4G [00:18<03:04, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  17%|▎ | 986M/5.68G [00:19<01:29, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.04G/11.4G [00:18<03:03, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▎ | 996M/5.68G [00:19<01:29, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.05G/11.4G [00:19<03:02, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.06G/11.4G [00:19<03:20, 51.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▏| 1.01G/5.68G [00:19<01:43, 45.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:   9%|▍    | 1.07G/11.4G [00:19<03:15, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▏| 1.02G/5.68G [00:19<01:38, 47.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.08G/11.4G [00:19<03:10, 53.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▏| 1.03G/5.68G [00:19<01:34, 48.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.09G/11.4G [00:19<03:07, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▏| 1.04G/5.68G [00:20<01:33, 49.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.10G/11.4G [00:20<03:05, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  18%|▏| 1.05G/5.68G [00:20<01:31, 50.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.11G/11.4G [00:20<03:04, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▏| 1.06G/5.68G [00:20<01:30, 51.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.12G/11.4G [00:20<03:02, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▏| 1.07G/5.68G [00:20<01:30, 50.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▍    | 1.13G/11.4G [00:20<03:03, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▏| 1.08G/5.68G [00:20<01:29, 51.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌    | 1.14G/11.4G [00:20<03:03, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▏| 1.09G/5.68G [00:21<01:28, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  19%|▏| 1.10G/5.68G [00:21<01:27, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌    | 1.15G/11.4G [00:21<04:27, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▏| 1.11G/5.68G [00:21<01:26, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌    | 1.16G/11.4G [00:21<04:00, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▏| 1.12G/5.68G [00:21<01:26, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌    | 1.17G/11.4G [00:21<03:42, 45.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▏| 1.13G/5.68G [00:21<01:26, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  10%|▌    | 1.18G/11.4G [00:21<03:29, 48.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▏| 1.14G/5.68G [00:22<01:26, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.20G/11.4G [00:21<03:18, 51.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  20%|▏| 1.15G/5.68G [00:22<01:26, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.21G/11.4G [00:22<03:13, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.16G/5.68G [00:22<01:25, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.22G/11.4G [00:22<03:08, 53.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.17G/5.68G [00:22<01:25, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.23G/11.4G [00:22<03:04, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.18G/5.68G [00:22<01:25, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.24G/11.4G [00:22<03:01, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.20G/5.68G [00:23<01:25, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.25G/11.4G [00:22<03:00, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.26G/11.4G [00:23<03:02, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.21G/5.68G [00:23<01:26, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.27G/11.4G [00:23<03:02, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  21%|▏| 1.22G/5.68G [00:23<01:25, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.28G/11.4G [00:23<03:02, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▏| 1.23G/5.68G [00:23<01:24, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.29G/11.4G [00:23<03:01, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▏| 1.24G/5.68G [00:23<01:24, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  11%|▌    | 1.30G/11.4G [00:23<03:00, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.31G/11.4G [00:24<02:59, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▏| 1.25G/5.68G [00:24<01:57, 37.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.32G/11.4G [00:24<02:58, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▏| 1.26G/5.68G [00:24<01:46, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.33G/11.4G [00:24<02:55, 57.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  22%|▏| 1.27G/5.68G [00:24<01:39, 44.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.34G/11.4G [00:24<02:55, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.28G/5.68G [00:24<01:34, 46.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.35G/11.4G [00:24<02:57, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.29G/5.68G [00:25<01:30, 48.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.36G/11.4G [00:24<02:56, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.37G/11.4G [00:25<02:56, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.30G/5.68G [00:25<01:27, 49.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.38G/11.4G [00:25<02:55, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.31G/5.68G [00:25<01:26, 50.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.39G/11.4G [00:25<02:56, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.32G/5.68G [00:25<01:24, 51.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.41G/11.4G [00:25<02:55, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  23%|▏| 1.33G/5.68G [00:25<01:23, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  12%|▌    | 1.42G/11.4G [00:25<02:56, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▏| 1.34G/5.68G [00:26<01:22, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.43G/11.4G [00:26<02:57, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▏| 1.35G/5.68G [00:26<01:22, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.44G/11.4G [00:26<02:56, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▏| 1.36G/5.68G [00:26<01:21, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.45G/11.4G [00:26<02:55, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▏| 1.37G/5.68G [00:26<01:21, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.46G/11.4G [00:26<02:52, 57.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  24%|▏| 1.38G/5.68G [00:26<01:20, 53.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.47G/11.4G [00:26<02:50, 58.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▏| 1.39G/5.68G [00:27<01:19, 53.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.48G/11.4G [00:26<02:51, 57.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▏| 1.41G/5.68G [00:27<01:19, 53.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.49G/11.4G [00:27<02:54, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▏| 1.42G/5.68G [00:27<01:19, 53.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.50G/11.4G [00:27<02:53, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▎| 1.43G/5.68G [00:27<01:19, 53.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.51G/11.4G [00:27<02:53, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▎| 1.44G/5.68G [00:27<01:19, 53.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.52G/11.4G [00:27<02:52, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  25%|▎| 1.45G/5.68G [00:28<01:19, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  13%|▋    | 1.53G/11.4G [00:27<02:52, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.54G/11.4G [00:28<02:52, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▎| 1.46G/5.68G [00:28<01:19, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.55G/11.4G [00:28<02:53, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▎| 1.47G/5.68G [00:28<01:19, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.56G/11.4G [00:28<02:54, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▎| 1.48G/5.68G [00:28<01:19, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.57G/11.4G [00:28<02:53, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▎| 1.49G/5.68G [00:28<01:19, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.58G/11.4G [00:28<02:52, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  26%|▎| 1.50G/5.68G [00:29<01:19, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.59G/11.4G [00:28<02:51, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▎| 1.51G/5.68G [00:29<01:19, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.60G/11.4G [00:29<02:51, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▎| 1.52G/5.68G [00:29<01:19, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.61G/11.4G [00:29<02:51, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▎| 1.53G/5.68G [00:29<01:19, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.63G/11.4G [00:29<02:53, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▎| 1.54G/5.68G [00:29<01:19, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.64G/11.4G [00:29<02:54, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  27%|▎| 1.55G/5.68G [00:30<01:18, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  14%|▋    | 1.65G/11.4G [00:29<02:53, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.56G/5.68G [00:30<01:18, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▋    | 1.66G/11.4G [00:30<02:51, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.57G/5.68G [00:30<01:18, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▋    | 1.67G/11.4G [00:30<02:50, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▋    | 1.68G/11.4G [00:30<02:49, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.58G/5.68G [00:30<01:18, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▋    | 1.69G/11.4G [00:30<02:50, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.59G/5.68G [00:30<01:17, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▋    | 1.70G/11.4G [00:30<02:51, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.60G/5.68G [00:31<01:17, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▊    | 1.71G/11.4G [00:31<02:52, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  28%|▎| 1.61G/5.68G [00:31<01:17, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▊    | 1.72G/11.4G [00:31<02:49, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▎| 1.63G/5.68G [00:31<01:17, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▊    | 1.73G/11.4G [00:31<02:47, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▎| 1.64G/5.68G [00:31<01:17, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▊    | 1.74G/11.4G [00:31<02:47, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▎| 1.65G/5.68G [00:31<01:16, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  15%|▊    | 1.75G/11.4G [00:31<02:47, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▎| 1.66G/5.68G [00:32<01:16, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.76G/11.4G [00:31<02:48, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  29%|▎| 1.67G/5.68G [00:32<01:16, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.77G/11.4G [00:32<02:49, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.68G/5.68G [00:32<01:16, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.78G/11.4G [00:32<02:48, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.69G/5.68G [00:32<01:16, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.79G/11.4G [00:32<02:48, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.70G/5.68G [00:32<01:16, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.80G/11.4G [00:32<02:47, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.81G/11.4G [00:32<02:47, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.71G/5.68G [00:33<01:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.82G/11.4G [00:33<02:48, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.72G/5.68G [00:33<01:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.84G/11.4G [00:33<02:49, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  30%|▎| 1.73G/5.68G [00:33<01:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.85G/11.4G [00:33<02:50, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▎| 1.74G/5.68G [00:33<01:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.86G/11.4G [00:33<02:49, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▎| 1.75G/5.68G [00:33<01:14, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  16%|▊    | 1.87G/11.4G [00:33<02:48, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▎| 1.76G/5.68G [00:34<01:14, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.88G/11.4G [00:33<02:47, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▎| 1.77G/5.68G [00:34<01:14, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.89G/11.4G [00:34<02:48, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.90G/11.4G [00:34<02:47, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  31%|▎| 1.78G/5.68G [00:34<01:40, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.91G/11.4G [00:34<02:55, 53.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▎| 1.79G/5.68G [00:34<01:32, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.92G/11.4G [00:34<02:52, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▎| 1.80G/5.68G [00:35<01:30, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.93G/11.4G [00:34<02:52, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.94G/11.4G [00:35<02:52, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▎| 1.81G/5.68G [00:35<01:25, 44.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.95G/11.4G [00:35<02:52, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▎| 1.82G/5.68G [00:35<01:22, 46.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.96G/11.4G [00:35<02:50, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  32%|▎| 1.84G/5.68G [00:35<01:19, 48.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.97G/11.4G [00:35<02:50, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.85G/5.68G [00:36<01:18, 49.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  17%|▊    | 1.98G/11.4G [00:35<02:49, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.86G/5.68G [00:36<01:16, 49.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 1.99G/11.4G [00:36<02:48, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.87G/5.68G [00:36<01:15, 50.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.00G/11.4G [00:36<02:47, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.88G/5.68G [00:36<01:14, 50.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.01G/11.4G [00:36<02:46, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.89G/5.68G [00:36<01:14, 50.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  33%|▎| 1.90G/5.68G [00:37<01:13, 51.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.02G/11.4G [00:36<03:57, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▎| 1.91G/5.68G [00:37<01:12, 51.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.03G/11.4G [00:37<03:35, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▎| 1.92G/5.68G [00:37<01:12, 51.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.04G/11.4G [00:37<03:21, 46.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▎| 1.93G/5.68G [00:37<01:11, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.06G/11.4G [00:37<03:08, 49.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▎| 1.94G/5.68G [00:37<01:10, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.07G/11.4G [00:37<03:02, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  34%|▎| 1.95G/5.68G [00:38<01:10, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.08G/11.4G [00:37<02:58, 52.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 1.96G/5.68G [00:38<01:11, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.09G/11.4G [00:38<02:55, 52.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 1.97G/5.68G [00:38<01:10, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  18%|▉    | 2.10G/11.4G [00:38<02:51, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 1.98G/5.68G [00:38<01:10, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.11G/11.4G [00:38<02:48, 55.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 1.99G/5.68G [00:38<01:10, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.12G/11.4G [00:38<02:46, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 2.00G/5.68G [00:39<01:10, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.13G/11.4G [00:38<02:44, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.14G/11.4G [00:38<02:43, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  35%|▎| 2.01G/5.68G [00:39<01:09, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.15G/11.4G [00:39<02:44, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▎| 2.02G/5.68G [00:39<01:09, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.16G/11.4G [00:39<02:45, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▎| 2.03G/5.68G [00:39<01:09, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.17G/11.4G [00:39<02:44, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▎| 2.04G/5.68G [00:39<01:09, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.18G/11.4G [00:39<02:43, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▎| 2.06G/5.68G [00:40<01:09, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.19G/11.4G [00:39<02:42, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  36%|▎| 2.07G/5.68G [00:40<01:08, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.20G/11.4G [00:40<02:41, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▎| 2.08G/5.68G [00:40<01:08, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  19%|▉    | 2.21G/11.4G [00:40<02:41, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▎| 2.09G/5.68G [00:40<01:08, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|▉    | 2.22G/11.4G [00:40<02:42, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▎| 2.10G/5.68G [00:40<01:08, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|▉    | 2.23G/11.4G [00:40<02:43, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▎| 2.11G/5.68G [00:41<01:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|▉    | 2.24G/11.4G [00:40<02:44, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  37%|▎| 2.12G/5.68G [00:41<01:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|▉    | 2.25G/11.4G [00:41<02:43, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.13G/5.68G [00:41<01:07, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|▉    | 2.26G/11.4G [00:41<02:42, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.14G/5.68G [00:41<01:07, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.28G/11.4G [00:41<02:41, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.15G/5.68G [00:41<01:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.29G/11.4G [00:41<02:40, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.30G/11.4G [00:41<02:40, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.16G/5.68G [00:42<01:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.31G/11.4G [00:41<02:42, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.17G/5.68G [00:42<01:06, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.32G/11.4G [00:42<02:42, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  38%|▍| 2.18G/5.68G [00:42<01:06, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  20%|█    | 2.33G/11.4G [00:42<02:41, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▍| 2.19G/5.68G [00:42<01:06, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.34G/11.4G [00:42<02:40, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▍| 2.20G/5.68G [00:42<01:06, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.35G/11.4G [00:42<02:39, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▍| 2.21G/5.68G [00:43<01:06, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.36G/11.4G [00:42<02:40, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▍| 2.22G/5.68G [00:43<01:05, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.37G/11.4G [00:43<02:40, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  39%|▍| 2.23G/5.68G [00:43<01:05, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.38G/11.4G [00:43<02:39, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.24G/5.68G [00:43<01:05, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.39G/11.4G [00:43<02:40, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.25G/5.68G [00:43<01:05, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.40G/11.4G [00:43<02:41, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.26G/5.68G [00:44<01:04, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.41G/11.4G [00:43<02:40, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.28G/5.68G [00:44<01:04, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.42G/11.4G [00:44<02:39, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.29G/5.68G [00:44<01:04, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  21%|█    | 2.43G/11.4G [00:44<02:38, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  40%|▍| 2.30G/5.68G [00:44<01:04, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.44G/11.4G [00:44<02:37, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.45G/11.4G [00:44<02:37, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▍| 2.31G/5.68G [00:44<01:04, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.46G/11.4G [00:44<02:39, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▍| 2.32G/5.68G [00:45<01:04, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.47G/11.4G [00:44<02:40, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▍| 2.33G/5.68G [00:45<01:03, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.49G/11.4G [00:45<02:40, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▍| 2.34G/5.68G [00:45<01:04, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.50G/11.4G [00:45<02:41, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  41%|▍| 2.35G/5.68G [00:45<01:03, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.51G/11.4G [00:45<02:40, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.36G/5.68G [00:45<01:03, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.52G/11.4G [00:45<02:38, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.37G/5.68G [00:46<01:02, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.53G/11.4G [00:45<02:37, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.38G/5.68G [00:46<01:02, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.54G/11.4G [00:46<02:36, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.39G/5.68G [00:46<01:02, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  22%|█    | 2.55G/11.4G [00:46<02:36, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.40G/5.68G [00:46<01:02, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.56G/11.4G [00:46<02:36, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  42%|▍| 2.41G/5.68G [00:46<01:02, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.57G/11.4G [00:46<02:38, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▍| 2.42G/5.68G [00:47<01:01, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.58G/11.4G [00:46<02:36, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▍| 2.43G/5.68G [00:47<01:01, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.59G/11.4G [00:47<02:36, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▍| 2.44G/5.68G [00:47<01:01, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.60G/11.4G [00:47<02:35, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▍| 2.45G/5.68G [00:47<01:01, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.61G/11.4G [00:47<02:34, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  43%|▍| 2.46G/5.68G [00:47<01:01, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▍| 2.47G/5.68G [00:48<01:01, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.62G/11.4G [00:47<03:46, 38.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▍| 2.49G/5.68G [00:48<01:00, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.63G/11.4G [00:48<03:26, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.64G/11.4G [00:48<03:11, 45.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.65G/11.4G [00:48<03:00, 48.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▍| 2.50G/5.68G [00:48<01:25, 37.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  23%|█▏   | 2.66G/11.4G [00:48<02:52, 50.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▍| 2.51G/5.68G [00:48<01:17, 40.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.67G/11.4G [00:48<02:48, 51.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  44%|▍| 2.52G/5.68G [00:49<01:11, 43.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.68G/11.4G [00:48<02:43, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.53G/5.68G [00:49<01:07, 46.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.69G/11.4G [00:49<02:40, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.54G/5.68G [00:49<01:05, 48.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.71G/11.4G [00:49<02:44, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.55G/5.68G [00:49<01:03, 49.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.72G/11.4G [00:49<02:40, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.56G/5.68G [00:49<01:01, 50.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.73G/11.4G [00:49<02:38, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.57G/5.68G [00:50<01:00, 51.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.74G/11.4G [00:49<02:37, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  45%|▍| 2.58G/5.68G [00:50<00:59, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.75G/11.4G [00:50<02:35, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▍| 2.59G/5.68G [00:50<00:58, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.76G/11.4G [00:50<02:35, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▍| 2.60G/5.68G [00:50<00:58, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.77G/11.4G [00:50<02:34, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▍| 2.61G/5.68G [00:50<00:58, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  24%|█▏   | 2.78G/11.4G [00:50<02:32, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▍| 2.62G/5.68G [00:51<00:57, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▏   | 2.79G/11.4G [00:50<02:32, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  46%|▍| 2.63G/5.68G [00:51<00:57, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▏   | 2.80G/11.4G [00:51<02:33, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.64G/5.68G [00:51<00:57, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▏   | 2.81G/11.4G [00:51<02:34, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.65G/5.68G [00:51<00:56, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▏   | 2.82G/11.4G [00:51<02:34, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.66G/5.68G [00:51<00:56, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▏   | 2.83G/11.4G [00:51<02:35, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.67G/5.68G [00:52<00:56, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.84G/11.4G [00:51<02:33, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.68G/5.68G [00:52<00:56, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.85G/11.4G [00:52<02:32, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  47%|▍| 2.69G/5.68G [00:52<00:56, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.86G/11.4G [00:52<02:31, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.87G/11.4G [00:52<02:30, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▍| 2.71G/5.68G [00:52<00:55, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.88G/11.4G [00:52<02:30, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▍| 2.72G/5.68G [00:52<00:55, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  25%|█▎   | 2.89G/11.4G [00:52<02:31, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▍| 2.73G/5.68G [00:53<00:55, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.90G/11.4G [00:52<02:31, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▍| 2.74G/5.68G [00:53<00:55, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.92G/11.4G [00:53<02:29, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  48%|▍| 2.75G/5.68G [00:53<00:54, 53.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.93G/11.4G [00:53<02:29, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▍| 2.76G/5.68G [00:53<00:54, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.94G/11.4G [00:53<02:28, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▍| 2.77G/5.68G [00:53<00:55, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.95G/11.4G [00:53<02:28, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▍| 2.78G/5.68G [00:54<00:55, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.96G/11.4G [00:53<02:28, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▍| 2.79G/5.68G [00:54<00:54, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.97G/11.4G [00:54<02:29, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  49%|▍| 2.80G/5.68G [00:54<00:54, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.98G/11.4G [00:54<02:29, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▍| 2.81G/5.68G [00:54<00:54, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 2.99G/11.4G [00:54<02:28, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▍| 2.82G/5.68G [00:54<00:54, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 3.00G/11.4G [00:54<02:27, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▍| 2.83G/5.68G [00:55<00:54, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  26%|█▎   | 3.01G/11.4G [00:54<02:27, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▌| 2.84G/5.68G [00:55<00:54, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.02G/11.4G [00:54<02:26, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.03G/11.4G [00:55<02:26, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▌| 2.85G/5.68G [00:55<00:53, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.04G/11.4G [00:55<02:28, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  50%|▌| 2.86G/5.68G [00:55<00:53, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.05G/11.4G [00:55<02:29, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|▌| 2.87G/5.68G [00:55<00:53, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.06G/11.4G [00:55<02:27, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|▌| 2.88G/5.68G [00:56<00:53, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.07G/11.4G [00:55<02:26, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|▌| 2.89G/5.68G [00:56<00:53, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.08G/11.4G [00:56<02:26, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|▌| 2.90G/5.68G [00:56<00:53, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.09G/11.4G [00:56<02:26, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  51%|▌| 2.92G/5.68G [00:56<00:52, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.10G/11.4G [00:56<02:26, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.93G/5.68G [00:56<00:52, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  27%|█▎   | 3.11G/11.4G [00:56<02:26, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.94G/5.68G [00:57<00:52, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.12G/11.4G [00:56<02:27, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.95G/5.68G [00:57<00:52, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.14G/11.4G [00:57<02:27, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.96G/5.68G [00:57<00:52, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.15G/11.4G [00:57<02:26, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.97G/5.68G [00:57<00:51, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.16G/11.4G [00:57<02:25, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  52%|▌| 2.98G/5.68G [00:57<00:51, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.17G/11.4G [00:57<02:25, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|▌| 2.99G/5.68G [00:58<00:51, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.18G/11.4G [00:57<02:25, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.19G/11.4G [00:57<02:25, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|▌| 3.00G/5.68G [00:58<00:51, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.20G/11.4G [00:58<02:26, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|▌| 3.01G/5.68G [00:58<00:50, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.21G/11.4G [00:58<02:27, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|▌| 3.02G/5.68G [00:58<00:50, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.22G/11.4G [00:58<02:27, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  53%|▌| 3.03G/5.68G [00:58<00:50, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  28%|█▍   | 3.23G/11.4G [00:58<02:25, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.04G/5.68G [00:59<00:50, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.24G/11.4G [00:58<02:24, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.05G/5.68G [00:59<00:49, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.25G/11.4G [00:59<02:23, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.06G/5.68G [00:59<00:50, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.26G/11.4G [00:59<02:23, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.07G/5.68G [00:59<00:49, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.27G/11.4G [00:59<02:22, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.28G/11.4G [00:59<02:23, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.08G/5.68G [01:00<01:07, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.29G/11.4G [00:59<02:24, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  54%|▌| 3.09G/5.68G [01:00<01:01, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.30G/11.4G [01:00<02:23, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.31G/11.4G [01:00<02:22, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|▌| 3.10G/5.68G [01:00<00:57, 44.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.32G/11.4G [01:00<02:22, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|▌| 3.11G/5.68G [01:00<00:54, 46.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.33G/11.4G [01:00<02:22, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|▌| 3.12G/5.68G [01:00<00:52, 48.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  29%|█▍   | 3.34G/11.4G [01:00<02:21, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|▌| 3.14G/5.68G [01:01<00:50, 50.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.36G/11.4G [01:00<02:22, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  55%|▌| 3.15G/5.68G [01:01<00:49, 51.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.37G/11.4G [01:01<02:23, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|▌| 3.16G/5.68G [01:01<00:48, 51.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|▌| 3.17G/5.68G [01:01<00:48, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|▌| 3.18G/5.68G [01:01<00:47, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.38G/11.4G [01:01<03:26, 38.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|▌| 3.19G/5.68G [01:02<00:47, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.39G/11.4G [01:01<03:06, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.40G/11.4G [01:01<02:51, 46.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  56%|▌| 3.20G/5.68G [01:02<00:46, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▍   | 3.41G/11.4G [01:02<02:41, 49.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.21G/5.68G [01:02<00:46, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▌   | 3.42G/11.4G [01:02<02:36, 50.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.22G/5.68G [01:02<00:46, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▌   | 3.43G/11.4G [01:02<02:32, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.23G/5.68G [01:02<00:46, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▌   | 3.44G/11.4G [01:02<02:28, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.24G/5.68G [01:03<00:46, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▌   | 3.45G/11.4G [01:02<02:25, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.25G/5.68G [01:03<00:46, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  30%|█▌   | 3.46G/11.4G [01:03<02:23, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  57%|▌| 3.26G/5.68G [01:03<00:45, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.47G/11.4G [01:03<02:21, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|▌| 3.27G/5.68G [01:03<00:45, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.48G/11.4G [01:03<02:20, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|▌| 3.28G/5.68G [01:03<00:45, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.49G/11.4G [01:03<02:20, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|▌| 3.29G/5.68G [01:04<00:45, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.50G/11.4G [01:03<02:20, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|▌| 3.30G/5.68G [01:04<00:45, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.51G/11.4G [01:04<02:19, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  58%|▌| 3.31G/5.68G [01:04<00:45, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.52G/11.4G [01:04<02:19, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.32G/5.68G [01:04<00:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.53G/11.4G [01:04<02:18, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.33G/5.68G [01:04<00:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.54G/11.4G [01:04<02:17, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.55G/11.4G [01:04<02:17, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.34G/5.68G [01:05<00:44, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.57G/11.4G [01:04<02:18, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.36G/5.68G [01:05<00:44, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  31%|█▌   | 3.58G/11.4G [01:05<02:19, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.37G/5.68G [01:05<00:44, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.59G/11.4G [01:05<02:17, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  59%|▌| 3.38G/5.68G [01:05<00:43, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.60G/11.4G [01:05<02:17, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|▌| 3.39G/5.68G [01:05<00:43, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.61G/11.4G [01:05<02:16, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|▌| 3.40G/5.68G [01:06<00:43, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.62G/11.4G [01:05<02:15, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|▌| 3.41G/5.68G [01:06<00:43, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.63G/11.4G [01:06<02:16, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|▌| 3.42G/5.68G [01:06<00:43, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.64G/11.4G [01:06<02:17, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  60%|▌| 3.43G/5.68G [01:06<00:42, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.65G/11.4G [01:06<02:17, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|▌| 3.44G/5.68G [01:06<00:42, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.66G/11.4G [01:06<02:16, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|▌| 3.45G/5.68G [01:07<00:42, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.67G/11.4G [01:06<02:15, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|▌| 3.46G/5.68G [01:07<00:42, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.68G/11.4G [01:06<02:15, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  32%|█▌   | 3.69G/11.4G [01:07<02:14, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|▌| 3.47G/5.68G [01:07<00:42, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.70G/11.4G [01:07<02:15, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  61%|▌| 3.48G/5.68G [01:07<00:41, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.71G/11.4G [01:07<02:16, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.49G/5.68G [01:07<00:41, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.72G/11.4G [01:07<02:16, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.50G/5.68G [01:08<00:41, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.73G/11.4G [01:07<02:15, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.51G/5.68G [01:08<00:41, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.74G/11.4G [01:08<02:14, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.52G/5.68G [01:08<00:41, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.75G/11.4G [01:08<02:14, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.53G/5.68G [01:08<00:40, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.76G/11.4G [01:08<02:13, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  62%|▌| 3.54G/5.68G [01:08<00:40, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.77G/11.4G [01:08<02:14, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|▋| 3.55G/5.68G [01:09<00:40, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.79G/11.4G [01:08<02:14, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|▋| 3.57G/5.68G [01:09<00:40, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  33%|█▋   | 3.80G/11.4G [01:09<02:15, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|▋| 3.58G/5.68G [01:09<00:40, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.81G/11.4G [01:09<02:14, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|▋| 3.59G/5.68G [01:09<00:39, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.82G/11.4G [01:09<02:13, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  63%|▋| 3.60G/5.68G [01:09<00:39, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.83G/11.4G [01:09<02:13, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.61G/5.68G [01:10<00:39, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.84G/11.4G [01:09<02:12, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.85G/11.4G [01:09<02:12, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.62G/5.68G [01:10<00:39, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.86G/11.4G [01:10<02:13, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.63G/5.68G [01:10<00:39, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.87G/11.4G [01:10<02:13, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.64G/5.68G [01:10<00:38, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.88G/11.4G [01:10<02:12, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.65G/5.68G [01:10<00:38, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.89G/11.4G [01:10<02:11, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  64%|▋| 3.66G/5.68G [01:11<00:38, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.90G/11.4G [01:10<02:11, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|▋| 3.67G/5.68G [01:11<00:38, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  34%|█▋   | 3.91G/11.4G [01:11<02:10, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|▋| 3.68G/5.68G [01:11<00:38, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.92G/11.4G [01:11<02:11, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|▋| 3.69G/5.68G [01:11<00:37, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.93G/11.4G [01:11<02:12, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|▋| 3.70G/5.68G [01:11<00:37, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.94G/11.4G [01:11<02:12, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  65%|▋| 3.71G/5.68G [01:12<00:37, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.95G/11.4G [01:11<02:11, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|▋| 3.72G/5.68G [01:12<00:37, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.96G/11.4G [01:12<02:11, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|▋| 3.73G/5.68G [01:12<00:37, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▋   | 3.97G/11.4G [01:12<02:10, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|▋| 3.74G/5.68G [01:12<00:36, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▊   | 3.98G/11.4G [01:12<02:10, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▊   | 4.00G/11.4G [01:12<02:09, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|▋| 3.75G/5.68G [01:12<00:36, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▊   | 4.01G/11.4G [01:12<02:10, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  66%|▋| 3.76G/5.68G [01:13<00:36, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▊   | 4.02G/11.4G [01:12<02:11, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.77G/5.68G [01:13<00:36, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  35%|█▊   | 4.03G/11.4G [01:13<02:10, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.79G/5.68G [01:13<00:35, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.04G/11.4G [01:13<02:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.80G/5.68G [01:13<00:35, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.05G/11.4G [01:13<02:09, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.81G/5.68G [01:13<00:35, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.06G/11.4G [01:13<02:08, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.07G/11.4G [01:13<02:08, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.82G/5.68G [01:14<00:48, 38.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.08G/11.4G [01:14<02:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  67%|▋| 3.83G/5.68G [01:14<00:44, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.09G/11.4G [01:14<02:09, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|▋| 3.84G/5.68G [01:14<00:41, 44.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.10G/11.4G [01:14<02:08, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.11G/11.4G [01:14<02:08, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|▋| 3.85G/5.68G [01:14<00:39, 46.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.12G/11.4G [01:14<02:08, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|▋| 3.86G/5.68G [01:15<00:37, 48.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.13G/11.4G [01:14<02:08, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|▋| 3.87G/5.68G [01:15<00:36, 49.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  36%|█▊   | 4.14G/11.4G [01:15<02:07, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  68%|▋| 3.88G/5.68G [01:15<00:35, 50.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.15G/11.4G [01:15<02:08, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.89G/5.68G [01:15<00:34, 51.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.16G/11.4G [01:15<02:09, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.90G/5.68G [01:15<00:34, 51.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.17G/11.4G [01:15<02:09, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.91G/5.68G [01:16<00:34, 51.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.18G/11.4G [01:15<02:08, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.92G/5.68G [01:16<00:33, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.19G/11.4G [01:16<02:08, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.93G/5.68G [01:16<00:33, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.20G/11.4G [01:16<02:07, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  69%|▋| 3.94G/5.68G [01:16<00:33, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.22G/11.4G [01:16<02:07, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|▋| 3.95G/5.68G [01:16<00:32, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.23G/11.4G [01:16<02:06, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|▋| 3.96G/5.68G [01:17<00:32, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.24G/11.4G [01:16<02:07, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|▋| 3.97G/5.68G [01:17<00:32, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.25G/11.4G [01:17<02:08, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|▋| 3.98G/5.68G [01:17<00:32, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  37%|█▊   | 4.26G/11.4G [01:17<02:09, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  70%|▋| 4.00G/5.68G [01:17<00:31, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.27G/11.4G [01:17<02:09, 54.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.01G/5.68G [01:17<00:31, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.28G/11.4G [01:17<02:08, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.02G/5.68G [01:18<00:31, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.29G/11.4G [01:17<02:06, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.03G/5.68G [01:18<00:31, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.30G/11.4G [01:17<02:05, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.31G/11.4G [01:18<02:05, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.04G/5.68G [01:18<00:30, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.32G/11.4G [01:18<02:04, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.05G/5.68G [01:18<00:30, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.33G/11.4G [01:18<02:05, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  71%|▋| 4.06G/5.68G [01:18<00:30, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.34G/11.4G [01:18<02:06, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|▋| 4.07G/5.68G [01:19<00:30, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.35G/11.4G [01:18<02:06, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|▋| 4.08G/5.68G [01:19<00:30, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.36G/11.4G [01:19<02:05, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|▋| 4.09G/5.68G [01:19<00:29, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  38%|█▉   | 4.37G/11.4G [01:19<02:04, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|▋| 4.10G/5.68G [01:19<00:29, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.38G/11.4G [01:19<02:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  72%|▋| 4.11G/5.68G [01:19<00:29, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.39G/11.4G [01:19<02:03, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|▋| 4.12G/5.68G [01:20<00:29, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.40G/11.4G [01:19<02:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|▋| 4.13G/5.68G [01:20<00:29, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.41G/11.4G [01:20<02:04, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|▋| 4.14G/5.68G [01:20<00:29, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|▋| 4.15G/5.68G [01:20<00:28, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.42G/11.4G [01:20<03:02, 38.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  73%|▋| 4.16G/5.68G [01:20<00:28, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.44G/11.4G [01:20<02:45, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.17G/5.68G [01:21<00:28, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.45G/11.4G [01:20<02:31, 45.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.18G/5.68G [01:21<00:28, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.46G/11.4G [01:21<02:22, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.19G/5.68G [01:21<00:28, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.47G/11.4G [01:21<02:15, 50.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.20G/5.68G [01:21<00:28, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.48G/11.4G [01:21<02:11, 52.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.22G/5.68G [01:21<00:27, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  39%|█▉   | 4.49G/11.4G [01:21<02:09, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  74%|▋| 4.23G/5.68G [01:22<00:27, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|█▉   | 4.50G/11.4G [01:21<02:08, 53.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|▋| 4.24G/5.68G [01:22<00:27, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|█▉   | 4.51G/11.4G [01:22<02:05, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|▋| 4.25G/5.68G [01:22<00:27, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|█▉   | 4.52G/11.4G [01:22<02:03, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|▊| 4.26G/5.68G [01:22<00:27, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|█▉   | 4.53G/11.4G [01:22<02:01, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|▊| 4.27G/5.68G [01:22<00:26, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|█▉   | 4.54G/11.4G [01:22<02:07, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  75%|▊| 4.28G/5.68G [01:23<00:26, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██   | 4.55G/11.4G [01:22<02:05, 54.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.29G/5.68G [01:23<00:26, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██   | 4.56G/11.4G [01:22<02:03, 55.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██   | 4.57G/11.4G [01:23<02:02, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.30G/5.68G [01:23<00:26, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██   | 4.58G/11.4G [01:23<02:02, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.31G/5.68G [01:23<00:26, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  40%|██   | 4.59G/11.4G [01:23<02:02, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.32G/5.68G [01:23<00:25, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.60G/11.4G [01:23<02:00, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.33G/5.68G [01:24<00:25, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.61G/11.4G [01:23<02:00, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  76%|▊| 4.34G/5.68G [01:24<00:25, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.62G/11.4G [01:24<01:59, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|▊| 4.35G/5.68G [01:24<00:24, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.63G/11.4G [01:24<01:58, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|▊| 4.36G/5.68G [01:24<00:24, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.65G/11.4G [01:24<01:58, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|▊| 4.37G/5.68G [01:24<00:25, 51.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.66G/11.4G [01:24<01:59, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|▊| 4.38G/5.68G [01:25<00:24, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.67G/11.4G [01:24<02:00, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  77%|▊| 4.39G/5.68G [01:25<00:24, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.68G/11.4G [01:25<02:00, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|▊| 4.40G/5.68G [01:25<00:24, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.69G/11.4G [01:25<01:59, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|▊| 4.41G/5.68G [01:25<00:24, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.70G/11.4G [01:25<01:58, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|▊| 4.42G/5.68G [01:25<00:23, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  41%|██   | 4.71G/11.4G [01:25<01:57, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.72G/11.4G [01:25<01:56, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|▊| 4.44G/5.68G [01:26<00:23, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.73G/11.4G [01:25<01:57, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  78%|▊| 4.45G/5.68G [01:26<00:23, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.74G/11.4G [01:26<01:58, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.46G/5.68G [01:26<00:23, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.75G/11.4G [01:26<01:58, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.47G/5.68G [01:26<00:23, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.76G/11.4G [01:26<01:57, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.48G/5.68G [01:26<00:22, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.77G/11.4G [01:26<01:56, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.49G/5.68G [01:27<00:22, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.78G/11.4G [01:26<01:56, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.50G/5.68G [01:27<00:22, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.79G/11.4G [01:27<01:55, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  79%|▊| 4.51G/5.68G [01:27<00:22, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.80G/11.4G [01:27<01:55, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|▊| 4.52G/5.68G [01:27<00:22, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.81G/11.4G [01:27<01:56, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|▊| 4.53G/5.68G [01:27<00:21, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  42%|██   | 4.82G/11.4G [01:27<01:56, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|▊| 4.54G/5.68G [01:28<00:21, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.83G/11.4G [01:27<01:55, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|▊| 4.55G/5.68G [01:28<00:21, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.84G/11.4G [01:28<01:55, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  80%|▊| 4.56G/5.68G [01:28<00:21, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.85G/11.4G [01:28<01:54, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.87G/11.4G [01:28<01:54, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.57G/5.68G [01:28<00:21, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.88G/11.4G [01:28<01:54, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.58G/5.68G [01:28<00:20, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.89G/11.4G [01:28<01:55, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.59G/5.68G [01:29<00:20, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.90G/11.4G [01:28<01:55, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.60G/5.68G [01:29<00:20, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.91G/11.4G [01:29<01:54, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.61G/5.68G [01:29<00:20, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.92G/11.4G [01:29<01:54, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.93G/11.4G [01:29<01:53, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  81%|▊| 4.62G/5.68G [01:29<00:27, 38.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  43%|██▏  | 4.94G/11.4G [01:29<01:53, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|▊| 4.63G/5.68G [01:30<00:24, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 4.95G/11.4G [01:29<01:53, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|▊| 4.65G/5.68G [01:30<00:22, 44.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 4.96G/11.4G [01:30<01:54, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|▊| 4.66G/5.68G [01:30<00:21, 47.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 4.97G/11.4G [01:30<01:54, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|▊| 4.67G/5.68G [01:30<00:20, 48.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 4.98G/11.4G [01:30<01:53, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  82%|▊| 4.68G/5.68G [01:30<00:20, 49.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 4.99G/11.4G [01:30<01:52, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.00G/11.4G [01:30<01:52, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|▊| 4.69G/5.68G [01:31<00:19, 50.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.01G/11.4G [01:30<01:51, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|▊| 4.70G/5.68G [01:31<00:18, 51.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.02G/11.4G [01:31<01:51, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|▊| 4.71G/5.68G [01:31<00:18, 52.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.03G/11.4G [01:31<01:52, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|▊| 4.72G/5.68G [01:31<00:18, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.04G/11.4G [01:31<01:52, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  83%|▊| 4.73G/5.68G [01:31<00:17, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  44%|██▏  | 5.05G/11.4G [01:31<01:51, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.74G/5.68G [01:32<00:17, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▏  | 5.06G/11.4G [01:31<01:51, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.75G/5.68G [01:32<00:17, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▏  | 5.08G/11.4G [01:32<01:50, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.76G/5.68G [01:32<00:17, 52.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▏  | 5.09G/11.4G [01:32<01:50, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.77G/5.68G [01:32<00:17, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▏  | 5.10G/11.4G [01:32<01:50, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.78G/5.68G [01:32<00:16, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▏  | 5.11G/11.4G [01:32<01:51, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  84%|▊| 4.79G/5.68G [01:33<00:16, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.12G/11.4G [01:32<01:51, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|▊| 4.80G/5.68G [01:33<00:16, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.13G/11.4G [01:33<01:50, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|▊| 4.81G/5.68G [01:33<00:16, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.14G/11.4G [01:33<01:50, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|▊| 4.82G/5.68G [01:33<00:16, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.15G/11.4G [01:33<01:49, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.16G/11.4G [01:33<01:48, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|▊| 4.83G/5.68G [01:33<00:16, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  45%|██▎  | 5.17G/11.4G [01:33<01:49, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  85%|▊| 4.84G/5.68G [01:34<00:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.18G/11.4G [01:33<01:51, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.85G/5.68G [01:34<00:15, 52.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.19G/11.4G [01:34<01:51, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.87G/5.68G [01:34<00:15, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.20G/11.4G [01:34<01:49, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.88G/5.68G [01:34<00:15, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.21G/11.4G [01:34<01:48, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.89G/5.68G [01:34<00:14, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.22G/11.4G [01:34<01:47, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.90G/5.68G [01:35<00:14, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.23G/11.4G [01:34<01:47, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  86%|▊| 4.91G/5.68G [01:35<00:14, 52.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.24G/11.4G [01:35<01:47, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|▊| 4.92G/5.68G [01:35<00:14, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.25G/11.4G [01:35<01:48, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|▊| 4.93G/5.68G [01:35<00:14, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.26G/11.4G [01:35<01:48, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|▊| 4.94G/5.68G [01:35<00:14, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  46%|██▎  | 5.27G/11.4G [01:35<01:49, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|▊| 4.95G/5.68G [01:36<00:13, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.28G/11.4G [01:35<01:47, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  87%|▊| 4.96G/5.68G [01:36<00:13, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.30G/11.4G [01:36<01:48, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 4.97G/5.68G [01:36<00:13, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.31G/11.4G [01:36<01:47, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.32G/11.4G [01:36<01:46, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 4.98G/5.68G [01:36<00:13, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.33G/11.4G [01:36<01:47, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 4.99G/5.68G [01:36<00:13, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.34G/11.4G [01:36<01:50, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 5.00G/5.68G [01:37<00:12, 52.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.35G/11.4G [01:36<01:49, 55.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 5.01G/5.68G [01:37<00:12, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.36G/11.4G [01:37<01:49, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  88%|▉| 5.02G/5.68G [01:37<00:12, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.37G/11.4G [01:37<01:47, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|▉| 5.03G/5.68G [01:37<00:12, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.38G/11.4G [01:37<01:46, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|▉| 5.04G/5.68G [01:37<00:12, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  47%|██▎  | 5.39G/11.4G [01:37<01:45, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|▉| 5.05G/5.68G [01:38<00:11, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.40G/11.4G [01:37<01:44, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|▉| 5.06G/5.68G [01:38<00:11, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.41G/11.4G [01:38<01:45, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  89%|▉| 5.08G/5.68G [01:38<00:11, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.42G/11.4G [01:38<01:46, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|▉| 5.09G/5.68G [01:38<00:11, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.43G/11.4G [01:38<01:46, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|▉| 5.10G/5.68G [01:38<00:11, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.44G/11.4G [01:38<01:45, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|▉| 5.11G/5.68G [01:39<00:10, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.45G/11.4G [01:38<01:44, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|▉| 5.12G/5.68G [01:39<00:10, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.46G/11.4G [01:38<01:43, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.47G/11.4G [01:39<01:43, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  90%|▉| 5.13G/5.68G [01:39<00:10, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.48G/11.4G [01:39<01:43, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.14G/5.68G [01:39<00:10, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.49G/11.4G [01:39<01:44, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.15G/5.68G [01:39<00:10, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  48%|██▍  | 5.51G/11.4G [01:39<01:44, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.16G/5.68G [01:40<00:09, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.52G/11.4G [01:39<01:44, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.17G/5.68G [01:40<00:09, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.53G/11.4G [01:40<01:43, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.18G/5.68G [01:40<00:09, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.54G/11.4G [01:40<01:42, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  91%|▉| 5.19G/5.68G [01:40<00:09, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.55G/11.4G [01:40<01:42, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|▉| 5.20G/5.68G [01:40<00:09, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.56G/11.4G [01:40<01:42, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|▉| 5.21G/5.68G [01:41<00:08, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.57G/11.4G [01:40<01:42, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|▉| 5.22G/5.68G [01:41<00:08, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.58G/11.4G [01:41<01:43, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|▉| 5.23G/5.68G [01:41<00:08, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.59G/11.4G [01:41<01:43, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  92%|▉| 5.24G/5.68G [01:41<00:08, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.60G/11.4G [01:41<01:42, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.25G/5.68G [01:41<00:08, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.61G/11.4G [01:41<01:41, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.26G/5.68G [01:42<00:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  49%|██▍  | 5.62G/11.4G [01:41<01:40, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▍  | 5.63G/11.4G [01:41<01:41, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.27G/5.68G [01:42<00:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▍  | 5.64G/11.4G [01:42<01:41, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.28G/5.68G [01:42<00:07, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▍  | 5.65G/11.4G [01:42<01:42, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.30G/5.68G [01:42<00:07, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▍  | 5.66G/11.4G [01:42<01:41, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  93%|▉| 5.31G/5.68G [01:42<00:07, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▍  | 5.67G/11.4G [01:42<01:40, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|▉| 5.32G/5.68G [01:43<00:06, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.68G/11.4G [01:42<01:40, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|▉| 5.33G/5.68G [01:43<00:06, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|▉| 5.34G/5.68G [01:43<00:06, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.69G/11.4G [01:43<02:25, 39.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|▉| 5.35G/5.68G [01:43<00:06, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.70G/11.4G [01:43<02:12, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  94%|▉| 5.36G/5.68G [01:43<00:06, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.71G/11.4G [01:43<02:03, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|▉| 5.37G/5.68G [01:44<00:05, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.73G/11.4G [01:43<01:55, 48.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|▉| 5.38G/5.68G [01:44<00:05, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  50%|██▌  | 5.74G/11.4G [01:44<01:50, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|▉| 5.39G/5.68G [01:44<00:05, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.75G/11.4G [01:44<01:46, 52.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|▉| 5.40G/5.68G [01:44<00:05, 52.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.76G/11.4G [01:44<01:43, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.77G/11.4G [01:44<01:42, 54.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  95%|▉| 5.41G/5.68G [01:45<00:06, 37.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.78G/11.4G [01:44<01:42, 54.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.42G/5.68G [01:45<00:06, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.79G/11.4G [01:45<01:41, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.43G/5.68G [01:45<00:05, 44.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.80G/11.4G [01:45<01:40, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.81G/11.4G [01:45<01:39, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.44G/5.68G [01:45<00:04, 46.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.82G/11.4G [01:45<01:38, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.45G/5.68G [01:45<00:04, 48.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.83G/11.4G [01:45<01:37, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.46G/5.68G [01:46<00:04, 49.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.84G/11.4G [01:45<01:38, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  96%|▉| 5.47G/5.68G [01:46<00:03, 51.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  51%|██▌  | 5.85G/11.4G [01:46<01:38, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|▉| 5.48G/5.68G [01:46<00:03, 51.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.86G/11.4G [01:46<01:38, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|▉| 5.49G/5.68G [01:46<00:03, 52.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.87G/11.4G [01:46<01:37, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|▉| 5.51G/5.68G [01:46<00:03, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.88G/11.4G [01:46<01:36, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|▉| 5.52G/5.68G [01:47<00:03, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.89G/11.4G [01:46<01:36, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  97%|▉| 5.53G/5.68G [01:47<00:02, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.90G/11.4G [01:47<01:35, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.54G/5.68G [01:47<00:02, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.91G/11.4G [01:47<01:36, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.55G/5.68G [01:47<00:02, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.92G/11.4G [01:47<01:36, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.56G/5.68G [01:47<00:02, 53.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.93G/11.4G [01:47<01:37, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.57G/5.68G [01:48<00:02, 53.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.95G/11.4G [01:47<01:36, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.58G/5.68G [01:48<00:01, 53.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  52%|██▌  | 5.96G/11.4G [01:48<01:35, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  98%|▉| 5.59G/5.68G [01:48<00:01, 52.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 5.97G/11.4G [01:48<01:34, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 5.98G/11.4G [01:48<01:34, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|▉| 5.60G/5.68G [01:48<00:01, 52.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 5.99G/11.4G [01:48<01:35, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|▉| 5.61G/5.68G [01:48<00:01, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.00G/11.4G [01:48<01:35, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|▉| 5.62G/5.68G [01:49<00:01, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.01G/11.4G [01:48<01:35, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|▉| 5.63G/5.68G [01:49<00:00, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.02G/11.4G [01:49<01:34, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors:  99%|▉| 5.64G/5.68G [01:49<00:00, 52.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.03G/11.4G [01:49<01:34, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors: 100%|▉| 5.65G/5.68G [01:49<00:00, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.04G/11.4G [01:49<01:33, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors: 100%|▉| 5.66G/5.68G [01:49<00:00, 52.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.05G/11.4G [01:49<01:33, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "diffusion_pytorch_model.safetensors: 100%|▉| 5.67G/5.68G [01:50<00:00, 52.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "diffusion_pytorch_model.safetensors: 100%|█| 5.68G/5.68G [01:50<00:00, 51.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors\n",
      "Fetching 22 files:  73%|█████████████████▍      | 16/22 [01:51<00:45,  7.55s/it]\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  53%|██▋  | 6.07G/11.4G [01:50<01:34, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.08G/11.4G [01:50<01:34, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.09G/11.4G [01:50<01:33, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.10G/11.4G [01:50<01:33, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.11G/11.4G [01:50<01:32, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.12G/11.4G [01:50<01:32, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.13G/11.4G [01:51<01:33, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.14G/11.4G [01:51<01:34, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.16G/11.4G [01:51<01:34, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.17G/11.4G [01:51<01:34, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.18G/11.4G [01:51<01:32, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  54%|██▋  | 6.19G/11.4G [01:52<01:32, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▋  | 6.20G/11.4G [01:52<01:31, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▋  | 6.21G/11.4G [01:52<01:30, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▋  | 6.22G/11.4G [01:52<01:30, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▋  | 6.23G/11.4G [01:52<01:30, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▋  | 6.24G/11.4G [01:53<01:31, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.25G/11.4G [01:53<01:30, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.26G/11.4G [01:53<01:30, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.27G/11.4G [01:53<01:29, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.28G/11.4G [01:53<01:29, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.29G/11.4G [01:53<01:29, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  55%|██▊  | 6.30G/11.4G [01:54<01:29, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.31G/11.4G [01:54<01:30, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.32G/11.4G [01:54<01:29, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.33G/11.4G [01:54<01:28, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.34G/11.4G [01:54<01:28, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.35G/11.4G [01:55<01:28, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.36G/11.4G [01:55<01:28, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.38G/11.4G [01:55<01:28, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.39G/11.4G [01:55<01:29, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.40G/11.4G [01:55<01:28, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.41G/11.4G [01:56<01:27, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  56%|██▊  | 6.42G/11.4G [01:56<01:26, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.43G/11.4G [01:56<01:26, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.44G/11.4G [01:56<01:27, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.45G/11.4G [01:56<01:27, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.46G/11.4G [01:56<01:26, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.47G/11.4G [01:57<01:25, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.48G/11.4G [01:57<01:25, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.49G/11.4G [01:57<01:25, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.50G/11.4G [01:57<01:24, 57.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.51G/11.4G [01:58<02:05, 38.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.52G/11.4G [01:58<01:52, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  57%|██▊  | 6.53G/11.4G [01:58<01:43, 46.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.54G/11.4G [01:58<01:37, 49.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.55G/11.4G [01:58<01:33, 51.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.56G/11.4G [01:59<01:30, 52.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.57G/11.4G [01:59<01:29, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.59G/11.4G [01:59<01:27, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.60G/11.4G [01:59<01:25, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.61G/11.4G [01:59<01:24, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.62G/11.4G [01:59<01:24, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.63G/11.4G [02:00<01:24, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  58%|██▉  | 6.64G/11.4G [02:00<01:24, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.65G/11.4G [02:00<01:23, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.66G/11.4G [02:00<01:22, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.67G/11.4G [02:00<01:22, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.68G/11.4G [02:01<01:22, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.69G/11.4G [02:01<01:21, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.70G/11.4G [02:01<01:22, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.71G/11.4G [02:01<01:22, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.72G/11.4G [02:01<01:22, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.73G/11.4G [02:02<01:21, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.74G/11.4G [02:02<01:21, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  59%|██▉  | 6.75G/11.4G [02:02<01:20, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.76G/11.4G [02:02<01:21, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.77G/11.4G [02:02<01:21, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.78G/11.4G [02:02<01:21, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.79G/11.4G [02:03<01:21, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.81G/11.4G [02:03<01:21, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|██▉  | 6.82G/11.4G [02:03<01:20, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███  | 6.83G/11.4G [02:03<01:20, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███  | 6.84G/11.4G [02:03<01:20, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███  | 6.85G/11.4G [02:04<01:20, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███  | 6.86G/11.4G [02:04<01:20, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  60%|███  | 6.87G/11.4G [02:04<01:19, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.88G/11.4G [02:04<01:19, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.89G/11.4G [02:04<01:18, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.90G/11.4G [02:04<01:18, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.91G/11.4G [02:05<01:18, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.92G/11.4G [02:05<01:19, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.93G/11.4G [02:05<01:19, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.94G/11.4G [02:05<01:19, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.95G/11.4G [02:05<01:18, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.96G/11.4G [02:06<01:18, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.97G/11.4G [02:06<01:17, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  61%|███  | 6.98G/11.4G [02:06<01:17, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 6.99G/11.4G [02:06<01:17, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.00G/11.4G [02:06<01:17, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.01G/11.4G [02:07<01:17, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.03G/11.4G [02:07<01:17, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.04G/11.4G [02:07<01:16, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.05G/11.4G [02:07<01:16, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.06G/11.4G [02:07<01:15, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.07G/11.4G [02:07<01:15, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.08G/11.4G [02:08<01:16, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.09G/11.4G [02:08<01:16, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  62%|███  | 7.10G/11.4G [02:08<01:16, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.11G/11.4G [02:08<01:15, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.12G/11.4G [02:08<01:14, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.13G/11.4G [02:09<01:14, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.14G/11.4G [02:09<01:14, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.15G/11.4G [02:09<01:14, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.16G/11.4G [02:09<01:14, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.17G/11.4G [02:09<01:14, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.18G/11.4G [02:10<01:13, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.19G/11.4G [02:10<01:13, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.20G/11.4G [02:10<01:13, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  63%|███▏ | 7.21G/11.4G [02:10<01:13, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.22G/11.4G [02:10<01:13, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.24G/11.4G [02:10<01:13, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.25G/11.4G [02:11<01:12, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.26G/11.4G [02:11<01:12, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.27G/11.4G [02:11<01:11, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.28G/11.4G [02:11<01:11, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.29G/11.4G [02:11<01:12, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.30G/11.4G [02:12<01:12, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.31G/11.4G [02:12<01:12, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  64%|███▏ | 7.32G/11.4G [02:12<01:11, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.33G/11.4G [02:12<01:11, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.34G/11.4G [02:12<01:10, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.35G/11.4G [02:12<01:10, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.36G/11.4G [02:13<01:10, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.37G/11.4G [02:13<01:11, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▏ | 7.38G/11.4G [02:13<01:11, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▎ | 7.39G/11.4G [02:13<01:10, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▎ | 7.40G/11.4G [02:13<01:09, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▎ | 7.41G/11.4G [02:14<01:09, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▎ | 7.42G/11.4G [02:14<01:08, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  65%|███▎ | 7.43G/11.4G [02:14<01:09, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.44G/11.4G [02:14<01:09, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.46G/11.4G [02:14<01:09, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.47G/11.4G [02:15<01:08, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.48G/11.4G [02:15<01:08, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.49G/11.4G [02:15<01:07, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.50G/11.4G [02:15<01:07, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.51G/11.4G [02:15<01:08, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.52G/11.4G [02:15<01:08, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.53G/11.4G [02:16<01:07, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.54G/11.4G [02:16<01:07, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  66%|███▎ | 7.55G/11.4G [02:16<01:06, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.56G/11.4G [02:16<01:06, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.57G/11.4G [02:16<01:07, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.58G/11.4G [02:17<01:07, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.59G/11.4G [02:17<01:07, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.60G/11.4G [02:17<01:06, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.61G/11.4G [02:17<01:06, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.62G/11.4G [02:17<01:05, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.63G/11.4G [02:18<01:36, 38.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.64G/11.4G [02:18<01:27, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.65G/11.4G [02:18<01:20, 45.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  67%|███▎ | 7.67G/11.4G [02:18<01:15, 48.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.68G/11.4G [02:19<01:12, 51.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.69G/11.4G [02:19<01:09, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.70G/11.4G [02:19<01:07, 54.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.71G/11.4G [02:19<01:06, 54.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.72G/11.4G [02:19<01:06, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.73G/11.4G [02:19<01:05, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.74G/11.4G [02:20<01:04, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.75G/11.4G [02:20<01:03, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.76G/11.4G [02:20<01:03, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.77G/11.4G [02:20<01:03, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  68%|███▍ | 7.78G/11.4G [02:20<01:03, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.79G/11.4G [02:21<01:03, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.80G/11.4G [02:21<01:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.81G/11.4G [02:21<01:02, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.82G/11.4G [02:21<01:02, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.83G/11.4G [02:21<01:01, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.84G/11.4G [02:21<01:02, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.85G/11.4G [02:22<01:02, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.86G/11.4G [02:22<01:01, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.87G/11.4G [02:22<01:01, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.89G/11.4G [02:22<01:00, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  69%|███▍ | 7.90G/11.4G [02:22<01:00, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▍ | 7.91G/11.4G [02:23<01:00, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▍ | 7.92G/11.4G [02:23<01:00, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▍ | 7.93G/11.4G [02:23<01:01, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▍ | 7.94G/11.4G [02:23<01:00, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▍ | 7.95G/11.4G [02:23<01:00, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▌ | 7.96G/11.4G [02:23<00:59, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▌ | 7.97G/11.4G [02:24<00:59, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▌ | 7.98G/11.4G [02:24<00:59, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▌ | 7.99G/11.4G [02:24<01:00, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  70%|███▌ | 8.00G/11.4G [02:24<01:00, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.01G/11.4G [02:24<00:59, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.02G/11.4G [02:25<00:59, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.03G/11.4G [02:25<00:58, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.04G/11.4G [02:25<00:58, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.05G/11.4G [02:25<00:58, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.06G/11.4G [02:25<00:58, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.07G/11.4G [02:26<00:58, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.08G/11.4G [02:26<00:58, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.10G/11.4G [02:26<00:57, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.11G/11.4G [02:26<00:57, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  71%|███▌ | 8.12G/11.4G [02:26<00:57, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.13G/11.4G [02:26<00:57, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.14G/11.4G [02:27<00:57, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.15G/11.4G [02:27<00:57, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.16G/11.4G [02:27<00:56, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.17G/11.4G [02:27<00:56, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.18G/11.4G [02:27<00:55, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.19G/11.4G [02:28<00:55, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.20G/11.4G [02:28<00:55, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.21G/11.4G [02:28<00:55, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.22G/11.4G [02:28<00:55, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  72%|███▌ | 8.23G/11.4G [02:28<00:55, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.24G/11.4G [02:29<00:55, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.25G/11.4G [02:29<00:54, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.26G/11.4G [02:29<00:54, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.27G/11.4G [02:29<00:54, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.28G/11.4G [02:29<00:54, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.29G/11.4G [02:29<00:54, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.30G/11.4G [02:30<00:54, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.32G/11.4G [02:30<01:17, 39.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.33G/11.4G [02:30<01:10, 43.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.34G/11.4G [02:30<01:05, 46.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  73%|███▋ | 8.35G/11.4G [02:31<01:02, 48.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.36G/11.4G [02:31<00:58, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.37G/11.4G [02:31<00:56, 52.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.38G/11.4G [02:31<00:55, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.39G/11.4G [02:31<00:54, 55.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.40G/11.4G [02:32<00:53, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.41G/11.4G [02:32<00:53, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.42G/11.4G [02:32<00:52, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.43G/11.4G [02:32<00:52, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.44G/11.4G [02:32<00:51, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.45G/11.4G [02:32<00:51, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  74%|███▋ | 8.46G/11.4G [02:33<00:51, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▋ | 8.47G/11.4G [02:33<00:51, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▋ | 8.48G/11.4G [02:33<00:51, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▋ | 8.49G/11.4G [02:33<00:51, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▋ | 8.50G/11.4G [02:33<00:50, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▋ | 8.51G/11.4G [02:34<00:50, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.52G/11.4G [02:34<00:50, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.54G/11.4G [02:34<00:50, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.55G/11.4G [02:34<00:49, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.56G/11.4G [02:34<00:49, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.57G/11.4G [02:35<00:49, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  75%|███▊ | 8.58G/11.4G [02:35<00:49, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.59G/11.4G [02:35<00:49, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.60G/11.4G [02:35<00:49, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.61G/11.4G [02:35<00:48, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.62G/11.4G [02:35<00:48, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.63G/11.4G [02:36<00:48, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.64G/11.4G [02:36<00:48, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.65G/11.4G [02:36<00:48, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.66G/11.4G [02:36<00:48, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.67G/11.4G [02:36<00:47, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  76%|███▊ | 8.68G/11.4G [02:37<00:47, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.69G/11.4G [02:37<00:46, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.70G/11.4G [02:37<00:47, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.71G/11.4G [02:37<00:47, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.72G/11.4G [02:37<00:47, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.73G/11.4G [02:38<00:46, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.75G/11.4G [02:38<00:46, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.76G/11.4G [02:38<00:45, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.77G/11.4G [02:38<00:45, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.78G/11.4G [02:38<00:45, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.79G/11.4G [02:38<00:45, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  77%|███▊ | 8.80G/11.4G [02:39<00:46, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.81G/11.4G [02:39<00:45, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.82G/11.4G [02:39<00:45, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.83G/11.4G [02:39<00:44, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.84G/11.4G [02:39<00:44, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.85G/11.4G [02:40<00:44, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.86G/11.4G [02:40<00:44, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.87G/11.4G [02:40<00:44, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.88G/11.4G [02:40<00:44, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.89G/11.4G [02:40<00:43, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.90G/11.4G [02:41<00:43, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  78%|███▉ | 8.91G/11.4G [02:41<00:43, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.92G/11.4G [02:41<00:42, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.93G/11.4G [02:41<00:43, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.94G/11.4G [02:41<00:43, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.95G/11.4G [02:41<00:43, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.97G/11.4G [02:42<00:42, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.98G/11.4G [02:42<00:42, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 8.99G/11.4G [02:42<00:41, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 9.00G/11.4G [02:42<00:41, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 9.01G/11.4G [02:42<00:41, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 9.02G/11.4G [02:43<00:41, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  79%|███▉ | 9.03G/11.4G [02:43<00:41, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|███▉ | 9.04G/11.4G [02:43<00:41, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|███▉ | 9.05G/11.4G [02:43<00:40, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|███▉ | 9.06G/11.4G [02:43<00:40, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|███▉ | 9.07G/11.4G [02:43<00:40, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|███▉ | 9.08G/11.4G [02:44<00:40, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.09G/11.4G [02:44<00:40, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.10G/11.4G [02:44<00:40, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.11G/11.4G [02:44<00:40, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.12G/11.4G [02:44<00:39, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.13G/11.4G [02:45<00:39, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  80%|████ | 9.14G/11.4G [02:45<00:39, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.15G/11.4G [02:45<00:39, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.16G/11.4G [02:45<00:39, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.18G/11.4G [02:45<00:39, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.19G/11.4G [02:46<00:39, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.20G/11.4G [02:46<00:38, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.21G/11.4G [02:46<00:38, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.22G/11.4G [02:46<00:38, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.23G/11.4G [02:46<00:37, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.24G/11.4G [02:46<00:37, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.25G/11.4G [02:47<00:37, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  81%|████ | 9.26G/11.4G [02:47<00:37, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.27G/11.4G [02:47<00:37, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.28G/11.4G [02:47<00:36, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.29G/11.4G [02:47<00:36, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.30G/11.4G [02:48<00:36, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.31G/11.4G [02:48<00:36, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.32G/11.4G [02:48<00:36, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.33G/11.4G [02:48<00:36, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.34G/11.4G [02:48<00:35, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.35G/11.4G [02:49<00:35, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  82%|████ | 9.36G/11.4G [02:49<00:35, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.37G/11.4G [02:49<00:34, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.38G/11.4G [02:49<00:34, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.40G/11.4G [02:50<00:51, 38.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.41G/11.4G [02:50<00:46, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.42G/11.4G [02:50<00:42, 45.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.43G/11.4G [02:50<00:39, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.44G/11.4G [02:50<00:37, 50.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.45G/11.4G [02:50<00:36, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.46G/11.4G [02:51<00:35, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.47G/11.4G [02:51<00:35, 53.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  83%|████▏| 9.48G/11.4G [02:51<00:34, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.49G/11.4G [02:51<00:33, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.50G/11.4G [02:51<00:33, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.51G/11.4G [02:52<00:32, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.52G/11.4G [02:52<00:32, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.53G/11.4G [02:52<00:32, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.54G/11.4G [02:52<00:32, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.55G/11.4G [02:52<00:32, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.56G/11.4G [02:53<00:32, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.57G/11.4G [02:53<00:31, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.58G/11.4G [02:53<00:31, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  84%|████▏| 9.59G/11.4G [02:53<00:31, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.60G/11.4G [02:53<00:31, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.62G/11.4G [02:53<00:31, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.63G/11.4G [02:54<00:31, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.64G/11.4G [02:54<00:30, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.65G/11.4G [02:54<00:30, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▏| 9.66G/11.4G [02:54<00:30, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▎| 9.67G/11.4G [02:54<00:29, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▎| 9.68G/11.4G [02:55<00:30, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▎| 9.69G/11.4G [02:55<00:29, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▎| 9.70G/11.4G [02:55<00:30, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  85%|████▎| 9.71G/11.4G [02:55<00:29, 55.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.72G/11.4G [02:55<00:29, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.73G/11.4G [02:56<00:28, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.74G/11.4G [02:56<00:28, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.75G/11.4G [02:56<00:28, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.76G/11.4G [02:56<00:28, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.77G/11.4G [02:56<00:28, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.78G/11.4G [02:56<00:28, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.79G/11.4G [02:57<00:27, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.80G/11.4G [02:57<00:27, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.81G/11.4G [02:57<00:27, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  86%|████▎| 9.83G/11.4G [02:57<00:26, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.84G/11.4G [02:57<00:27, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.85G/11.4G [02:58<00:26, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.86G/11.4G [02:58<00:26, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.87G/11.4G [02:58<00:26, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.88G/11.4G [02:58<00:26, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.89G/11.4G [02:58<00:25, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.90G/11.4G [02:58<00:25, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.91G/11.4G [02:59<00:25, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.92G/11.4G [02:59<00:25, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.93G/11.4G [02:59<00:25, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  87%|████▎| 9.94G/11.4G [02:59<00:25, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 9.95G/11.4G [02:59<00:24, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 9.96G/11.4G [03:00<00:24, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 9.97G/11.4G [03:00<00:24, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 9.98G/11.4G [03:00<00:24, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 9.99G/11.4G [03:00<00:24, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 10.0G/11.4G [03:00<00:24, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 10.0G/11.4G [03:01<00:27, 48.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 10.0G/11.4G [03:01<00:26, 50.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 10.0G/11.4G [03:01<00:25, 52.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  88%|████▍| 10.0G/11.4G [03:01<00:24, 53.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:01<00:23, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:02<00:23, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:02<00:23, 54.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:02<00:22, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:02<00:33, 38.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:03<00:29, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:03<00:27, 46.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:03<00:25, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.1G/11.4G [03:03<00:24, 50.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.2G/11.4G [03:03<00:23, 52.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  89%|████▍| 10.2G/11.4G [03:04<00:22, 53.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:04<00:22, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:04<00:21, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:04<00:20, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:04<00:20, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:04<00:20, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▍| 10.2G/11.4G [03:05<00:20, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▌| 10.2G/11.4G [03:05<00:19, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▌| 10.2G/11.4G [03:05<00:19, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▌| 10.3G/11.4G [03:05<00:19, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▌| 10.3G/11.4G [03:05<00:19, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  90%|████▌| 10.3G/11.4G [03:06<00:19, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:06<00:19, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:06<00:18, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:06<00:18, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:06<00:18, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:06<00:18, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:07<00:18, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.3G/11.4G [03:07<00:18, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.4G/11.4G [03:07<00:17, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.4G/11.4G [03:07<00:17, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.4G/11.4G [03:07<00:17, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  91%|████▌| 10.4G/11.4G [03:08<00:17, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.4G/11.4G [03:08<00:16, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.4G/11.4G [03:08<00:16, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.4G/11.4G [03:08<00:16, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.4G/11.4G [03:08<00:16, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.4G/11.4G [03:09<00:16, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:09<00:16, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:09<00:15, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:09<00:15, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:09<00:15, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:09<00:15, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  92%|████▌| 10.5G/11.4G [03:10<00:15, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.5G/11.4G [03:10<00:14, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.5G/11.4G [03:10<00:14, 58.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.5G/11.4G [03:10<00:14, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.5G/11.4G [03:10<00:14, 57.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:14, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:14, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:13, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:13, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:13, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:11<00:13, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  93%|████▋| 10.6G/11.4G [03:12<00:13, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.6G/11.4G [03:12<00:12, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.6G/11.4G [03:12<00:12, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:12<00:12, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:12<00:12, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:13<00:12, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:13<00:11, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:13<00:11, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:13<00:11, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:13<00:11, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  94%|████▋| 10.7G/11.4G [03:14<00:11, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.7G/11.4G [03:14<00:11, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.7G/11.4G [03:14<00:10, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.8G/11.4G [03:14<00:10, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.8G/11.4G [03:14<00:10, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.8G/11.4G [03:14<00:10, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▋| 10.8G/11.4G [03:15<00:10, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▊| 10.8G/11.4G [03:15<00:09, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▊| 10.8G/11.4G [03:15<00:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▊| 10.8G/11.4G [03:15<00:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▊| 10.8G/11.4G [03:15<00:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  95%|████▊| 10.8G/11.4G [03:16<00:09, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:16<00:09, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:16<00:08, 55.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:16<00:08, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:16<00:08, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:08, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:08, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:07, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:07, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:07, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 10.9G/11.4G [03:17<00:07, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  96%|████▊| 11.0G/11.4G [03:18<00:07, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:18<00:06, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:18<00:06, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:18<00:06, 56.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:18<00:06, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:19<00:06, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:19<00:06, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:19<00:05, 56.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.0G/11.4G [03:19<00:05, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.1G/11.4G [03:19<00:05, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.1G/11.4G [03:19<00:05, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  97%|████▊| 11.1G/11.4G [03:20<00:05, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:20<00:04, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:20<00:04, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:20<00:04, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:20<00:04, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:21<00:04, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:21<00:04, 56.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.1G/11.4G [03:21<00:03, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.2G/11.4G [03:21<00:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.2G/11.4G [03:21<00:03, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.2G/11.4G [03:22<00:03, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  98%|████▉| 11.2G/11.4G [03:22<00:04, 37.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.2G/11.4G [03:22<00:03, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.2G/11.4G [03:22<00:03, 45.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.2G/11.4G [03:23<00:02, 48.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.2G/11.4G [03:23<00:02, 50.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.2G/11.4G [03:23<00:02, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:23<00:02, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:23<00:01, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:24<00:01, 54.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:24<00:01, 55.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:24<00:01, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth:  99%|████▉| 11.3G/11.4G [03:24<00:01, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|████▉| 11.3G/11.4G [03:24<00:00, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|████▉| 11.3G/11.4G [03:24<00:00, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|████▉| 11.3G/11.4G [03:25<00:00, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|████▉| 11.3G/11.4G [03:25<00:00, 56.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|████▉| 11.4G/11.4G [03:25<00:00, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "models_t5_umt5-xxl-enc-bf16.pth: 100%|█████| 11.4G/11.4G [03:25<00:00, 55.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n",
      "Fetching 22 files: 100%|████████████████████████| 22/22 [03:27<00:00,  9.41s/it]\n",
      "/workspace/Wan2.1-T2V-1.3B\n"
     ]
    }
   ],
   "source": [
    "!pip install \"huggingface_hub[cli]\"\n",
    "!mkdir Wan2.1-T2V-1.3B\n",
    "!huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./Wan2.1-T2V-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.7.0+cu128 torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install ./flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSE-cp311-cp311-linux_x86_64.whl -q\n",
    "import flash_attn_2_cuda as flash_attn_gpu\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "cziQM03zMHO4",
    "outputId": "33f63a1d-59f5-4a19-ccd1-80175ab4a3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# dataset = '{dataset_path.replace(os.sep, \"/\")}'\n",
    "import os\n",
    "import torch\n",
    "# # use bfloat16 for the entire notebook\n",
    "# torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "# if torch.cuda.get_device_properties(0).major >= 8:\n",
    "#     # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "#     torch.backends.cuda.matmul.allow_tf32 = True\n",
    "#     torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import imageio.v2 as iio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from loguru import logger as guru\n",
    "\n",
    "# from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# Florence model import - required dependency\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "def extract_video_name(path):\n",
    "  \"\"\"Intelligently recognize video name from path\"\"\"\n",
    "  if not path:\n",
    "      return \"sequence\"\n",
    "\n",
    "  # If path contains video_frames_, it means extracted from video\n",
    "  if \"video_frames_\" in path:\n",
    "      # Use timestamp as video name\n",
    "      import re\n",
    "      match = re.search(r'video_frames_(\\d+)', path)\n",
    "      if match:\n",
    "          timestamp = match.group(1)\n",
    "          return f\"video_{timestamp}\"\n",
    "\n",
    "  # Otherwise use directory name\n",
    "  path_obj = Path(path)\n",
    "  dir_name = path_obj.name\n",
    "\n",
    "  # If directory name is empty or special directory, use parent directory name\n",
    "  if not dir_name or dir_name in ['.', '..']:\n",
    "      dir_name = path_obj.parent.name\n",
    "\n",
    "  # Clean directory name, remove possible special characters\n",
    "  import re\n",
    "  clean_name = re.sub(r'[^\\w\\-_]', '_', dir_name)\n",
    "\n",
    "  return clean_name if clean_name else \"sequence\"\n",
    "\n",
    "def isimage(p):\n",
    "    ext = os.path.splitext(p.lower())[-1]\n",
    "    return ext in [\".png\", \".jpg\", \".jpeg\"]\n",
    "def set_img_dir(img_dir: str) -> int:\n",
    "    # self._clear_image()\n",
    "    image = None\n",
    "    frame_index = 0\n",
    "    cur_mask = None\n",
    "    cur_logit = None\n",
    "    masks_all = []\n",
    "    bbox_masks_all = []\n",
    "\n",
    "    guru.info(f\"Scanning directory: {img_dir}\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        guru.error(f\"Directory does not exist: {img_dir}\")\n",
    "        return 0\n",
    "\n",
    "    all_files = os.listdir(img_dir)\n",
    "    guru.info(f\"Total {len(all_files)} files in directory: {all_files}\")\n",
    "\n",
    "    img_paths = [\n",
    "        os.path.abspath(os.path.join(img_dir, p)) for p in sorted(all_files) if isimage(p)\n",
    "    ]\n",
    "\n",
    "    guru.info(f\"Found {len(img_paths)} image files: {[os.path.basename(p) for p in img_paths]}\")\n",
    "\n",
    "    return len(img_paths)\n",
    "\n",
    "\n",
    "def validate_frame_count(frames):\n",
    "    \"\"\"Validate if frame count follows 4N+1 format and is within 5-81 range\"\"\"\n",
    "    if frames < 5 or frames > 81:\n",
    "        return False, f\"Frame count must be between 5-81, current value: {frames}\"\n",
    "\n",
    "    if (frames - 1) % 4 != 0:\n",
    "        # Find closest valid value\n",
    "        valid_values = [4*n + 1 for n in range(1, 21) if 5 <= 4*n + 1 <= 81]\n",
    "        closest = min(valid_values, key=lambda x: abs(x - frames))\n",
    "        return False, f\"Frame count must follow 4N+1 format (N is positive integer), suggested: {closest}\"\n",
    "\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def resize_and_crop_frame(frame, target_width, target_height):\n",
    "    \"\"\"Resize and crop frame, following resize_video.py logic\"\"\"\n",
    "    if target_width <= 0 or target_height <= 0:\n",
    "        return frame\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # If already target size, return directly\n",
    "    if w == target_width and h == target_height:\n",
    "        return frame\n",
    "\n",
    "    # Determine if landscape or portrait\n",
    "    is_landscape = w >= h\n",
    "    target_is_landscape = target_width >= target_height\n",
    "\n",
    "    # Calculate scaling and cropping\n",
    "    if is_landscape == target_is_landscape:\n",
    "        # Same orientation\n",
    "        if w / h > target_width / target_height:\n",
    "            # Width ratio is larger, adjust height first\n",
    "            new_height = target_height\n",
    "            new_width = int(w * (target_height / h))\n",
    "            frame = cv2.resize(frame, (new_width, new_height))\n",
    "            # Crop center part\n",
    "            start_x = (new_width - target_width) // 2\n",
    "            frame = frame[:, start_x:start_x + target_width]\n",
    "        else:\n",
    "            # Height ratio is larger, adjust width first\n",
    "            new_width = target_width\n",
    "            new_height = int(h * (target_width / w))\n",
    "            frame = cv2.resize(frame, (new_width, new_height))\n",
    "            # Crop center part\n",
    "            start_y = (new_height - target_height) // 2\n",
    "            frame = frame[start_y:start_y + target_height, :]\n",
    "    else:\n",
    "        # Different orientation, need rotation or special handling\n",
    "        # Simple handling here: scale and crop according to target ratio\n",
    "        scale_w = target_width / w\n",
    "        scale_h = target_height / h\n",
    "        scale = max(scale_w, scale_h)  # Choose larger scale ratio to ensure coverage\n",
    "\n",
    "        new_width = int(w * scale)\n",
    "        new_height = int(h * scale)\n",
    "        frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "        # Center crop\n",
    "        start_x = max(0, (new_width - target_width) // 2)\n",
    "        start_y = max(0, (new_height - target_height) // 2)\n",
    "        end_x = min(new_width, start_x + target_width)\n",
    "        end_y = min(new_height, start_y + target_height)\n",
    "\n",
    "        frame = frame[start_y:end_y, start_x:end_x]\n",
    "\n",
    "        # If cropped size is insufficient, add padding (black borders)\n",
    "        if frame.shape[:2] != (target_height, target_width):\n",
    "            padded_frame = np.zeros((target_height, target_width, 3), dtype=frame.dtype)\n",
    "            h_offset = (target_height - frame.shape[0]) // 2\n",
    "            w_offset = (target_width - frame.shape[1]) // 2\n",
    "            padded_frame[h_offset:h_offset+frame.shape[0], w_offset:w_offset+frame.shape[1]] = frame\n",
    "            frame = padded_frame\n",
    "\n",
    "    return frame\n",
    "print('done')\n",
    "# extract_button.click(\n",
    "#     actually_extract_frames,\n",
    "#     [input_video_field, video_target_frames, video_target_width, video_target_height],\n",
    "#     [instruction, extracted_dir_field, frame_index, input_image, mask_dir_field]\n",
    "# )\n",
    "def actually_extract_frames(video_file, target_frames, target_width, target_height):\n",
    "  if video_file is None:\n",
    "      error_msg = \"Please upload a video file first\"\n",
    "      return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "  # Validate frame count\n",
    "  is_valid, error_msg = validate_frame_count(int(target_frames))\n",
    "  if not is_valid:\n",
    "      return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "  # Create temporary directory to store extracted frames\n",
    "  temp_dir = os.path.join(\"./tmp\", f\"video_frames_{int(time.time())}\")\n",
    "  os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "  try:\n",
    "      # Get video information\n",
    "      cap = cv2.VideoCapture(video_file)\n",
    "      if not cap.isOpened():\n",
    "          error_msg = \"Cannot open video file\"\n",
    "          return error_msg, None, None, \"./processed_data/sequence\"\n",
    "\n",
    "      orig_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "      orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "      orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "      # Determine final frame count\n",
    "      final_frames = int(target_frames)\n",
    "      final_width = int(target_width)\n",
    "      final_height = int(target_height)\n",
    "\n",
    "      # Calculate sampling interval\n",
    "      if final_frames >= orig_frame_count:\n",
    "          # Extract all frames\n",
    "          frame_indices = list(range(orig_frame_count))\n",
    "      else:\n",
    "          # Uniform sampling\n",
    "          interval = orig_frame_count / final_frames\n",
    "          frame_indices = [min(int(i * interval), orig_frame_count - 1) for i in range(final_frames)]\n",
    "\n",
    "      # Extract frames\n",
    "      extracted_frames = 0\n",
    "      guru.info(f\"Starting frame extraction, target frame count: {len(frame_indices)}\")\n",
    "      guru.info(f\"Save directory: {temp_dir}\")\n",
    "\n",
    "      for i, frame_idx in enumerate(frame_indices):\n",
    "          cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "          ret, frame = cap.read()\n",
    "          if not ret:\n",
    "              guru.warning(f\"Cannot read frame {frame_idx}\")\n",
    "              break\n",
    "\n",
    "          # Adjust resolution\n",
    "          if orig_width != final_width or orig_height != final_height:\n",
    "              # frame = prompts.resize_and_crop_frame(frame, final_width, final_height) # prompts = PromptGUI(checkpoint_dir, model_cfg)\n",
    "              frame = resize_and_crop_frame(frame, final_width, final_height)\n",
    "\n",
    "          # Save frame\n",
    "          output_path = os.path.join(temp_dir, f\"{i:05d}.jpg\")\n",
    "          try:\n",
    "              success = cv2.imwrite(output_path, frame)\n",
    "              if success:\n",
    "                  extracted_frames += 1\n",
    "                  if extracted_frames <= 3 or extracted_frames % 10 == 0:  # Only log first 3 and every 10th\n",
    "                      guru.info(f\"Successfully saved frame {i}: {output_path}\")\n",
    "              else:\n",
    "                  guru.error(f\"cv2.imwrite returned failure: {output_path}\")\n",
    "          except Exception as e:\n",
    "              guru.error(f\"Exception occurred while saving frame {i}: {e}\")\n",
    "\n",
    "      guru.info(f\"Actually extracted {extracted_frames} frames\")\n",
    "\n",
    "      cap.release()\n",
    "\n",
    "      # Check what files are actually in the directory\n",
    "      actual_files = os.listdir(temp_dir)\n",
    "      guru.info(f\"After extraction, files in directory {temp_dir}: {actual_files}\")\n",
    "\n",
    "      if extracted_frames == 0:\n",
    "          error_msg = \"Failed to extract any frames\"\n",
    "          return error_msg\n",
    "\n",
    "      # Automatically load extracted frames and get SAM features\n",
    "      # Convert to absolute path\n",
    "      abs_temp_dir = os.path.abspath(temp_dir)\n",
    "      guru.info(f\"Using absolute path: {abs_temp_dir}\")\n",
    "      video_name = extract_video_name(abs_temp_dir)\n",
    "      num_imgs = set_img_dir(abs_temp_dir)\n",
    "      if num_imgs == 0:\n",
    "          error_msg = \"No image files found in extracted directory\"\n",
    "          return error_msg, temp_dir, None, \"./processed_data/sequence\"\n",
    "\n",
    "      # slider = gr.Slider(minimum=0, maximum=num_imgs - 1, value=0, step=1)\n",
    "      # first_image = set_input_image(0)\n",
    "      # guru.debug(f\"Setting frame {i} / {len(img_paths)}\")\n",
    "      # if i < 0 or i >= len(self.img_paths):\n",
    "      #     return image\n",
    "      # self.clear_points()\n",
    "      # self.frame_index = i\n",
    "      # image = iio.imread(self.img_paths[i])\n",
    "      # self.image = image\n",
    "\n",
    "      # return image\n",
    "\n",
    "\n",
    "      # Automatically get SAM features\n",
    "      # sam_message, sam_image = get_sam_features()\n",
    "\n",
    "      # Generate processing information\n",
    "      process_info = []\n",
    "      if target_frames != orig_frame_count:\n",
    "          process_info.append(f\"Frame count: {orig_frame_count} -> {extracted_frames}\")\n",
    "      if target_width != orig_width or target_height != orig_height:\n",
    "          process_info.append(f\"Resolution: {orig_width}x{orig_height} -> {final_width}x{final_height}\")\n",
    "\n",
    "      message = f\"Video frames extracted to: {temp_dir}, total {extracted_frames} frames.\"\n",
    "      if process_info:\n",
    "          message += f\" Processing: {', '.join(process_info)}.\"\n",
    "      # message += f\" {sam_message}\"\n",
    "\n",
    "\n",
    "      # default_output_path = get_default_output_path()\n",
    "      \"\"\"Generate default output path\"\"\"\n",
    "      if video_name:\n",
    "          default_output_path = f\"./processed_data/{video_name}\"\n",
    "      else:\n",
    "          default_output_path = \"./processed_data/sequence\"\n",
    "\n",
    "      # return message, temp_dir, sam_image if sam_image is not None else first_image, default_output_path\n",
    "      return message, temp_dir, default_output_path\n",
    "\n",
    "  except Exception as e:\n",
    "      error_msg = f\"Frame extraction failed: {str(e)}\"\n",
    "      return error_msg, \"./processed_data/sequence\"\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEEMMAvgQNnb",
    "outputId": "a78b3104-3bc6-41d4-986d-0146a62bfc52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-26 05:53:09.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mStarting frame extraction, target frame count: 49\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:09.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mSave directory: ./tmp/video_frames_1753509189\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:09.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 0: ./tmp/video_frames_1753509189/00000.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:09.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 1: ./tmp/video_frames_1753509189/00001.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:09.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 2: ./tmp/video_frames_1753509189/00002.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:10.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 9: ./tmp/video_frames_1753509189/00009.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:11.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 19: ./tmp/video_frames_1753509189/00019.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:11.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 29: ./tmp/video_frames_1753509189/00029.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:12.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m232\u001b[0m - \u001b[1mSuccessfully saved frame 39: ./tmp/video_frames_1753509189/00039.jpg\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mActually extracted 49 frames\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1mAfter extraction, files in directory ./tmp/video_frames_1753509189: ['00048.jpg', '00047.jpg', '00046.jpg', '00045.jpg', '00044.jpg', '00043.jpg', '00042.jpg', '00041.jpg', '00040.jpg', '00039.jpg', '00038.jpg', '00037.jpg', '00036.jpg', '00035.jpg', '00034.jpg', '00033.jpg', '00032.jpg', '00031.jpg', '00030.jpg', '00029.jpg', '00028.jpg', '00027.jpg', '00026.jpg', '00025.jpg', '00024.jpg', '00023.jpg', '00022.jpg', '00021.jpg', '00020.jpg', '00019.jpg', '00018.jpg', '00017.jpg', '00016.jpg', '00015.jpg', '00014.jpg', '00013.jpg', '00012.jpg', '00011.jpg', '00010.jpg', '00009.jpg', '00008.jpg', '00007.jpg', '00006.jpg', '00005.jpg', '00004.jpg', '00003.jpg', '00002.jpg', '00001.jpg', '00000.jpg']\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mactually_extract_frames\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mUsing absolute path: /workspace/tmp/video_frames_1753509189\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mScanning directory: /workspace/tmp/video_frames_1753509189\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTotal 49 files in directory: ['00048.jpg', '00047.jpg', '00046.jpg', '00045.jpg', '00044.jpg', '00043.jpg', '00042.jpg', '00041.jpg', '00040.jpg', '00039.jpg', '00038.jpg', '00037.jpg', '00036.jpg', '00035.jpg', '00034.jpg', '00033.jpg', '00032.jpg', '00031.jpg', '00030.jpg', '00029.jpg', '00028.jpg', '00027.jpg', '00026.jpg', '00025.jpg', '00024.jpg', '00023.jpg', '00022.jpg', '00021.jpg', '00020.jpg', '00019.jpg', '00018.jpg', '00017.jpg', '00016.jpg', '00015.jpg', '00014.jpg', '00013.jpg', '00012.jpg', '00011.jpg', '00010.jpg', '00009.jpg', '00008.jpg', '00007.jpg', '00006.jpg', '00005.jpg', '00004.jpg', '00003.jpg', '00002.jpg', '00001.jpg', '00000.jpg']\u001b[0m\n",
      "\u001b[32m2025-07-26 05:53:13.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mset_img_dir\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mFound 49 image files: ['00000.jpg', '00001.jpg', '00002.jpg', '00003.jpg', '00004.jpg', '00005.jpg', '00006.jpg', '00007.jpg', '00008.jpg', '00009.jpg', '00010.jpg', '00011.jpg', '00012.jpg', '00013.jpg', '00014.jpg', '00015.jpg', '00016.jpg', '00017.jpg', '00018.jpg', '00019.jpg', '00020.jpg', '00021.jpg', '00022.jpg', '00023.jpg', '00024.jpg', '00025.jpg', '00026.jpg', '00027.jpg', '00028.jpg', '00029.jpg', '00030.jpg', '00031.jpg', '00032.jpg', '00033.jpg', '00034.jpg', '00035.jpg', '00036.jpg', '00037.jpg', '00038.jpg', '00039.jpg', '00040.jpg', '00041.jpg', '00042.jpg', '00043.jpg', '00044.jpg', '00045.jpg', '00046.jpg', '00047.jpg', '00048.jpg']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "message, temp_dir, default_output_path = actually_extract_frames('/workspace/vid.mp4',49,832,480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4poNbrTRRqnF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/tmp/video_frames_1753509189'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir=temp_dir.replace('.','/workspace')\n",
    "temp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "68rr2xlzSLnd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/processed_data/video_1753509189\n",
      "/workspace/processed_data/video_1753509189/traindata\n"
     ]
    }
   ],
   "source": [
    "default_output_path = default_output_path.replace('.','/workspace')\n",
    "traindata_path = default_output_path + '/traindata'\n",
    "print(default_output_path)\n",
    "print(traindata_path)\n",
    "# traindata_path = '/workspace/processed_data/video_1753339507/traindata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oYMVUOvEWl-2"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"{traindata_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {temp_dir}/* {traindata_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(os.listdir(temp_dir)))\n",
    "# !pip install imageio[ffmpeg]\n",
    "# ffmpeg -version\n",
    "import imageio.v2 as iio\n",
    "florence_model = None\n",
    "florence_processor = None\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import imageio.v2 as iio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from loguru import logger as guru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41개 파일 이름 변경 완료.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 정렬된 파일 리스트 가져오기 (00000.jpg ~ 순서대로)\n",
    "file_list = sorted(f for f in os.listdir(temp_dir) if f.endswith('.jpg') and len(f) == 9)\n",
    "\n",
    "for idx, filename in enumerate(file_list, start=1):\n",
    "    old_path = os.path.join(temp_dir, filename)\n",
    "    new_filename = f\"frame_{idx:05d}.jpg\"\n",
    "    new_path = os.path.join(temp_dir, new_filename)\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "print(f\"{len(file_list)}개 파일 이름 변경 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/processed_data/video_1753461854/traindata'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_path\n",
    "workspace/processed_data/video_1753461854/traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "IgU8LlEJRm5P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# !mv {temp_dir}/* {traindata_path}\n",
    "total_frames = 41\n",
    "video_name = f'sequence_all_frames_{total_frames}.mp4'\n",
    "output_video = os.path.join(traindata_path,video_name)\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg', '-y',  # Overwrite existing files\n",
    "    '-framerate', '5',  # Set frame rate to 5fps\n",
    "    '-i', os.path.join(temp_dir, 'frame_%05d.jpg'),\n",
    "    '-c:v', 'libx264',  # Use h264 encoding\n",
    "    '-pix_fmt', 'yuv420p',  # Set pixel format\n",
    "    '-crf', '11',  # Set video quality\n",
    "    output_video\n",
    "]\n",
    "try:\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "except:\n",
    "    print('None')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_text ='Two teenagers quietly gaze out over a vibrant city skyline nestled behind a vast, sunlit forest. Framed by leafy trees and an open balcony, the scene captures a moment of calm and wonder beneath a brilliant summer sky filled with fluffy clouds'\n",
    "video_name =extract_video_name('/content/vid.mp4')\n",
    "txt_filename = video_name.replace('.mp4', '.txt')\n",
    "txt_path = os.path.join(default_output_path, 'traindata', 'sequence_all_frames_41.txt')\n",
    "# workspace/processed_data/video_1753507807/traindata/sequence_all_frames_41.mp4\n",
    "with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(caption_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d-lST9fLXT_T"
   },
   "outputs": [],
   "source": [
    "def create_configs(output_dir: str, ckpt_path: str, learning_rate: float = 1e-7,\n",
    "                  save_every_n_epochs: int = 10, epochs: int = 20, precision: str = \"fp16\"):\n",
    "        \"\"\"Create training configuration files\"\"\"\n",
    "        configs_dir = os.path.join(output_dir, 'configs')\n",
    "        os.makedirs(configs_dir, exist_ok=True)\n",
    "\n",
    "        # Create dataset.toml\n",
    "        dataset_config = f\"\"\"enable_ar_bucket = false\n",
    "\n",
    "[[directory]]\n",
    "path = '{os.path.join(output_dir, \"traindata\").replace(os.sep, \"/\")}'\n",
    "num_repeats = 1\n",
    "\"\"\"\n",
    "\n",
    "        dataset_path = os.path.join(configs_dir, 'dataset.toml')\n",
    "        with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(dataset_config)\n",
    "\n",
    "        # Configure different parameters based on precision\n",
    "        if precision == \"fp16\":\n",
    "            transformer_dtype = 'bfloat16'\n",
    "            optimizer_type = 'AdamW'\n",
    "            stabilize_line = \"\"  # Don't add stabilize parameter\n",
    "        else:  # fp8\n",
    "            transformer_dtype = 'float8'\n",
    "            optimizer_type = 'AdamW8bitKahan'\n",
    "            stabilize_line = \"stabilize = true\"\n",
    "\n",
    "        # Create training.toml\n",
    "        training_config = f\"\"\"output_dir = '{os.path.join(output_dir, \"lora\").replace(os.sep, \"/\")}'\n",
    "dataset = '{dataset_path.replace(os.sep, \"/\")}'\n",
    "\n",
    "epochs = {epochs}\n",
    "micro_batch_size_per_gpu = 1\n",
    "pipeline_stages = 1\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_clipping = 1\n",
    "warmup_steps = 0\n",
    "\n",
    "eval_every_n_epochs = 1000000\n",
    "eval_before_first_step = true\n",
    "eval_micro_batch_size_per_gpu = 1\n",
    "eval_gradient_accumulation_steps = 1\n",
    "\n",
    "save_every_n_epochs = {save_every_n_epochs}\n",
    "checkpoint_every_n_minutes = 1000000000\n",
    "activation_checkpointing = 'unsloth'\n",
    "partition_method = 'parameters'\n",
    "save_dtype = 'bfloat16'\n",
    "caching_batch_size = 1\n",
    "steps_per_print = 1\n",
    "video_clip_mode = 'single_beginning'\n",
    "blocks_to_swap = 32\n",
    "\n",
    "[model]\n",
    "type = 'wan'\n",
    "ckpt_path = '{ckpt_path.replace(os.sep, \"/\")}'\n",
    "dtype = 'bfloat16'\n",
    "transformer_dtype = '{transformer_dtype}'\n",
    "timestep_sample_method = 'uniform'\n",
    "\n",
    "[adapter]\n",
    "type = 'lora'\n",
    "rank = 16\n",
    "dtype = 'bfloat16'\n",
    "exclude_linear_modules = [\"k_img\", \"v_img\"]\n",
    "\n",
    "[optimizer]\n",
    "type = '{optimizer_type}'\n",
    "lr = {learning_rate}\n",
    "betas = [0.9, 0.99]\n",
    "weight_decay = 0.01\n",
    "{stabilize_line}\n",
    "\"\"\"\n",
    "\n",
    "        training_path = os.path.join(configs_dir, 'training.toml')\n",
    "        with open(training_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(training_config)\n",
    "\n",
    "        return configs_dir, dataset_path, training_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "St_g35-WXaAN",
    "outputId": "a0d2ebf6-8647-4271-a96d-93c9cf68e2e3"
   },
   "outputs": [],
   "source": [
    "# create_configs(default_output_path,'Wan2.1-I2V-14B-480P')/content/Wan2.1-T2V-1.3B\n",
    "configs_dir, dataset_path, training_path =create_configs(default_output_path,'/workspace/Wan2.1-T2V-1.3B',learning_rate=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/processed_data/video_1753509189/configs'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import flash_attn_interface\n",
    "    FLASH_ATTN_3_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_3_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    FLASH_ATTN_2_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_2_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "\n",
    "__all__ = [\n",
    "    'flash_attention',\n",
    "    'attention',\n",
    "]\n",
    "\n",
    "\n",
    "def flash_attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    version=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    q:              [B, Lq, Nq, C1].\n",
    "    k:              [B, Lk, Nk, C1].\n",
    "    v:              [B, Lk, Nk, C2]. Nq must be divisible by Nk.\n",
    "    q_lens:         [B].\n",
    "    k_lens:         [B].\n",
    "    dropout_p:      float. Dropout probability.\n",
    "    softmax_scale:  float. The scaling of QK^T before applying softmax.\n",
    "    causal:         bool. Whether to apply causal attention mask.\n",
    "    window_size:    (left right). If not (-1, -1), apply sliding window local attention.\n",
    "    deterministic:  bool. If True, slightly slower and uses more memory.\n",
    "    dtype:          torch.dtype. Apply when dtype of q/k/v is not float16/bfloat16.\n",
    "    \"\"\"\n",
    "    half_dtypes = (torch.float16, torch.bfloat16)\n",
    "    assert dtype in half_dtypes\n",
    "    assert q.device.type == 'cuda' and q.size(-1) <= 256\n",
    "\n",
    "    # params\n",
    "    b, lq, lk, out_dtype = q.size(0), q.size(1), k.size(1), q.dtype\n",
    "\n",
    "    def half(x):\n",
    "        return x if x.dtype in half_dtypes else x.to(dtype)\n",
    "\n",
    "    # preprocess query\n",
    "    if q_lens is None:\n",
    "        q = half(q.flatten(0, 1))\n",
    "        q_lens = torch.tensor(\n",
    "            [lq] * b, dtype=torch.int32).to(\n",
    "                device=q.device, non_blocking=True)\n",
    "    else:\n",
    "        q = half(torch.cat([u[:v] for u, v in zip(q, q_lens)]))\n",
    "\n",
    "    # preprocess key, value\n",
    "    if k_lens is None:\n",
    "        k = half(k.flatten(0, 1))\n",
    "        v = half(v.flatten(0, 1))\n",
    "        k_lens = torch.tensor(\n",
    "            [lk] * b, dtype=torch.int32).to(\n",
    "                device=k.device, non_blocking=True)\n",
    "    else:\n",
    "        k = half(torch.cat([u[:v] for u, v in zip(k, k_lens)]))\n",
    "        v = half(torch.cat([u[:v] for u, v in zip(v, k_lens)]))\n",
    "\n",
    "    q = q.to(v.dtype)\n",
    "    k = k.to(v.dtype)\n",
    "\n",
    "    if q_scale is not None:\n",
    "        q = q * q_scale\n",
    "\n",
    "    if version is not None and version == 3 and not FLASH_ATTN_3_AVAILABLE:\n",
    "        warnings.warn(\n",
    "            'Flash attention 3 is not available, use flash attention 2 instead.'\n",
    "        )\n",
    "\n",
    "    # apply attention\n",
    "    if (version is None or version == 3) and FLASH_ATTN_3_AVAILABLE:\n",
    "        # Note: dropout_p, window_size are not supported in FA3 now.\n",
    "        x = flash_attn_interface.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            seqused_q=None,\n",
    "            seqused_k=None,\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            deterministic=deterministic)[0].unflatten(0, (b, lq))\n",
    "    else:\n",
    "        assert FLASH_ATTN_2_AVAILABLE\n",
    "        x = flash_attn.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic).unflatten(0, (b, lq))\n",
    "\n",
    "    # output\n",
    "    return x.type(out_dtype)\n",
    "\n",
    "\n",
    "def attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    fa_version=None,\n",
    "):\n",
    "    if FLASH_ATTN_2_AVAILABLE or FLASH_ATTN_3_AVAILABLE:\n",
    "        return flash_attention(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            q_lens=q_lens,\n",
    "            k_lens=k_lens,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            q_scale=q_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic,\n",
    "            dtype=dtype,\n",
    "            version=fa_version,\n",
    "        )\n",
    "    else:\n",
    "        if q_lens is not None or k_lens is not None:\n",
    "            warnings.warn(\n",
    "                'Padding mask is disabled when using scaled_dot_product_attention. It can have a significant impact on performance.'\n",
    "            )\n",
    "        attn_mask = None\n",
    "\n",
    "        q = q.transpose(1, 2).to(dtype)\n",
    "        k = k.transpose(1, 2).to(dtype)\n",
    "        v = v.transpose(1, 2).to(dtype)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=attn_mask, is_causal=causal, dropout_p=dropout_p)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aAX6ufw6eJYB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassRegistry:\n",
    "    def __init__(self):\n",
    "        self.classes = dict()\n",
    "        self.args = dict()\n",
    "        self.arg_keys = None\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.classes[item]\n",
    "\n",
    "    def add_to_registry(self, name):\n",
    "        def add_class_by_name(cls):\n",
    "            self.classes[name] = cls\n",
    "            return cls\n",
    "\n",
    "        return add_class_by_name\n",
    "\n",
    "\n",
    "lora_prosessors = ClassRegistry()\n",
    "lora_linear_layers = ClassRegistry()\n",
    "\n",
    "\n",
    "@lora_linear_layers.add_to_registry(\"lora\")\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4, training=True, sig_type=None,):\n",
    "        super().__init__()\n",
    "\n",
    "        if rank > min(in_features, out_features):\n",
    "            raise ValueError(\n",
    "                f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\"\n",
    "            )\n",
    "        self.rank = rank\n",
    "\n",
    "        self.down = nn.Linear(in_features, rank, bias=False)\n",
    "        self.up = nn.Linear(rank, out_features, bias=False)\n",
    "\n",
    "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, hidden_states, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones((1, self.rank))\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        dtype = self.down.weight.dtype\n",
    "\n",
    "        down_hidden_states = self.down(hidden_states.to(dtype)) * mask.to(hidden_states.device)\n",
    "        up_hidden_states = self.up(down_hidden_states)\n",
    "\n",
    "        return up_hidden_states.to(orig_dtype)\n",
    "\n",
    "\n",
    "class LoRACrossAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 base_cross_attn,\n",
    "                 hidden_size,\n",
    "                 lora_linear_layer=LoRALinearLayer,\n",
    "                 cross_attention_dim=None,\n",
    "                 rank=4,\n",
    "                 alpha=1.0,\n",
    "                 sig_type=None):\n",
    "        super().__init__()\n",
    "        self.base = base_cross_attn\n",
    "\n",
    "        # LoRA layers\n",
    "        self.to_q_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_k_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_v_lora = lora_linear_layer(\n",
    "            cross_attention_dim or hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "        self.to_out_lora = lora_linear_layer(\n",
    "            hidden_size, hidden_size, rank, sig_type=sig_type\n",
    "        )\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # LoRA parameter registration (for easy optimizer filtering)\n",
    "        self.lora_layers = [\n",
    "            self.to_q_lora, self.to_k_lora, self.to_v_lora, self.to_out_lora\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, context, context_lens):\n",
    "        b, _, _ = x.shape\n",
    "        n = self.base.num_heads\n",
    "        d = self.base.head_dim\n",
    "\n",
    "        # compute query, key, value with LoRA\n",
    "        q = self.base.norm_q(\n",
    "            self.base.q(x) + self.scaling * self.to_q_lora(x)\n",
    "        ).view(b, -1, n, d)\n",
    "\n",
    "        k = self.base.norm_k(\n",
    "            self.base.k(context) + self.scaling * self.to_k_lora(context)\n",
    "        ).view(b, -1, n, d)\n",
    "\n",
    "        v = self.base.v(context) + self.scaling * self.to_v_lora(context)\n",
    "        v = v.view(b, -1, n, d)\n",
    "\n",
    "        # compute attention\n",
    "        x = flash_attention(q, k, v, k_lens=context_lens)\n",
    "\n",
    "        # output projection with LoRA\n",
    "        x = x.flatten(2)\n",
    "        x = self.base.o(x) + self.scaling * self.to_out_lora(x)\n",
    "        return x\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "import wan\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "@autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "class alpha():\n",
    "    pass\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "1xEOPeaj-3ps",
    "outputId": "0ee14fea-afdb-4e5d-80a4-9207422b3217",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#wan.py\n",
    "from torch.amp import autocast\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "# sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), '../submodules/Wan2_1'))\n",
    "sys.path.insert(0, '/workspace/Wan2.1')\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import set_module_tensor_to_device\n",
    "\n",
    "from base import BasePipeline, PreprocessMediaFile, make_contiguous\n",
    "# from common import AUTOCAST_DTYPE #주범 None값\n",
    "AUTOCAST_DTYPE = torch.bfloat16\n",
    "from offloading import ModelOffloader\n",
    "import wan\n",
    "from wan.modules.t5 import T5Encoder, T5Decoder, T5Model\n",
    "from wan.modules.tokenizers import HuggingfaceTokenizer\n",
    "from wan.modules.vae import WanVAE\n",
    "from wan.modules.model import (\n",
    "    WanModel, sinusoidal_embedding_1d, WanLayerNorm, WanSelfAttention, WAN_CROSSATTENTION_CLASSES\n",
    ")\n",
    "from wan.modules.clip import CLIPModel\n",
    "from wan import configs as wan_configs\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "KEEP_IN_HIGH_PRECISION = ['norm', 'bias', 'patch_embedding', 'text_embedding', 'time_embedding', 'time_projection', 'head', 'modulation']\n",
    "\n",
    "\n",
    "class WanModelFromSafetensors(WanModel):\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        weights_file,\n",
    "        config_file,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        transformer_dtype=torch.bfloat16,\n",
    "    ):\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        config.pop(\"_class_name\", None)\n",
    "        config.pop(\"_diffusers_version\", None)\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model = cls(**config)\n",
    "\n",
    "        state_dict = load_file(weights_file, device='cpu')\n",
    "        state_dict = {\n",
    "            re.sub(r'^model\\.diffusion_model\\.', '', k): v for k, v in state_dict.items()\n",
    "        }\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            dtype_to_use = torch_dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "            set_module_tensor_to_device(model, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "\n",
    "        return model\n",
    "\n",
    "def vae_encode(tensor, vae):\n",
    "    return vae.model.encode(tensor, vae.scale)\n",
    "\n",
    "def umt5_keys_mapping_comfy(state_dict):\n",
    "    import re\n",
    "    # define key mappings rule\n",
    "    def execute_mapping(original_key):\n",
    "        # Token embedding mapping\n",
    "        if original_key == \"shared.weight\":\n",
    "            return \"token_embedding.weight\"\n",
    "\n",
    "        # Final layer norm mapping\n",
    "        if original_key == \"encoder.final_layer_norm.weight\":\n",
    "            return \"norm.weight\"\n",
    "\n",
    "        # Block layer mappings\n",
    "        block_match = re.match(r\"encoder\\.block\\.(\\d+)\\.layer\\.(\\d+)\\.(.+)\", original_key)\n",
    "        if block_match:\n",
    "            block_num = block_match.group(1)\n",
    "            layer_type = int(block_match.group(2))\n",
    "            rest = block_match.group(3)\n",
    "\n",
    "            # self-attn layer（layer.0）\n",
    "            if layer_type == 0:\n",
    "                if \"SelfAttention\" in rest:\n",
    "                    attn_part = rest.split(\".\")[1]\n",
    "                    if attn_part in [\"q\", \"k\", \"v\", \"o\"]:\n",
    "                        return f\"blocks.{block_num}.attn.{attn_part}.weight\"\n",
    "                    elif attn_part == \"relative_attention_bias\":\n",
    "                        return f\"blocks.{block_num}.pos_embedding.embedding.weight\"\n",
    "                elif rest == \"layer_norm.weight\":\n",
    "                    return f\"blocks.{block_num}.norm1.weight\"\n",
    "\n",
    "            # FFN Layer（layer.1）\n",
    "            elif layer_type == 1:\n",
    "                if \"DenseReluDense\" in rest:\n",
    "                    parts = rest.split(\".\")\n",
    "                    if parts[1] == \"wi_0\":\n",
    "                        return f\"blocks.{block_num}.ffn.gate.0.weight\"\n",
    "                    elif parts[1] == \"wi_1\":\n",
    "                        return f\"blocks.{block_num}.ffn.fc1.weight\"\n",
    "                    elif parts[1] == \"wo\":\n",
    "                        return f\"blocks.{block_num}.ffn.fc2.weight\"\n",
    "                elif rest == \"layer_norm.weight\":\n",
    "                    return f\"blocks.{block_num}.norm2.weight\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    new_state_dict = {}\n",
    "    unmapped_keys = []\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = execute_mapping(key)\n",
    "        if new_key:\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            unmapped_keys.append(key)\n",
    "\n",
    "    print(f\"Unmapped keys (usually safe to ignore): {unmapped_keys}\")\n",
    "    del state_dict\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def umt5_keys_mapping_kijai(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"attention.\", \"attn.\")\n",
    "        new_key = new_key.replace(\"final_norm.weight\", \"norm.weight\")\n",
    "        new_state_dict[new_key] = value\n",
    "    del state_dict\n",
    "    return new_state_dict\n",
    "\n",
    "def umt5_keys_mapping(state_dict):\n",
    "    if 'blocks.0.attn.k.weight' in state_dict:\n",
    "        print(\"loading kijai warpper umt5 safetensors model...\")\n",
    "        return umt5_keys_mapping_kijai(state_dict)\n",
    "    else:\n",
    "        print(\"loading comfyui repacked umt5 safetensors model...\")\n",
    "        return umt5_keys_mapping_comfy(state_dict)\n",
    "\n",
    "# We can load T5 a lot faster by copying some code so we can construct the model\n",
    "# inside an init_empty_weights() context.\n",
    "\n",
    "def _t5(name,\n",
    "        encoder_only=False,\n",
    "        decoder_only=False,\n",
    "        return_tokenizer=False,\n",
    "        tokenizer_kwargs={},\n",
    "        dtype=torch.float32,\n",
    "        device='cpu',\n",
    "        **kwargs):\n",
    "    # sanity check\n",
    "    assert not (encoder_only and decoder_only)\n",
    "\n",
    "    # params\n",
    "    if encoder_only:\n",
    "        model_cls = T5Encoder\n",
    "        kwargs['vocab'] = kwargs.pop('vocab_size')\n",
    "        kwargs['num_layers'] = kwargs.pop('encoder_layers')\n",
    "        _ = kwargs.pop('decoder_layers')\n",
    "    elif decoder_only:\n",
    "        model_cls = T5Decoder\n",
    "        kwargs['vocab'] = kwargs.pop('vocab_size')\n",
    "        kwargs['num_layers'] = kwargs.pop('decoder_layers')\n",
    "        _ = kwargs.pop('encoder_layers')\n",
    "    else:\n",
    "        model_cls = T5Model\n",
    "\n",
    "    # init model\n",
    "    with torch.device(device):\n",
    "        model = model_cls(**kwargs)\n",
    "\n",
    "    # init tokenizer\n",
    "    if return_tokenizer:\n",
    "        tokenizer = HuggingfaceTokenizer(f'google/{name}', **tokenizer_kwargs)\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def umt5_xxl(**kwargs):\n",
    "    cfg = dict(\n",
    "        vocab_size=256384,\n",
    "        dim=4096,\n",
    "        dim_attn=4096,\n",
    "        dim_ffn=10240,\n",
    "        num_heads=64,\n",
    "        encoder_layers=24,\n",
    "        decoder_layers=24,\n",
    "        num_buckets=32,\n",
    "        shared_pos=False,\n",
    "        dropout=0.1)\n",
    "    cfg.update(**kwargs)\n",
    "    return _t5('umt5-xxl', **cfg)\n",
    "\n",
    "\n",
    "class T5EncoderModel:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_len,\n",
    "        dtype=torch.bfloat16,\n",
    "        device=torch.cuda.current_device(),\n",
    "        checkpoint_path=None,\n",
    "        tokenizer_path=None,\n",
    "        shard_fn=None,\n",
    "    ):\n",
    "        self.text_len = text_len\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "\n",
    "        # init model\n",
    "        with init_empty_weights():\n",
    "            model = umt5_xxl(\n",
    "                encoder_only=True,\n",
    "                return_tokenizer=False,\n",
    "                dtype=dtype,\n",
    "                device=device).eval().requires_grad_(False)\n",
    "\n",
    "        if checkpoint_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(checkpoint_path, device='cpu')\n",
    "            state_dict = umt5_keys_mapping(state_dict)\n",
    "        else:\n",
    "            state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "        model.load_state_dict(state_dict, assign=True)\n",
    "        self.model = model\n",
    "        if shard_fn is not None:\n",
    "            self.model = shard_fn(self.model, sync_module_states=False)\n",
    "        else:\n",
    "            self.model.to(self.device)\n",
    "        # init tokenizer\n",
    "        self.tokenizer = HuggingfaceTokenizer(\n",
    "            name=tokenizer_path, seq_len=text_len, clean='whitespace')\n",
    "\n",
    "    def __call__(self, texts, device):\n",
    "        ids, mask = self.tokenizer(\n",
    "            texts, return_mask=True, add_special_tokens=True)\n",
    "        ids = ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        seq_lens = mask.gt(0).sum(dim=1).long()\n",
    "        context = self.model(ids, mask)\n",
    "        return [u[:v] for u, v in zip(context, seq_lens)]\n",
    "\n",
    "\n",
    "# Wrapper to hold both VAE and CLIP, so we can move both to/from GPU together.\n",
    "class VaeAndClip(nn.Module):\n",
    "    def __init__(self, vae, clip):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.clip = clip\n",
    "\n",
    "\n",
    "class WanAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cross_attn_type,\n",
    "                 dim,\n",
    "                 ffn_dim,\n",
    "                 num_heads,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 cross_attn_norm=False,\n",
    "                 eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.cross_attn_norm = cross_attn_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        self.norm1 = WanLayerNorm(dim, eps)\n",
    "        self.self_attn = WanSelfAttention(dim, num_heads, window_size, qk_norm,\n",
    "                                          eps)\n",
    "        self.norm3 = WanLayerNorm(\n",
    "            dim, eps,\n",
    "            elementwise_affine=True) if cross_attn_norm else nn.Identity()\n",
    "        self.cross_attn = WAN_CROSSATTENTION_CLASSES[cross_attn_type](dim,\n",
    "                                                                      num_heads,\n",
    "                                                                      (-1, -1),\n",
    "                                                                      qk_norm,\n",
    "                                                                      eps)\n",
    "        self.norm2 = WanLayerNorm(dim, eps)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, ffn_dim), nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(ffn_dim, dim))\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        e,\n",
    "        seq_lens,\n",
    "        grid_sizes,\n",
    "        freqs,\n",
    "        context,\n",
    "        context_lens,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, C]\n",
    "            e(Tensor): Shape [B, 6, C]\n",
    "            seq_lens(Tensor): Shape [B], length of each sequence in batch\n",
    "            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)\n",
    "            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]\n",
    "        \"\"\"\n",
    "        e = (self.modulation + e).chunk(6, dim=1)\n",
    "\n",
    "        # self-attention\n",
    "        y = self.self_attn(\n",
    "            self.norm1(x) * (1 + e[1]) + e[0], seq_lens, grid_sizes,\n",
    "            freqs)\n",
    "        x = x + y * e[2]\n",
    "\n",
    "        # cross-attention & ffn function\n",
    "        def cross_attn_ffn(x, context, context_lens, e):\n",
    "            x = x + self.cross_attn(self.norm3(x), context, context_lens)\n",
    "            y = self.ffn(self.norm2(x) * (1 + e[4]) + e[3])\n",
    "            x = x + y * e[5]\n",
    "            return x\n",
    "\n",
    "        x = cross_attn_ffn(x, context, context_lens, e)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, out_dim, patch_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        out_dim = math.prod(patch_size) * out_dim\n",
    "        self.norm = WanLayerNorm(dim, eps)\n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 2, dim) / dim**0.5)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L1, C]\n",
    "            e(Tensor): Shape [B, C]\n",
    "        \"\"\"\n",
    "        with torch.autocast('cuda', dtype=torch.float32):\n",
    "            e = (self.modulation + e.unsqueeze(1)).chunk(2, dim=1)\n",
    "            x = (self.head(self.norm(x) * (1 + e[1]) + e[0]))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Patch these to remove some forced casting to float32, saving memory.\n",
    "wan.modules.model.WanAttentionBlock = WanAttentionBlock\n",
    "wan.modules.model.Head = Head\n",
    "\n",
    "\n",
    "class WanPipeline(BasePipeline):\n",
    "    name = 'wan'\n",
    "    framerate = 16\n",
    "    checkpointable_layers = ['TransformerLayer']\n",
    "    adapter_target_modules = ['WanAttentionBlock']\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_config = self.config['model']\n",
    "        self.offloader = ModelOffloader('dummy', [], 0, 0, True, torch.device('cuda'), False, debug=False)\n",
    "        ckpt_dir = self.model_config['ckpt_path']\n",
    "        dtype = self.model_config['dtype']\n",
    "\n",
    "        # SkyReels V2 uses 24 FPS. There seems to be no better way to autodetect this.\n",
    "        if 'skyreels' in Path(ckpt_dir).name.lower():\n",
    "            skyreels = True\n",
    "            self.framerate = 24\n",
    "            # FPS is different so make sure to use a new cache dir\n",
    "            self.name = 'skyreels_v2'\n",
    "        else:\n",
    "            skyreels = False\n",
    "\n",
    "        self.original_model_config_path = os.path.join(ckpt_dir, 'config.json')\n",
    "        with open(self.original_model_config_path) as f:\n",
    "            json_config = json.load(f)\n",
    "        self.i2v = (json_config['model_type'] == 'i2v')\n",
    "        self.flf2v = (json_config['model_type'] == 'flf2v')\n",
    "        if self.i2v:\n",
    "            if skyreels:\n",
    "                self.name = 'skyreels_v2_i2v'\n",
    "            else:\n",
    "                self.name = 'wan_i2v'\n",
    "        if self.flf2v:\n",
    "            assert not skyreels\n",
    "            self.name = 'wan_flf2v'\n",
    "        model_dim = json_config['dim']\n",
    "        if not self.i2v and model_dim == 1536:\n",
    "            wan_config = wan_configs.t2v_1_3B\n",
    "        elif self.i2v and model_dim == 1536: # There is no official i2v 1.3b model, but there is https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP\n",
    "            # This is a hack,\n",
    "            wan_config = wan_configs.t2v_1_3B\n",
    "            # The following lines are taken from https://github.com/Wan-Video/Wan2.1/blob/main/wan/configs/wan_i2v_14B.py\n",
    "            wan_config.clip_model = 'clip_xlm_roberta_vit_h_14'\n",
    "            wan_config.clip_dtype = torch.float16\n",
    "            wan_config.clip_checkpoint = 'models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth'\n",
    "            wan_config.clip_tokenizer = 'xlm-roberta-large'\n",
    "        elif self.i2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.i2v_14B\n",
    "        elif self.flf2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.flf2v_14B\n",
    "        elif not self.i2v and model_dim == 5120:\n",
    "            wan_config = wan_configs.t2v_14B\n",
    "        else:\n",
    "            raise RuntimeError(f'Could not autodetect model variant. model_dim={model_dim}, i2v={self.i2v}, flf2v={self.flf2v}')\n",
    "\n",
    "        # This is the outermost class, which isn't a nn.Module\n",
    "        t5_model_path = self.model_config['llm_path'] if self.model_config.get('llm_path', None) else os.path.join(ckpt_dir, wan_config.t5_checkpoint)\n",
    "        self.text_encoder = T5EncoderModel(\n",
    "            text_len=wan_config.text_len,\n",
    "            dtype=dtype,\n",
    "            device='cpu',\n",
    "            checkpoint_path=t5_model_path,\n",
    "            tokenizer_path=os.path.join(ckpt_dir, wan_config.t5_tokenizer),\n",
    "            shard_fn=None,\n",
    "        )\n",
    "\n",
    "        # Same here, this isn't a nn.Module.\n",
    "        # TODO: by default the VAE is float32, and therefore so are the latents. Do we want to change that?\n",
    "        self.vae = WanVAE(\n",
    "            vae_pth=os.path.join(ckpt_dir, wan_config.vae_checkpoint),\n",
    "            device='cpu',\n",
    "        )\n",
    "        # These need to be on the device the VAE will be moved to during caching.\n",
    "        self.vae.mean = self.vae.mean.to('cuda')\n",
    "        self.vae.std = self.vae.std.to('cuda')\n",
    "        self.vae.scale = [self.vae.mean, 1.0 / self.vae.std]\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            self.clip = CLIPModel(\n",
    "                dtype=dtype,\n",
    "                device='cpu',\n",
    "                checkpoint_path=os.path.join(ckpt_dir, wan_config.clip_checkpoint),\n",
    "                tokenizer_path=os.path.join(ckpt_dir, wan_config.clip_tokenizer)\n",
    "            )\n",
    "\n",
    "    # delay loading transformer to save RAM\n",
    "    def load_diffusion_model(self):\n",
    "        dtype = self.model_config['dtype']\n",
    "        transformer_dtype = self.model_config.get('transformer_dtype', dtype)\n",
    "\n",
    "        if transformer_path := self.model_config.get('transformer_path', None):\n",
    "            self.transformer = WanModelFromSafetensors.from_pretrained(\n",
    "                transformer_path,\n",
    "                self.original_model_config_path,\n",
    "                torch_dtype=dtype,\n",
    "                transformer_dtype=transformer_dtype,\n",
    "            )\n",
    "        else:\n",
    "            ckpt_path = Path(self.model_config['ckpt_path'])\n",
    "            with init_empty_weights():\n",
    "                self.transformer = WanModel.from_config(ckpt_path / 'config.json')\n",
    "            state_dict = {}\n",
    "            for shard in ckpt_path.glob('*.safetensors'):\n",
    "                with safetensors.safe_open(shard, framework=\"pt\", device=\"cpu\") as f:\n",
    "                    for key in f.keys():\n",
    "                        state_dict[key] = f.get_tensor(key)\n",
    "            for name, param in self.transformer.named_parameters():\n",
    "                dtype_to_use = dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "                set_module_tensor_to_device(self.transformer, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "#-------------------------------------------------------------------\n",
    "        #lora 파인튜닝\n",
    "        for i,block in enumerate(self.transformer.blocks):\n",
    "            cross_attention_dim = block.cross_attn.dim\n",
    "            attnprocessor = LoRACrossAttention(base_cross_attn=block.cross_attn,hidden_size = cross_attention_dim,cross_attention_dim=cross_attention_dim)\n",
    "            block.cross_attn = attnprocessor\n",
    "            for param in block.cross_attn.base.parameters():\n",
    "              param.requires_grad = False\n",
    "\n",
    "            # Unfreeze LoRA layers\n",
    "            for layer in block.cross_attn.lora_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "#-------------------------------------------------------------------\n",
    "        self.transformer.train()\n",
    "        # We'll need the original parameter name for saving, and the name changes once we wrap modules for pipeline parallelism,\n",
    "        # so store it in an attribute here. Same thing below if we're training a lora and creating lora weights.\n",
    "        for name, p in self.transformer.named_parameters():\n",
    "            p.original_name = name\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.diffusers_pipeline, name)\n",
    "\n",
    "    def get_vae(self):\n",
    "        vae = self.vae.model\n",
    "        clip = self.clip.model if self.i2v or self.flf2v else None\n",
    "        return VaeAndClip(vae, clip)\n",
    "\n",
    "    def get_text_encoders(self):\n",
    "        # Return the inner nn.Module\n",
    "        return [self.text_encoder.model]\n",
    "\n",
    "    def save_adapter(self, save_dir, peft_state_dict):\n",
    "        self.peft_config.save_pretrained(save_dir)\n",
    "        # ComfyUI format.\n",
    "        peft_state_dict = {'diffusion_model.'+k: v for k, v in peft_state_dict.items()}\n",
    "        safetensors.torch.save_file(peft_state_dict, save_dir / 'adapter_model.safetensors', metadata={'format': 'pt'})\n",
    "\n",
    "    # def save_model(self, save_dir, diffusers_sd):\n",
    "    def save_model(self, save_dir, state_dict):\n",
    "        safetensors.torch.save_file(state_dict, save_dir / 'model.safetensors', metadata={'format': 'pt'})\n",
    "\n",
    "    def get_preprocess_media_file_fn(self):\n",
    "        return PreprocessMediaFile(\n",
    "            self.config,\n",
    "            support_video=True,\n",
    "            framerate=self.framerate,\n",
    "            round_height=8,\n",
    "            round_width=8,\n",
    "            round_frames=4,\n",
    "        )\n",
    "\n",
    "    def get_call_vae_fn(self, vae_and_clip):\n",
    "        def fn(tensor):\n",
    "            vae = vae_and_clip.vae\n",
    "            p = next(vae.parameters())\n",
    "            tensor = tensor.to(p.device, p.dtype)\n",
    "\n",
    "            clip = vae_and_clip.clip\n",
    "            if clip is not None:\n",
    "                assert tensor.ndim == 5, f'i2v/flf2v must train on videos, got tensor with shape {tensor.shape}'\n",
    "\n",
    "                # Get video frame count and split points\n",
    "                bs, c, total_frames, h, w = tensor.shape\n",
    "                assert (total_frames - 1) % 3 == 0, f'Video must have 1+3K frames, got {total_frames} frames'\n",
    "                K = (total_frames - 1) // 3\n",
    "\n",
    "                # Frame 1: for CLIP encoding\n",
    "                first_frame = tensor[:, :, 0:1, ...].clone()\n",
    "                clip_context = self.clip.visual(first_frame.to(p.device, p.dtype))\n",
    "\n",
    "                if self.flf2v:\n",
    "                    # For FLF2V, also need last frame CLIP features\n",
    "                    last_frame = tensor[:, :, -1:, ...].clone()\n",
    "                    clip_context = torch.cat([clip_context, self.clip.visual(last_frame.to(p.device, p.dtype))], dim=1)\n",
    "\n",
    "                # Build tensor for VAE encoding\n",
    "                # Frames 2 to K+1: pseudo video condition (keep original content)\n",
    "                condition_frames = tensor[:, :, 1:K+1, ...].clone()\n",
    "\n",
    "                # Frames K+2 to 2K+1: training target\n",
    "                target_frames = tensor[:, :, K+1:2*K+1, ...].clone()\n",
    "\n",
    "                # Directly encode condition frames\n",
    "                y = vae_encode(condition_frames, self.vae)\n",
    "\n",
    "                # Directly encode training target frames\n",
    "                latents = vae_encode(target_frames, self.vae)\n",
    "\n",
    "                ret = {'latents': latents, 'y': y, 'clip_context': clip_context}\n",
    "\n",
    "                # Process mask video (frames 2K+2 to 3K+1)\n",
    "                mask_frames = tensor[:, :, 2*K+1:, ...].clone()\n",
    "                ret['mask_frames'] = mask_frames\n",
    "\n",
    "            else:\n",
    "                latents = vae_encode(tensor, self.vae)\n",
    "                ret = {'latents': latents}\n",
    "\n",
    "            return ret\n",
    "        return fn\n",
    "\n",
    "    def get_call_text_encoder_fn(self, text_encoder):\n",
    "        def fn(caption, is_video):\n",
    "            # Args are lists\n",
    "            p = next(text_encoder.parameters())\n",
    "            ids, mask = self.text_encoder.tokenizer(caption, return_mask=True, add_special_tokens=True)\n",
    "            ids = ids.to(p.device)\n",
    "            mask = mask.to(p.device)\n",
    "            seq_lens = mask.gt(0).sum(dim=1).long()\n",
    "            with torch.autocast(device_type=p.device.type, dtype=p.dtype):\n",
    "                text_embeddings = text_encoder(ids, mask)\n",
    "                return {'text_embeddings': text_embeddings, 'seq_lens': seq_lens}\n",
    "        return fn\n",
    "\n",
    "    def prepare_inputs(self, inputs, timestep_quantile=None):\n",
    "        latents = inputs['latents'].float()\n",
    "        # TODO: why does text_embeddings become float32 here? It's bfloat16 coming out of the text encoder.\n",
    "        text_embeddings = inputs['text_embeddings']\n",
    "        seq_lens = inputs['seq_lens']\n",
    "        mask = inputs['mask']\n",
    "        y = inputs['y'] if self.i2v or self.flf2v else None\n",
    "        clip_context = inputs['clip_context'] if self.i2v or self.flf2v else None\n",
    "        mask_frames = inputs.get('mask_frames', None) if self.i2v or self.flf2v else None\n",
    "\n",
    "        bs, channels, num_frames, h, w = latents.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # make mask (bs, 1, img_h, img_w)\n",
    "            mask = F.interpolate(mask, size=(h, w), mode='nearest-exact')  # resize to latent spatial dimension\n",
    "            mask = mask.unsqueeze(2)  # make mask same number of dims as target\n",
    "\n",
    "        timestep_sample_method = self.model_config.get('timestep_sample_method', 'logit_normal')\n",
    "\n",
    "        if timestep_sample_method == 'logit_normal':\n",
    "            dist = torch.distributions.normal.Normal(0, 1)\n",
    "        elif timestep_sample_method == 'uniform':\n",
    "            dist = torch.distributions.uniform.Uniform(0, 1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if timestep_quantile is not None:\n",
    "            t = dist.icdf(torch.full((bs,), timestep_quantile, device=latents.device))\n",
    "        else:\n",
    "            t = dist.sample((bs,)).to(latents.device)\n",
    "\n",
    "        if timestep_sample_method == 'logit_normal':\n",
    "            sigmoid_scale = self.model_config.get('sigmoid_scale', 1.0)\n",
    "            t = t * sigmoid_scale\n",
    "            t = torch.sigmoid(t)\n",
    "\n",
    "        if shift := self.model_config.get('shift', None):\n",
    "            t = (t * shift) / (1 + (shift - 1) * t)\n",
    "\n",
    "        x_1 = latents\n",
    "        x_0 = torch.randn_like(x_1)\n",
    "        t_expanded = t.view(-1, 1, 1, 1, 1)\n",
    "        x_t = (1 - t_expanded) * x_1 + t_expanded * x_0\n",
    "        target = x_0 - x_1\n",
    "\n",
    "        # timestep input to model needs to be in range [0, 1000]\n",
    "        t = t * 1000\n",
    "\n",
    "        return (\n",
    "            (x_t, y, t, text_embeddings, seq_lens, clip_context, mask_frames),\n",
    "            (target, mask),\n",
    "        )\n",
    "\n",
    "    def to_layers(self):\n",
    "        transformer = self.transformer\n",
    "        layers = [InitialLayer(transformer)]\n",
    "        for i, block in enumerate(transformer.blocks):\n",
    "            layers.append(TransformerLayer(block, i, self.offloader))\n",
    "        layers.append(FinalLayer(transformer))\n",
    "        return layers\n",
    "\n",
    "    def enable_block_swap(self, blocks_to_swap):\n",
    "        transformer = self.transformer\n",
    "        blocks = transformer.blocks\n",
    "        num_blocks = len(blocks)\n",
    "        assert (\n",
    "            blocks_to_swap <= num_blocks - 2\n",
    "        ), f'Cannot swap more than {num_blocks - 2} blocks. Requested {blocks_to_swap} blocks to swap.'\n",
    "        self.offloader = ModelOffloader(\n",
    "            'TransformerBlock', blocks, num_blocks, blocks_to_swap, True, torch.device('cuda'), self.config['reentrant_activation_checkpointing']\n",
    "        )\n",
    "        transformer.blocks = None\n",
    "        transformer.to('cuda')\n",
    "        transformer.blocks = blocks\n",
    "        self.prepare_block_swap_training()\n",
    "        print(f'Block swap enabled. Swapping {blocks_to_swap} blocks out of {num_blocks} blocks.')\n",
    "\n",
    "    def prepare_block_swap_training(self):\n",
    "        self.offloader.enable_block_swap()\n",
    "        self.offloader.set_forward_only(False)\n",
    "        self.offloader.prepare_block_devices_before_forward()\n",
    "\n",
    "    def prepare_block_swap_inference(self, disable_block_swap=False):\n",
    "        if disable_block_swap:\n",
    "            self.offloader.disable_block_swap()\n",
    "        self.offloader.set_forward_only(True)\n",
    "        self.offloader.prepare_block_devices_before_forward()\n",
    "\n",
    "\n",
    "class InitialLayer(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = model.patch_embedding\n",
    "        self.time_embedding = model.time_embedding\n",
    "        self.text_embedding = model.text_embedding\n",
    "        self.time_projection = model.time_projection\n",
    "        self.i2v = (model.model_type == 'i2v')\n",
    "        self.flf2v = (model.model_type == 'flf2v')\n",
    "        if self.i2v or self.flf2v:\n",
    "            self.img_emb = model.img_emb\n",
    "        self.model = [model]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.model[0], name)\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        for item in inputs:\n",
    "            if torch.is_floating_point(item):\n",
    "                item.requires_grad_(True)\n",
    "\n",
    "        x, y, t, context, text_seq_lens, clip_fea, mask_frames = inputs\n",
    "        bs, channels, f, h, w = x.shape\n",
    "        if clip_fea.numel() == 0:\n",
    "            clip_fea = None\n",
    "        context = [emb[:length] for emb, length in zip(context, text_seq_lens)]\n",
    "\n",
    "        device = self.patch_embedding.weight.device\n",
    "        if self.freqs.device != device:\n",
    "            self.freqs = self.freqs.to(device)\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            # Build mask based on mask_frames\n",
    "            if mask_frames is not None:\n",
    "                # Process mask_frames following reference logic\n",
    "                # 1. Convert to single channel (calculate mean)\n",
    "                mask = mask_frames.mean(dim=1, keepdim=False)  # (bs, f, h_orig, w_orig)\n",
    "\n",
    "                # 2. Interpolate to latent space resolution\n",
    "                mask = F.interpolate(\n",
    "                    mask,\n",
    "                    size=(h, w),\n",
    "                    mode='nearest'\n",
    "                )\n",
    "\n",
    "                # 3. Binarize: black (low values) corresponds to 1, white (high values) corresponds to 0\n",
    "                # Assuming input is in [-1,1] range, first normalize to [0,1]\n",
    "                mask = (mask + 1) / 2\n",
    "                mask = (mask < 0.5).float()  # Black regions as 1, white regions as 0\n",
    "\n",
    "                mask = torch.concat([torch.repeat_interleave(mask[:, 0:1], repeats=4, dim=1), mask[:, 1:]], dim=1)\n",
    "                mask = mask.view(mask.shape[0], mask.shape[1] // 4, 4, mask.shape[2], mask.shape[3])\n",
    "                mask = mask.transpose(1, 2)\n",
    "\n",
    "            else:\n",
    "                # If no mask_frames, use original logic\n",
    "                mask = torch.zeros((bs, 4, f, h, w), device=x.device, dtype=x.dtype)\n",
    "                mask[:, :, 0, ...] = 1\n",
    "                if self.flf2v:\n",
    "                    mask[:, :, -1, ...] = 1\n",
    "\n",
    "            y = torch.cat([mask, y], dim=1)\n",
    "            x = [torch.cat([u, v], dim=0) for u, v in zip(x, y)]\n",
    "\n",
    "        # embeddings\n",
    "        x = [self.patch_embedding(u.unsqueeze(0)) for u in x]\n",
    "        grid_sizes = torch.stack(\n",
    "            [torch.tensor(u.shape[2:], dtype=torch.long) for u in x])\n",
    "        x = [u.flatten(2).transpose(1, 2) for u in x]\n",
    "        seq_lens = torch.tensor([u.size(1) for u in x], dtype=torch.long)\n",
    "        seq_len = seq_lens.max()\n",
    "        x = torch.cat([\n",
    "            torch.cat([u, u.new_zeros(1, seq_len - u.size(1), u.size(2))],\n",
    "                      dim=1) for u in x\n",
    "        ])\n",
    "\n",
    "        # time embeddings\n",
    "        e = self.time_embedding(sinusoidal_embedding_1d(self.freq_dim, t).to(x.device, torch.float32))\n",
    "        e0 = self.time_projection(e).unflatten(1, (6, self.dim))\n",
    "\n",
    "        # context\n",
    "        context_lens = None\n",
    "        context = self.text_embedding(\n",
    "            torch.stack([\n",
    "                torch.cat(\n",
    "                    [u, u.new_zeros(self.text_len - u.size(0), u.size(1))])\n",
    "                for u in context\n",
    "            ]))\n",
    "\n",
    "        if self.i2v or self.flf2v:\n",
    "            assert clip_fea is not None\n",
    "            if self.flf2v:\n",
    "                self.img_emb.emb_pos.data = self.img_emb.emb_pos.data.to(clip_fea.device, torch.float32)\n",
    "                clip_fea = clip_fea.view(-1, 257, 1280)\n",
    "            context_clip = self.img_emb(clip_fea)  # bs x 257 (x2) x dim\n",
    "            context = torch.concat([context_clip, context], dim=1)\n",
    "\n",
    "        # pipeline parallelism needs everything on the GPU\n",
    "        seq_lens = seq_lens.to(x.device)\n",
    "        grid_sizes = grid_sizes.to(x.device)\n",
    "\n",
    "        return make_contiguous(x, e, e0, seq_lens, grid_sizes, self.freqs, context)\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, block, block_idx, offloader):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.block_idx = block_idx\n",
    "        self.offloader = offloader\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        x, e, e0, seq_lens, grid_sizes, freqs, context = inputs\n",
    "\n",
    "        self.offloader.wait_for_block(self.block_idx)\n",
    "        x = self.block(x, e0, seq_lens, grid_sizes, freqs, context, None)\n",
    "        self.offloader.submit_move_blocks_forward(self.block_idx)\n",
    "\n",
    "        return make_contiguous(x, e, e0, seq_lens, grid_sizes, freqs, context)\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.head = model.head\n",
    "        self.model = [model]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.model[0], name)\n",
    "\n",
    "    @autocast('cuda', dtype=AUTOCAST_DTYPE)\n",
    "    def forward(self, inputs):\n",
    "        x, e, e0, seq_lens, grid_sizes, freqs, context = inputs\n",
    "        x = self.head(x, e)\n",
    "        x = self.unpatchify(x, grid_sizes)\n",
    "        return torch.stack(x, dim=0)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_mask_construction()\n",
    "#     test_new_frame_structure()\n",
    "\n",
    "\n",
    "# ckpt_path = Path('Wan2.1-T2V-1.3B')\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     transformer = WanModel.from_config(ckpt_path / 'config.json')\n",
    "# state_dict = {}\n",
    "# for shard in ckpt_path.glob('*.safetensors'):\n",
    "#     with safetensors.safe_open(shard, framework=\"pt\", device=\"cpu\") as f:\n",
    "#         for key in f.keys():\n",
    "#             state_dict[key] = f.get_tensor(key)\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     dtype_to_use = dtype if any(keyword in name for keyword in KEEP_IN_HIGH_PRECISION) else transformer_dtype\n",
    "#     set_module_tensor_to_device(transformer, name, device='cpu', dtype=dtype_to_use, value=state_dict[name])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !pip install deepspeed easydict -qq\n",
    "import deepspeed\n",
    "import easydict as edict\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "from easydict import EasyDict as edict\n",
    "from pathlib import Path\n",
    "import deepspeed\n",
    "\n",
    "def set_config_defaults(config):\n",
    "    # Force the user to set this. If we made it a default of 1, it might use a lot of disk space.\n",
    "    assert 'save_every_n_epochs' in config\n",
    "\n",
    "    config.setdefault('pipeline_stages', 1)\n",
    "    config.setdefault('activation_checkpointing', False)\n",
    "    config['reentrant_activation_checkpointing'] = (config['activation_checkpointing'] == 'unsloth')\n",
    "    config.setdefault('warmup_steps', 0)\n",
    "    if 'save_dtype' in config:\n",
    "        config['save_dtype'] = DTYPE_MAP[config['save_dtype']]\n",
    "\n",
    "    model_config = config['model']\n",
    "    model_dtype_str = model_config['dtype']\n",
    "    model_config['dtype'] = DTYPE_MAP[model_dtype_str]\n",
    "    if transformer_dtype := model_config.get('transformer_dtype', None):\n",
    "        model_config['transformer_dtype'] = DTYPE_MAP.get(transformer_dtype, transformer_dtype)\n",
    "    model_config.setdefault('guidance', 1.0)\n",
    "\n",
    "    if 'adapter' in config:\n",
    "        adapter_config = config['adapter']\n",
    "        adapter_type = adapter_config['type']\n",
    "        if adapter_config['type'] == 'lora':\n",
    "            if 'alpha' in adapter_config:\n",
    "                raise NotImplementedError(\n",
    "                    'This script forces alpha=rank to make the saved LoRA format simpler and more predictable with downstream inference programs. Please remove alpha from the config.'\n",
    "                )\n",
    "            adapter_config['alpha'] = adapter_config['rank']\n",
    "            adapter_config.setdefault('dropout', 0.0)\n",
    "            adapter_config.setdefault('dtype', model_dtype_str)\n",
    "            adapter_config['dtype'] = DTYPE_MAP[adapter_config['dtype']]\n",
    "        else:\n",
    "            raise NotImplementedError(f'Adapter type {adapter_type} is not implemented')\n",
    "\n",
    "    config.setdefault('logging_steps', 1)\n",
    "    config.setdefault('eval_datasets', [])\n",
    "    config.setdefault('eval_gradient_accumulation_steps', 1)\n",
    "    config.setdefault('eval_every_n_steps', None)\n",
    "    config.setdefault('eval_every_n_epochs', None)\n",
    "    config.setdefault('eval_before_first_step', True)\n",
    "\n",
    "def get_config():\n",
    "    # 기본 설정\n",
    "    config = {\n",
    "        \"config\": training_path,  # Path to TOML configuration file\n",
    "        \"local_rank\": -1,  # Local rank passed from distributed launcher\n",
    "        \"resume_from_checkpoint\": None,  # None or True or folder path\n",
    "        \"regenerate_cache\": False,  # Force regenerate cache\n",
    "        \"cache_only\": False,  # Cache model inputs then exit\n",
    "        \"i_know_what_i_am_doing\": False,  # Skip certain checks\n",
    "        \"master_port\": 29500,  # Master port for distributed training\n",
    "        \"dump_dataset\": None,  # Path to dump decoded dataset\n",
    "    }\n",
    "\n",
    "    # DeepSpeed config arguments 추가\n",
    "    ds_parser = deepspeed.add_config_arguments(parser)\n",
    "    ds_defaults = ds_parser.parse_args([])  # 기본값만 추출\n",
    "    ds_config = vars(ds_defaults)  # Namespace → dict\n",
    "\n",
    "    # 합치기\n",
    "    config.update(ds_config)\n",
    "\n",
    "    return edict(config)\n",
    "args = get_config()\n",
    "print('done')\n",
    "# --deepspeed --config ./processed_data/your_sequence/configs/training.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.config = args.config +=/\n",
    "# workspace/processed_data/video_1753439446/configs/training.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BCDc05mCVcPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n",
      "Device ID: 0\n",
      "  Device Name: NVIDIA RTX A4500\n",
      "  Compute Capability: 8.6\n",
      "\n",
      "Using device_id: 0\n"
     ]
    }
   ],
   "source": [
    "# prompt: cuda device 이거 0인지 1인지 device_id확인\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "  # Get the number of available CUDA devices\n",
    "  num_cuda_devices = torch.cuda.device_count()\n",
    "  print(f\"Number of CUDA devices available: {num_cuda_devices}\")\n",
    "\n",
    "  # Iterate through available devices and print their properties\n",
    "  for i in range(num_cuda_devices):\n",
    "    device_name = torch.cuda.get_device_name(i)\n",
    "    device_capability = torch.cuda.get_device_capability(i)\n",
    "    print(f\"Device ID: {i}\")\n",
    "    print(f\"  Device Name: {device_name}\")\n",
    "    print(f\"  Compute Capability: {device_capability[0]}.{device_capability[1]}\")\n",
    "\n",
    "  # You can set the device_id based on your preference or configuration\n",
    "  # For example, to use the first device (usually device 0):\n",
    "  device_id = 0\n",
    "  print(f\"\\nUsing device_id: {device_id}\")\n",
    "\n",
    "else:\n",
    "  print(\"CUDA is not available. Please check your GPU and driver installation.\")\n",
    "  # Handle the case where CUDA is not available, e.g., set device_id to None or 'cpu'\n",
    "  device_id = None # Or 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "11gYBaksgz4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import imageio\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime, timezone\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import toml\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "from deepspeed.runtime.pipe import module as ds_pipe_module\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "\n",
    "import dataset as dataset_util\n",
    "import common\n",
    "from common import empty_cuda_cache #is_main_process, DTYPE_MAP, get_rank\n",
    "import saver\n",
    "from isolate_rng import isolate_rng\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    return dist.get_rank()\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "    \n",
    "from typing import Optional\n",
    "import sys\n",
    "import os.path\n",
    "import torch\n",
    "from torch import nn\n",
    "# import peft\n",
    "# from peft.tuners._buffer_dict import BufferDict\n",
    "from transformers import CLIPTextModel, AutoModel\n",
    "import deepspeed\n",
    "from deepspeed.runtime.pipe.schedule import (\n",
    "    SendGrad, RecvActivation, SendActivation, RecvGrad, LoadMicroBatch, ForwardPass, BackwardPass,\n",
    "    ReduceTiedGrads, ReduceGrads, OptimizerStep,\n",
    ")\n",
    "from deepspeed import comm as dist\n",
    "from deepspeed.utils import groups\n",
    "try:\n",
    "    from torch._six import inf\n",
    "except ModuleNotFoundError:\n",
    "    from torch import inf\n",
    "from deepspeed.accelerator import get_accelerator\n",
    "\n",
    "def train_schedule_steps(self):\n",
    "    prev_micro_batch_id = -1\n",
    "    total_steps = 2 * (self.micro_batches + self.stages - 1)\n",
    "    for step_id in range(total_steps):\n",
    "        # Map the step of the pipeline to the micro-batch id and also whether it is a\n",
    "        # forward or backward pass step.\n",
    "        micro_batch_id, is_forward = self._step_to_micro_batch(step_id)\n",
    "\n",
    "        if self._valid_micro_batch(prev_micro_batch_id):\n",
    "            prev_buffer = self._buffer_idx(prev_micro_batch_id)\n",
    "        if self._valid_micro_batch(micro_batch_id):\n",
    "            curr_buffer = self._buffer_idx(micro_batch_id)\n",
    "\n",
    "        cmds = []\n",
    "\n",
    "        # First/last stage loads\n",
    "        if self.stage_id == 0 or self.stage_id == self.stages - 1:\n",
    "            if is_forward and self._valid_micro_batch(micro_batch_id):\n",
    "                cmds.append(LoadMicroBatch(curr_buffer))\n",
    "\n",
    "        # Exchange activations\n",
    "        if is_forward:\n",
    "            if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                cmds.append(SendGrad(prev_buffer))\n",
    "            if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.prev_stage):\n",
    "                cmds.append(RecvActivation(curr_buffer))\n",
    "        else:\n",
    "            if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                cmds.append(RecvGrad(curr_buffer))\n",
    "            if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.next_stage):\n",
    "                cmds.append(SendActivation(prev_buffer))\n",
    "\n",
    "        # Computation\n",
    "        if self._valid_micro_batch(micro_batch_id):\n",
    "            if is_forward:\n",
    "                cmds.append(ForwardPass(curr_buffer))\n",
    "            else:\n",
    "                cmds.append(BackwardPass(curr_buffer))\n",
    "\n",
    "        # Model step at the end of the batch\n",
    "        if step_id == total_steps - 1:\n",
    "            cmds.append(ReduceTiedGrads())\n",
    "            cmds.append(ReduceGrads())\n",
    "            cmds.append(OptimizerStep())\n",
    "\n",
    "        # Prepare state for next time\n",
    "        prev_micro_batch_id = micro_batch_id\n",
    "        yield cmds\n",
    "def broadcast_model(self):\n",
    "    for n, p in self.module.named_parameters():\n",
    "        if torch.is_tensor(p) and p.requires_grad:\n",
    "            orig_device = p.device\n",
    "            move_to_gpu = (orig_device != self.device)\n",
    "            if move_to_gpu:\n",
    "                p.data = p.data.to(self.device)\n",
    "            dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)\n",
    "            if move_to_gpu:\n",
    "                p.data = p.data.to(orig_device)\n",
    "\n",
    "\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2, mpu=None):\n",
    "    \"\"\"Clips gradient norm of an iterable of parameters.\n",
    "\n",
    "    This has been adapted from Nvidia megatron. We add norm averaging\n",
    "    to consider MoE params when calculating norm as they will result\n",
    "    in different norms across different ranks.\n",
    "\n",
    "    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n",
    "    added functionality to handle model parallel parameters. Note that\n",
    "    the gradients are modified in place.\n",
    "\n",
    "    Arguments:\n",
    "        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n",
    "            single Tensor that will have gradients normalized\n",
    "        max_norm (float or int): max norm of the gradients\n",
    "        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n",
    "            infinity norm.\n",
    "\n",
    "    Returns:\n",
    "        Total norm of the parameters (viewed as a single vector).\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    all_norms = []\n",
    "    if norm_type == inf:\n",
    "        for p in parameters:\n",
    "            all_norms.append(p.grad.data.abs().max().float())\n",
    "        total_norm = torch.stack(all_norms).max()\n",
    "        total_norm = total_norm.to(get_accelerator().current_device_name())\n",
    "        # Take max across all GPUs.\n",
    "        if mpu is not None:\n",
    "            dist.all_reduce(total_norm, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n",
    "    else:\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            if mpu is not None:\n",
    "                if (mpu.get_model_parallel_rank() == 0) or deepspeed.runtime.utils.is_model_parallel_parameter(p):\n",
    "                    param_norm = p.grad.data.detach().float().norm(norm_type)\n",
    "                    all_norms.append(param_norm)\n",
    "            else:\n",
    "                param_norm = p.grad.data.detach().float().norm(norm_type)\n",
    "                all_norms.append(param_norm)\n",
    "        if len(all_norms) > 0:\n",
    "            total_norm = torch.stack(all_norms).square().sum().float()\n",
    "        else:\n",
    "            total_norm = get_accelerator().FloatTensor([0.0])\n",
    "        total_norm = total_norm.to(get_accelerator().current_device_name())\n",
    "        # Sum across all model parallel GPUs.\n",
    "        if mpu is not None:\n",
    "            dist.all_reduce(total_norm, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n",
    "        total_norm = total_norm.pow(1. / norm_type)\n",
    "\n",
    "    # Need to average total_norm across different GPUs due to the presence of moe params\n",
    "    pg = groups._get_data_parallel_group()\n",
    "    scaled_norm = total_norm * 1.0 / float(dist.get_world_size(group=pg))\n",
    "    scaled_norm_tensor = scaled_norm\n",
    "\n",
    "    dist.all_reduce(scaled_norm_tensor, group=pg)\n",
    "    total_norm = scaled_norm_tensor\n",
    "    # Change this from the original Deepspeed code.\n",
    "    if len(parameters) > 0:\n",
    "        total_norm = total_norm.to(parameters[0].device)\n",
    "\n",
    "    max_norm = torch.tensor([float(max_norm)], device=total_norm.device)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    tmp_tensor = torch.tensor([1.0], device=clip_coef.device)\n",
    "    clip_coef = torch.min(tmp_tensor, clip_coef)\n",
    "    for p in parameters:\n",
    "        p.grad.data.mul_(clip_coef)\n",
    "    return total_norm\n",
    "    \n",
    "def apply_patches():\n",
    "    # Prevent PEFT from downcasting LoRA weights to fp8 only for this script to upcast them again.\n",
    "    # TODO: probably should send a PR to PEFT. Default behavior looks like a mistake to me.\n",
    "    # peft.tuners.tuners_utils.BaseTunerLayer._move_adapter_to_device_of_base_layer = _move_adapter_to_device_of_base_layer\n",
    "\n",
    "    # Use torch_dtype to avoid needlessly loading the text encoder in float32, only to cast it right after.\n",
    "    # hyvideo.text_encoder.load_text_encoder = load_text_encoder\n",
    "\n",
    "    # LoadMicroBatch before sending / receiving activations so we can avoid a deadlock and broadcast the target\n",
    "    # from the first stage to the last stage. InferenceSchedule already has the commands in the right order\n",
    "    # and doesn't need this.\n",
    "    deepspeed.runtime.pipe.schedule.TrainSchedule.steps = train_schedule_steps\n",
    "\n",
    "    # This does two things:\n",
    "    # 1. For block swapping, some parameters will be on CPU when the DeepSpeedEngine is constructed. So we patch this to\n",
    "    #    first move those parameters to GPU, then back again when broadcasting the model weights from rank 0.\n",
    "    # 2. We skip broadcasting for parameters that don't require grad. These weights are static and always the same because\n",
    "    #    they were loaded from disk, so we can safely skip broadcasting and it's faster.\n",
    "    deepspeed.runtime.engine.DeepSpeedEngine._broadcast_model = broadcast_model\n",
    "\n",
    "# ... later in your code ...\n",
    "\n",
    "    # Don't fail if there are no trainable parameters on a stage.\n",
    "    deepspeed.runtime.engine.DeepSpeedEngine.clip_fp32_gradients = lambda self: clip_grad_norm_(parameters=self.module.parameters(), max_norm=self.gradient_clipping(), mpu=self.mpu)\n",
    "    from unsloth_utils import unsloth_checkpoint\n",
    "\n",
    "from pipeline import ManualPipelineModule\n",
    "\n",
    "wandb_enable = False\n",
    "\n",
    "TIMESTEP_QUANTILES_FOR_EVAL = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = True):\n",
    "        pass\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Monkeypatch this so it counts all layer parameters, not just trainable parameters.\n",
    "# This helps it divide the layers between GPUs more evenly when training a LoRA.\n",
    "def _count_all_layer_params(self):\n",
    "    param_counts = [0] * len(self._layer_specs)\n",
    "    for idx, layer in enumerate(self._layer_specs):\n",
    "        if isinstance(layer, ds_pipe_module.LayerSpec):\n",
    "            l = layer.build()\n",
    "            param_counts[idx] = sum(p.numel() for p in l.parameters())\n",
    "        elif isinstance(layer, nn.Module):\n",
    "            param_counts[idx] = sum(p.numel() for p in layer.parameters())\n",
    "    return param_counts\n",
    "ds_pipe_module.PipelineModule._count_layer_params = _count_all_layer_params\n",
    "\n",
    "def get_most_recent_run_dir(output_dir):\n",
    "    return list(sorted(glob.glob(os.path.join(output_dir, '*'))))[-1]\n",
    "\n",
    "\n",
    "def print_model_info(model):\n",
    "    if not is_main_process():\n",
    "        return\n",
    "    print(model)\n",
    "    for name, module in model.named_modules():\n",
    "        print(f'{type(module)}: {name}')\n",
    "        for pname, p in module.named_parameters(recurse=False):\n",
    "            print(pname)\n",
    "            print(p.dtype)\n",
    "            print(p.device)\n",
    "            print(p.requires_grad)\n",
    "            print()\n",
    "\n",
    "\n",
    "# Need to preload all micro batches since pulling from the dataloader does IPC between the\n",
    "# first and last stage. Can't do that during the train or inference pipeline schedule execution\n",
    "# because it conflicts with the send / recv steps.\n",
    "def get_data_iterator_for_step(dataloader, engine, num_micro_batches=None):\n",
    "    num_micro_batches = num_micro_batches or engine.micro_batches\n",
    "    if not (engine.is_first_stage() or engine.is_last_stage()):\n",
    "        return None\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    items = [next(dataloader_iter) for _ in range(num_micro_batches)]\n",
    "    return iter(items)\n",
    "\n",
    "\n",
    "def evaluate_single(model_engine, eval_dataloader, eval_gradient_accumulation_steps, quantile, pbar=None):\n",
    "    eval_dataloader.set_eval_quantile(quantile)\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        model_engine.reset_activation_shape()\n",
    "        iterator = get_data_iterator_for_step(eval_dataloader, model_engine, num_micro_batches=eval_gradient_accumulation_steps)\n",
    "        loss = model_engine.eval_batch(iterator, num_micro_batches=eval_gradient_accumulation_steps).item()\n",
    "        eval_dataloader.sync_epoch()\n",
    "        if pbar:\n",
    "            pbar.update(1)\n",
    "        total_loss += loss\n",
    "        count += 1\n",
    "        if eval_dataloader.epoch == 2:\n",
    "            break\n",
    "\n",
    "    eval_dataloader.reset()\n",
    "    return total_loss / count\n",
    "\n",
    "\n",
    "def _evaluate(model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps):\n",
    "    pbar_total = 0\n",
    "    for eval_dataloader in eval_dataloaders.values():\n",
    "        pbar_total += len(eval_dataloader) * len(TIMESTEP_QUANTILES_FOR_EVAL) // eval_gradient_accumulation_steps\n",
    "    if is_main_process():\n",
    "        print('Running eval')\n",
    "        pbar = tqdm(total=pbar_total)\n",
    "    else:\n",
    "        pbar = None\n",
    "\n",
    "    start = time.time()\n",
    "    for name, eval_dataloader in eval_dataloaders.items():\n",
    "        losses = []\n",
    "        for quantile in TIMESTEP_QUANTILES_FOR_EVAL:\n",
    "            loss = evaluate_single(model_engine, eval_dataloader, eval_gradient_accumulation_steps, quantile, pbar=pbar)\n",
    "            losses.append(loss)\n",
    "            if is_main_process():\n",
    "                tb_writer.add_scalar(f'{name}/loss_quantile_{quantile:.2f}', loss, step)\n",
    "                if wandb_enable:\n",
    "                    wandb.log({f'{name}/loss_quantile_{quantile:.2f}': loss, 'step': step})\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        if is_main_process():\n",
    "            tb_writer.add_scalar(f'{name}/loss', avg_loss, step)\n",
    "            if wandb_enable:\n",
    "                wandb.log({f'{name}/loss': avg_loss, 'step': step})\n",
    "\n",
    "    duration = time.time() - start\n",
    "    if is_main_process():\n",
    "        tb_writer.add_scalar('eval/eval_time_sec', duration, step)\n",
    "        if wandb_enable:\n",
    "            wandb.log({'eval/eval_time_sec': duration, 'step': step})\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "def evaluate(model, model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps, disable_block_swap):\n",
    "    if len(eval_dataloaders) == 0:\n",
    "        return\n",
    "    empty_cuda_cache()\n",
    "    model.prepare_block_swap_inference(disable_block_swap=disable_block_swap)\n",
    "    with torch.no_grad(), isolate_rng():\n",
    "        seed = get_rank()\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        _evaluate(model_engine, eval_dataloaders, tb_writer, step, eval_gradient_accumulation_steps)\n",
    "    empty_cuda_cache()\n",
    "    model.prepare_block_swap_training()\n",
    "\n",
    "\n",
    "def distributed_init(args):\n",
    "    \"\"\"Initialize distributed training environment.\"\"\"\n",
    "    world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "    rank = int(os.getenv('RANK', '0'))\n",
    "    local_rank = args.local_rank\n",
    "    # GPU 4개로 학습 시 world_size = 4\n",
    "    # GPU 0번 → rank=0, GPU 3번 → rank=3 cuda:3\n",
    "    # 하나의 머신에 GPU 4개: local_rank는 0,1,2,3\n",
    "    # 여러 머신이면 각 머신 안의 로컬 GPU 번호\n",
    "    # Set environment variables for distributed training\n",
    "    os.environ['MASTER_ADDR'] = os.getenv('MASTER_ADDR', 'localhost')\n",
    "    os.environ['MASTER_PORT'] = str(args.master_port)\n",
    "\n",
    "    return world_size, rank, local_rank\n",
    "\n",
    "\n",
    "def get_prodigy_d(optimizer):\n",
    "    d = 0\n",
    "    for group in optimizer.param_groups:\n",
    "        d += group['d']\n",
    "    return d / len(optimizer.param_groups)\n",
    "\n",
    "\n",
    "def _get_automagic_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            state = optimizer.state[p]\n",
    "            lr = optimizer._get_lr(group, state)\n",
    "            lrs.append(lr)\n",
    "    lrs = torch.stack(lrs)\n",
    "    return lrs, lrs.mean()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SKbdVqo6HyI4",
    "outputId": "e8484718-3a35-4c89-c48b-5ff6e5202824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 05:54:07,221] [INFO] [comm.py:676:init_distributed] cdb=None\n",
      "[2025-07-26 05:54:07,222] [INFO] [comm.py:707:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# output_dir = '/content/processed_data/video_1753273997/lora'\n",
    "# dataset = '/content/processed_data/video_1753273997/configs/dataset.toml'\n",
    "# from deepspeed import comm as dist\n",
    "import torch.distributed as dist\n",
    "\n",
    "apply_patches()\n",
    "\n",
    "# needed for broadcasting Queue in dataset.py\n",
    "mp.current_process().authkey = b'afsaskgfdjh4'\n",
    "\n",
    "with open(args.config) as f:\n",
    "    config = json.loads(json.dumps(toml.load(f)))\n",
    "\n",
    "set_config_defaults(config)\n",
    "common.AUTOCAST_DTYPE = config['model']['dtype']\n",
    "\n",
    "# Initialize distributed environment before deepspeed -> # dist.init_process_group()\n",
    "# world_size, rank, local_rank = distributed_init(args)\n",
    "local_rank = args.local_rank\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "rank = int(os.getenv('RANK', '0'))\n",
    "os.environ['MASTER_ADDR'] = os.getenv('MASTER_ADDR', 'localhost')\n",
    "os.environ['MASTER_PORT'] = str(args.master_port)\n",
    "\n",
    "# Now initialize deepspeed\n",
    "deepspeed.init_distributed()\n",
    "# deepspeed.init_distributed(dist_init_required=True)\n",
    "\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "# 수동으로 분산 초기화\n",
    "# dist.init_process_group(backend='nccl')\n",
    "\n",
    "# needed for broadcasting Queue in dataset.py\n",
    "torch.cuda.set_device(dist.get_rank())\n",
    "\n",
    "resume_from_checkpoint = (\n",
    "    args.resume_from_checkpoint if args.resume_from_checkpoint is not None\n",
    "    else config.get('resume_from_checkpoint', False)\n",
    ")\n",
    "regenerate_cache = (\n",
    "    args.regenerate_cache if args.regenerate_cache is not None\n",
    "    else config.get('regenerate_cache', False)\n",
    ")\n",
    "\n",
    "model_type = config['model']['type'] #type = 'wan'\n",
    "\n",
    "if model_type == 'flux':\n",
    "    from models import flux\n",
    "    model = flux.FluxPipeline(config)\n",
    "elif model_type == 'ltx-video':\n",
    "    from models import ltx_video\n",
    "    model = ltx_video.LTXVideoPipeline(config)\n",
    "elif model_type == 'hunyuan-video':\n",
    "    from models import hunyuan_video\n",
    "    model = hunyuan_video.HunyuanVideoPipeline(config)\n",
    "elif model_type == 'sdxl':\n",
    "    from models import sdxl\n",
    "    model = sdxl.SDXLPipeline(config)\n",
    "elif model_type == 'cosmos':\n",
    "    from models import cosmos\n",
    "    model = cosmos.CosmosPipeline(config)\n",
    "elif model_type == 'lumina_2':\n",
    "    from models import lumina_2\n",
    "    model = lumina_2.Lumina2Pipeline(config)\n",
    "elif model_type == 'wan':\n",
    "    # from models import wan\n",
    "    # model = wan.WanPipeline(config)\n",
    "    model = WanPipeline(config)\n",
    "elif model_type == 'chroma':\n",
    "    from models import chroma\n",
    "    model = chroma.ChromaPipeline(config)\n",
    "elif model_type == 'hidream':\n",
    "    from models import hidream\n",
    "    model = hidream.HiDreamPipeline(config)\n",
    "else:\n",
    "    raise NotImplementedError(f'Model type {model_type} is not implemented')\n",
    "\n",
    "\n",
    "with open(config['dataset']) as f:\n",
    "    dataset_config = toml.load(f)\n",
    "gradient_release = config['optimizer'].get('gradient_release', False)\n",
    "ds_config = {\n",
    "    'train_micro_batch_size_per_gpu': config.get('micro_batch_size_per_gpu', 1),\n",
    "    'gradient_accumulation_steps': config.get('gradient_accumulation_steps', 1),\n",
    "    # Can't do gradient clipping with gradient release, since there are no grads at the end of the step anymore.\n",
    "    'gradient_clipping': 0. if gradient_release else config.get('gradient_clipping', 1.0),\n",
    "    'steps_per_print': config.get('steps_per_print', 1),\n",
    "}\n",
    "caching_batch_size = config.get('caching_batch_size', 1)\n",
    "dataset_manager = dataset_util.DatasetManager(model, regenerate_cache=regenerate_cache, caching_batch_size=caching_batch_size)\n",
    "\n",
    "train_data = dataset_util.Dataset(dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "dataset_manager.register(train_data)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': '/workspace/processed_data/video_1753509189/lora',\n",
       " 'dataset': '/workspace/processed_data/video_1753509189/configs/dataset.toml',\n",
       " 'epochs': 20,\n",
       " 'micro_batch_size_per_gpu': 1,\n",
       " 'pipeline_stages': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'gradient_clipping': 1,\n",
       " 'warmup_steps': 0,\n",
       " 'eval_every_n_epochs': 1000000,\n",
       " 'eval_before_first_step': True,\n",
       " 'eval_micro_batch_size_per_gpu': 1,\n",
       " 'eval_gradient_accumulation_steps': 1,\n",
       " 'save_every_n_epochs': 10,\n",
       " 'checkpoint_every_n_minutes': 1000000000,\n",
       " 'activation_checkpointing': 'unsloth',\n",
       " 'partition_method': 'parameters',\n",
       " 'save_dtype': torch.bfloat16,\n",
       " 'caching_batch_size': 1,\n",
       " 'steps_per_print': 1,\n",
       " 'video_clip_mode': 'single_beginning',\n",
       " 'blocks_to_swap': 32,\n",
       " 'model': {'type': 'wan',\n",
       "  'ckpt_path': '/workspace/Wan2.1-T2V-1.3B',\n",
       "  'dtype': torch.bfloat16,\n",
       "  'transformer_dtype': torch.bfloat16,\n",
       "  'timestep_sample_method': 'uniform',\n",
       "  'guidance': 1.0},\n",
       " 'adapter': {'type': 'lora',\n",
       "  'rank': 16,\n",
       "  'dtype': torch.bfloat16,\n",
       "  'exclude_linear_modules': ['k_img', 'v_img'],\n",
       "  'alpha': 16,\n",
       "  'dropout': 0.0},\n",
       " 'optimizer': {'type': 'AdamW',\n",
       "  'lr': 1e-07,\n",
       "  'betas': [0.9, 0.99],\n",
       "  'weight_decay': 0.01},\n",
       " 'reentrant_activation_checkpointing': True,\n",
       " 'logging_steps': 1,\n",
       " 'eval_datasets': [],\n",
       " 'eval_every_n_steps': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_data_map = {}\n",
    "#if eval_list = [], for 문 passed\n",
    "for i, eval_dataset in enumerate(config['eval_datasets']):\n",
    "    if type(eval_dataset) == str:\n",
    "        name = f'eval{i}'\n",
    "        config_path = eval_dataset\n",
    "    else:\n",
    "        name = eval_dataset['name']\n",
    "        config_path = eval_dataset['config']\n",
    "    with open(config_path) as f:\n",
    "        eval_dataset_config = toml.load(f)\n",
    "    eval_data_map[name] = dataset_util.Dataset(eval_dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "    dataset_manager.register(eval_data_map[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using framerate=16\n",
      "caching metadata\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d41d1361244caf9137d634adb61756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 05:54:49,859] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00035.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,861] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00028.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,871] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00017.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,878] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00026.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,880] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00013.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,888] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00003.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,887] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00018.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,891] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00040.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,890] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00029.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,896] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00045.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,898] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00001.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,897] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00000.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,895] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00024.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,910] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00019.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,911] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00011.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,913] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00008.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,915] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00030.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,915] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00012.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,917] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00031.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,918] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00027.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,918] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00021.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,918] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00009.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,923] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00042.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,924] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00025.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,925] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00004.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,925] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00016.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,925] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00039.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,925] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00041.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,927] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00020.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,929] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00007.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,931] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00023.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,932] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00006.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,933] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00033.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,935] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00043.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,935] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00036.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,935] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00010.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,938] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00038.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,939] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00005.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,941] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00022.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,943] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00002.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,944] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00032.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,945] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00015.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,948] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00044.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,948] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00048.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,951] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00046.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,951] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00037.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,953] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00034.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,956] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00047.jpg. Using empty caption.\n",
      "[2025-07-26 05:54:49,964] [WARNING] [dataset.py:410:fn] Cound not find caption for /workspace/processed_data/video_1753509189/traindata/00014.jpg. Using empty caption.\n",
      "caching latents: /workspace/processed_data/video_1753509189/traindata\n",
      "caching latents: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9c92c97c0d411789eb90812773983c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching text embeddings: /workspace/processed_data/video_1753509189/traindata\n",
      "caching text embeddings: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2fa3cf0ceb42578f1db59489a6aec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95aa791884c64f00be0c1a6aa1d1f478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching metadata\n",
      "caching latents: /workspace/processed_data/video_1753509189/traindata\n",
      "caching latents: (None, None, None)\n",
      "caching text embeddings: /workspace/processed_data/video_1753509189/traindata\n",
      "caching text embeddings: (None, None, None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a709b983a55f454c96d07b7b806de592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:248: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'wan.modules.model.WanModel'>.load_config(...) followed by <class 'wan.modules.model.WanModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(pipe=0, data=0): 0}\n",
      "[2025-07-26 05:55:47,425] [INFO] [module.py:398:_partition_layers] Partitioning pipeline stages with method parameters\n",
      "stage=0 layers=32\n",
      "     0: InitialLayer\n",
      "     1: TransformerLayer\n",
      "     2: TransformerLayer\n",
      "     3: TransformerLayer\n",
      "     4: TransformerLayer\n",
      "     5: TransformerLayer\n",
      "     6: TransformerLayer\n",
      "     7: TransformerLayer\n",
      "     8: TransformerLayer\n",
      "     9: TransformerLayer\n",
      "    10: TransformerLayer\n",
      "    11: TransformerLayer\n",
      "    12: TransformerLayer\n",
      "    13: TransformerLayer\n",
      "    14: TransformerLayer\n",
      "    15: TransformerLayer\n",
      "    16: TransformerLayer\n",
      "    17: TransformerLayer\n",
      "    18: TransformerLayer\n",
      "    19: TransformerLayer\n",
      "    20: TransformerLayer\n",
      "    21: TransformerLayer\n",
      "    22: TransformerLayer\n",
      "    23: TransformerLayer\n",
      "    24: TransformerLayer\n",
      "    25: TransformerLayer\n",
      "    26: TransformerLayer\n",
      "    27: TransformerLayer\n",
      "    28: TransformerLayer\n",
      "    29: TransformerLayer\n",
      "    30: TransformerLayer\n",
      "    31: FinalLayer\n",
      "  loss: loss_fn\n",
      "[2025-07-26 05:55:47,929] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-07-26 05:55:47,930] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n",
      "[2025-07-26 05:55:47,946] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********\n",
      "\t self.dp_world_size=1\n",
      "\t self.mp_world_size=1\n",
      "\t self.seq_dp_world_size=1\n",
      "\t self.sequence_parallel_size=1\n",
      "***********************************************\n",
      "[2025-07-26 05:55:48,049] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-07-26 05:55:48,052] [INFO] [logging.py:107:log_dist] [Rank 0] Using client callable to create basic optimizer\n",
      "[2025-07-26 05:55:48,053] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-07-26 05:55:48,126] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-07-26 05:55:48,127] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2025-07-26 05:55:48,127] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-07-26 05:55:48,128] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-07-26 05:55:48,128] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "[2025-07-26 05:55:48,130] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\n",
      "[2025-07-26 05:55:48,131] [INFO] [config.py:954:print] DeepSpeedEngine configuration:\n",
      "[2025-07-26 05:55:48,131] [INFO] [config.py:958:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-07-26 05:55:48,132] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-07-26 05:55:48,132] [INFO] [config.py:958:print]   amp_enabled .................. False\n",
      "[2025-07-26 05:55:48,132] [INFO] [config.py:958:print]   amp_params ................... False\n",
      "[2025-07-26 05:55:48,133] [INFO] [config.py:958:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-07-26 05:55:48,133] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False\n",
      "[2025-07-26 05:55:48,133] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\n",
      "[2025-07-26 05:55:48,134] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-07-26 05:55:48,134] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-07-26 05:55:48,134] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-07-26 05:55:48,135] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7bcf7bb95150>\n",
      "[2025-07-26 05:55:48,135] [INFO] [config.py:958:print]   communication_data_type ...... None\n",
      "[2025-07-26 05:55:48,135] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False\n",
      "[2025-07-26 05:55:48,136] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-07-26 05:55:48,136] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False\n",
      "[2025-07-26 05:55:48,136] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False\n",
      "[2025-07-26 05:55:48,137] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-07-26 05:55:48,137] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False\n",
      "[2025-07-26 05:55:48,138] [INFO] [config.py:958:print]   dataloader_drop_last ......... False\n",
      "[2025-07-26 05:55:48,138] [INFO] [config.py:958:print]   disable_allgather ............ False\n",
      "[2025-07-26 05:55:48,138] [INFO] [config.py:958:print]   dump_state ................... False\n",
      "[2025-07-26 05:55:48,139] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False\n",
      "[2025-07-26 05:55:48,139] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-07-26 05:55:48,139] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-07-26 05:55:48,140] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-07-26 05:55:48,140] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-07-26 05:55:48,140] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-07-26 05:55:48,141] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-07-26 05:55:48,141] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False\n",
      "[2025-07-26 05:55:48,141] [INFO] [config.py:958:print]   elasticity_enabled ........... False\n",
      "[2025-07-26 05:55:48,142] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\n",
      "[2025-07-26 05:55:48,142] [INFO] [config.py:958:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-07-26 05:55:48,142] [INFO] [config.py:958:print]   global_rank .................. 0\n",
      "[2025-07-26 05:55:48,143] [INFO] [config.py:958:print]   grad_accum_dtype ............. None\n",
      "[2025-07-26 05:55:48,143] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 1\n",
      "[2025-07-26 05:55:48,143] [INFO] [config.py:958:print]   gradient_clipping ............ 1\n",
      "[2025-07-26 05:55:48,144] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-07-26 05:55:48,144] [INFO] [config.py:958:print]   graph_harvesting ............. False\n",
      "[2025-07-26 05:55:48,144] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-07-26 05:55:48,144] [INFO] [config.py:958:print]   load_universal_checkpoint .... False\n",
      "[2025-07-26 05:55:48,145] [INFO] [config.py:958:print]   memory_breakdown ............. False\n",
      "[2025-07-26 05:55:48,145] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False\n",
      "[2025-07-26 05:55:48,145] [INFO] [config.py:958:print]   mics_shard_size .............. -1\n",
      "[2025-07-26 05:55:48,146] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-07-26 05:55:48,146] [INFO] [config.py:958:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-07-26 05:55:48,146] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-07-26 05:55:48,147] [INFO] [config.py:958:print]   optimizer_name ............... None\n",
      "[2025-07-26 05:55:48,147] [INFO] [config.py:958:print]   optimizer_params ............. None\n",
      "[2025-07-26 05:55:48,147] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-07-26 05:55:48,148] [INFO] [config.py:958:print]   pld_enabled .................. False\n",
      "[2025-07-26 05:55:48,148] [INFO] [config.py:958:print]   pld_params ................... False\n",
      "[2025-07-26 05:55:48,148] [INFO] [config.py:958:print]   prescale_gradients ........... False\n",
      "[2025-07-26 05:55:48,149] [INFO] [config.py:958:print]   scheduler_name ............... None\n",
      "[2025-07-26 05:55:48,149] [INFO] [config.py:958:print]   scheduler_params ............. None\n",
      "[2025-07-26 05:55:48,149] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-07-26 05:55:48,149] [INFO] [config.py:958:print]   sparse_attention ............. None\n",
      "[2025-07-26 05:55:48,150] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False\n",
      "[2025-07-26 05:55:48,150] [INFO] [config.py:958:print]   steps_per_print .............. 1\n",
      "[2025-07-26 05:55:48,150] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-07-26 05:55:48,151] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-07-26 05:55:48,151] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None\n",
      "[2025-07-26 05:55:48,151] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False\n",
      "[2025-07-26 05:55:48,152] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None\n",
      "[2025-07-26 05:55:48,152] [INFO] [config.py:958:print]   train_batch_size ............. 1\n",
      "[2025-07-26 05:55:48,152] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-07-26 05:55:48,153] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False\n",
      "[2025-07-26 05:55:48,153] [INFO] [config.py:958:print]   use_node_local_storage ....... False\n",
      "[2025-07-26 05:55:48,153] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False\n",
      "[2025-07-26 05:55:48,153] [INFO] [config.py:958:print]   weight_quantization_config ... None\n",
      "[2025-07-26 05:55:48,154] [INFO] [config.py:958:print]   world_size ................... 1\n",
      "[2025-07-26 05:55:48,154] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False\n",
      "[2025-07-26 05:55:48,154] [INFO] [config.py:958:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-07-26 05:55:48,155] [INFO] [config.py:958:print]   zero_enabled ................. False\n",
      "[2025-07-26 05:55:48,155] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-07-26 05:55:48,155] [INFO] [config.py:958:print]   zero_optimization_stage ...... 0\n",
      "[2025-07-26 05:55:48,156] [INFO] [config.py:944:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1, \n",
      "    \"steps_per_print\": 1\n",
      "}\n",
      "[2025-07-26 05:55:48,156] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=1 micro_batch_size=1\n",
      "[2025-07-26 05:55:48,156] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-07-26 05:55:48,239] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=32 [0, 32) STAGE_PARAMS=1137079360 (1137.079M) TOTAL_PARAMS=1137079360 (1137.079M) UNIQUE_PARAMS=1137079360 (1137.079M)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from unsloth_utils import unsloth_checkpoint\n",
    "# if args.dump_dataset:\n",
    "#     # only works for flux 코드 있는데 지움\n",
    "\n",
    "\n",
    "dataset_manager.cache()\n",
    "if args.cache_only:\n",
    "    quit()\n",
    "\n",
    "model.load_diffusion_model()\n",
    "\n",
    "# [adapter]\n",
    "# type = 'lora'\n",
    "# rank = 16\n",
    "# dtype = 'bfloat16'\n",
    "# exclude_linear_modules = [\"k_img\", \"v_img\"]\n",
    "\n",
    "# if adapter_config := config.get('adapter', None):\n",
    "#     model.configure_adapter(adapter_config)\n",
    "#     is_adapter = True\n",
    "#     if init_from_existing := adapter_config.get('init_from_existing', None):\n",
    "#         model.load_adapter_weights(init_from_existing)\n",
    "# else:\n",
    "#     is_adapter = False\n",
    "\n",
    "# if this is a new run, create a new dir for it\n",
    "if not resume_from_checkpoint and is_main_process():\n",
    "    run_dir = os.path.join(config['output_dir'], datetime.now(timezone.utc).strftime('%Y%m%d_%H-%M-%S'))\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    shutil.copy(args.config, run_dir)\n",
    "    shutil.copy(config['dataset'], run_dir)\n",
    "    for eval_dataset in config['eval_datasets']:\n",
    "        shutil.copy(eval_dataset['config'], run_dir)\n",
    "# wait for all processes then get the most recent dir (may have just been created)\n",
    "dist.barrier()\n",
    "if resume_from_checkpoint is True:  # No specific folder provided, use most recent\n",
    "    run_dir = get_most_recent_run_dir(config['output_dir'])\n",
    "elif isinstance(resume_from_checkpoint, str):  # Specific folder provided\n",
    "    run_dir = os.path.join(config['output_dir'], resume_from_checkpoint)\n",
    "    if not os.path.exists(run_dir):\n",
    "        raise ValueError(f\"Checkpoint directory {run_dir} does not exist\")\n",
    "else:  # Not resuming, use most recent (newly created) dir\n",
    "    run_dir = get_most_recent_run_dir(config['output_dir'])\n",
    "\n",
    "# WandB logging\n",
    "wandb_enable = config.get('monitoring', {}).get('enable_wandb', False)\n",
    "if wandb_enable:\n",
    "    wandb_api_key     = config['monitoring']['wandb_api_key']\n",
    "    wandb_tracker     = config['monitoring']['wandb_tracker_name']\n",
    "    wandb_run_name    = config['monitoring']['wandb_run_name']\n",
    "    logging_dir       = run_dir\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    wandb.init(\n",
    "        project=wandb_tracker,\n",
    "        name=wandb_run_name,\n",
    "        config=config,\n",
    "        dir=logging_dir\n",
    "    )\n",
    "\n",
    "# Block swapping\n",
    "# if blocks_to_swap := config.get('blocks_to_swap', 0):\n",
    "#     assert config['pipeline_stages'] == 1, 'Block swapping only works with pipeline_stages=1'\n",
    "#     assert 'adapter' in config, 'Block swapping only works when training LoRA'\n",
    "#     # Don't automatically move to GPU, we'll do that ourselves.\n",
    "#     def to(self, *args, **kwargs):\n",
    "#         pass\n",
    "#     deepspeed.pipe.PipelineModule.to = to\n",
    "#     model.enable_block_swap(blocks_to_swap)\n",
    "\n",
    "layers = model.to_layers()\n",
    "additional_pipeline_module_kwargs = {}\n",
    "activation_checkpointing = config['activation_checkpointing']\n",
    "if activation_checkpointing:\n",
    "    if activation_checkpointing == True:\n",
    "        # TODO: block swapping doesn't work with Deepspeed non-reentrant checkpoint, but PyTorch native one is fine. Some\n",
    "        # weights end up on CPU where they shouldn't. Why? Are we giving anything up by not using the Deepspeed implementation?\n",
    "        #checkpoint_func = deepspeed.checkpointing.non_reentrant_checkpoint\n",
    "        from functools import partial\n",
    "        checkpoint_func = partial(torch.utils.checkpoint.checkpoint, use_reentrant=False)\n",
    "    elif activation_checkpointing == 'unsloth':\n",
    "        checkpoint_func = unsloth_checkpoint\n",
    "    else:\n",
    "        raise NotImplementedError(f'activation_checkpointing={activation_checkpointing} is not implemented')\n",
    "    additional_pipeline_module_kwargs.update({\n",
    "        'activation_checkpoint_interval': 1,\n",
    "        'checkpointable_layers': model.checkpointable_layers,\n",
    "        'activation_checkpoint_func': checkpoint_func,\n",
    "    })\n",
    "\n",
    "num_stages = config.get('pipeline_stages', 1)\n",
    "partition_method=config.get('partition_method', 'parameters')\n",
    "partition_split = config.get('partition_split',[len(layers) / num_stages])\n",
    "pipeline_model = ManualPipelineModule(\n",
    "    layers=layers,\n",
    "    num_stages=num_stages,\n",
    "    partition_method=partition_method,\n",
    "    manual_partition_split=partition_split,\n",
    "    loss_fn=model.get_loss_fn(),\n",
    "    **additional_pipeline_module_kwargs\n",
    ")\n",
    "parameters_to_train = [p for p in pipeline_model.parameters() if p.requires_grad]\n",
    "\n",
    "def get_optimizer(model_parameters):\n",
    "    if len(model_parameters) == 0:\n",
    "        return DummyOptimizer()\n",
    "# [optimizer]\n",
    "# type = 'AdamW8bitKahan'\n",
    "# lr = 0.001\n",
    "# betas = [0.9, 0.99]\n",
    "# weight_decay = 0.01\n",
    "# stabilize = true\n",
    "\n",
    "    optim_config = config['optimizer']\n",
    "    optim_type = optim_config['type']\n",
    "    optim_type_lower = optim_type.lower()\n",
    "\n",
    "    args = []\n",
    "    kwargs = {k: v for k, v in optim_config.items() if k not in ['type', 'gradient_release']}\n",
    "\n",
    "    if optim_type_lower == 'adamw':\n",
    "        # TODO: fix this. I'm getting \"fatal error: cuda_runtime.h: No such file or directory\"\n",
    "        # when Deepspeed tries to build the fused Adam extension.\n",
    "        # klass = deepspeed.ops.adam.FusedAdam\n",
    "        klass = torch.optim.AdamW\n",
    "    elif optim_type_lower == 'adamw8bit':\n",
    "        import bitsandbytes\n",
    "        klass = bitsandbytes.optim.AdamW8bit\n",
    "    elif optim_type_lower == 'adamw_optimi':\n",
    "        import optimi\n",
    "        klass = optimi.AdamW\n",
    "    elif optim_type_lower == 'stableadamw':\n",
    "        import optimi\n",
    "        klass = optimi.StableAdamW\n",
    "    elif optim_type_lower == 'sgd':\n",
    "        klass = torch.optim.SGD\n",
    "    elif optim_type_lower == 'adamw8bitkahan':\n",
    "        import adamw_8bit\n",
    "        klass = adamw_8bit.AdamW8bitKahan\n",
    "    elif optim_type_lower == 'offload':\n",
    "        from torchao.prototype.low_bit_optim import CPUOffloadOptimizer\n",
    "        klass = CPUOffloadOptimizer\n",
    "        args.append(torch.optim.AdamW)\n",
    "        kwargs['fused'] = True\n",
    "    elif optim_type_lower == 'automagic':\n",
    "        from optimizers import automagic\n",
    "        klass = automagic.Automagic\n",
    "    else:\n",
    "        import pytorch_optimizer\n",
    "        klass = getattr(pytorch_optimizer, optim_type)\n",
    "    #없음\n",
    "    if optim_config.get('gradient_release', False):\n",
    "        # Prevent deepspeed from logging every single param group lr\n",
    "        def _report_progress(self, step):\n",
    "            lr = self.get_lr()\n",
    "            mom = self.get_mom()\n",
    "            deepspeed.utils.logging.log_dist(f\"step={step}, skipped={self.skipped_steps}, lr={lr[0]}, mom={mom[0]}\", ranks=[0])\n",
    "        deepspeed.runtime.engine.DeepSpeedEngine._report_progress = _report_progress\n",
    "\n",
    "        # Deepspeed executes all the code to reduce grads across data parallel ranks even if the DP world size is 1.\n",
    "        # As part of this, any grads that are None are set to zeros. We're doing gradient release to save memory,\n",
    "        # so we have to avoid this.\n",
    "        def _exec_reduce_grads(self):\n",
    "            assert self.mpu.get_data_parallel_world_size() == 1, 'When using gradient release, data parallel world size must be 1. Make sure pipeline_stages = num_gpus.'\n",
    "            return\n",
    "        deepspeed.runtime.pipe.engine.PipelineEngine._INSTRUCTION_MAP[deepspeed.runtime.pipe.schedule.ReduceGrads] = _exec_reduce_grads\n",
    "\n",
    "        # When pipelining multiple forward and backward passes, normally updating the parameter in-place causes an error when calling\n",
    "        # backward() on future micro-batches. But we can modify .data directly so the autograd engine doesn't detect in-place modifications.\n",
    "        # TODO: this is unbelievably hacky and not mathematically sound, I'm just seeing if it works at all.\n",
    "        def add_(self, *args, **kwargs):\n",
    "            self.data.add_(*args, **kwargs)\n",
    "        for p in model_parameters:\n",
    "            p.add_ = add_.__get__(p)\n",
    "\n",
    "        if 'foreach' in inspect.signature(klass).parameters:\n",
    "            kwargs['foreach'] = False\n",
    "\n",
    "        # We're doing an optimizer step for each micro-batch. Scale momentum and EMA betas so that the contribution\n",
    "        # decays at the same rate it would if we were doing one step per batch like normal.\n",
    "        # Reference: https://alexeytochin.github.io/posts/batch_size_vs_momentum/batch_size_vs_momentum.html\n",
    "        gas = ds_config['gradient_accumulation_steps']\n",
    "        if 'betas' in kwargs:\n",
    "            for i in range(len(kwargs['betas'])):\n",
    "                kwargs['betas'][i] = kwargs['betas'][i] ** (1/gas)\n",
    "        if 'momentum' in kwargs:\n",
    "            kwargs['momentum'] = kwargs['momentum'] ** (1/gas)\n",
    "\n",
    "        optimizer_dict = {}\n",
    "        for pg in model.get_param_groups(model_parameters):\n",
    "            param_kwargs = kwargs.copy()\n",
    "            if isinstance(pg, dict):\n",
    "                # param group\n",
    "                for p in pg['params']:\n",
    "                    param_kwargs['lr'] = pg['lr']\n",
    "                    optimizer_dict[p] = klass([p], **param_kwargs)\n",
    "            else:\n",
    "                # param\n",
    "                optimizer_dict[pg] = klass([pg], **param_kwargs)\n",
    "\n",
    "        def optimizer_hook(p):\n",
    "            optimizer_dict[p].step()\n",
    "            optimizer_dict[p].zero_grad()\n",
    "\n",
    "        for p in model_parameters:\n",
    "            p.register_post_accumulate_grad_hook(optimizer_hook)\n",
    "\n",
    "        from optimizers import gradient_release\n",
    "        return gradient_release.GradientReleaseOptimizerWrapper(list(optimizer_dict.values()))\n",
    "    else:\n",
    "        model_parameters = model.get_param_groups(model_parameters)\n",
    "        return klass(model_parameters, *args, **kwargs)\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    args=args,\n",
    "    model=pipeline_model,\n",
    "    model_parameters=parameters_to_train,\n",
    "    optimizer=get_optimizer,\n",
    "    config=ds_config,\n",
    ")\n",
    "model.model_engine = model_engine\n",
    "if model_engine.is_pipe_parallel:\n",
    "      grid = model_engine.grid\n",
    "      model_engine.first_last_stage_group = dist.new_group(ranks=[grid.pp_group[0], grid.pp_group[-1]])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
    "        #0임 -> 10으로 해보자\n",
    "if config['warmup_steps'] > 0:\n",
    "    warmup_steps = config['warmup_steps']\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1/warmup_steps, total_iters=warmup_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, lr_scheduler], milestones=[warmup_steps])\n",
    "model_engine.lr_scheduler = lr_scheduler\n",
    "\n",
    "train_data.post_init(\n",
    "    model_engine.grid.get_data_parallel_rank(),\n",
    "    model_engine.grid.get_data_parallel_world_size(),\n",
    "    model_engine.train_micro_batch_size_per_gpu(),\n",
    "    model_engine.gradient_accumulation_steps(),\n",
    "    config.get('image_micro_batch_size_per_gpu', model_engine.train_micro_batch_size_per_gpu()),\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Might be useful because we set things in fp16 / bf16 without explicitly enabling Deepspeed fp16 mode.\n",
    "# Unsure if really needed.\n",
    "communication_data_type = config['lora']['dtype'] if 'lora' in config else config['model']['dtype']\n",
    "model_engine.communication_data_type = communication_data_type\n",
    "\n",
    "train_dataloader = dataset_util.PipelineDataLoader(train_data, model_engine, model_engine.gradient_accumulation_steps(), model)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file': ['/workspace/processed_data/video_1753452111/traindata/00006.jpg'],\n",
       " 'caption': [''],\n",
       " 'latents': tensor([[[[[ 0.4820,  0.5311,  0.6354,  ...,  0.6336,  0.5539,  0.5367],\n",
       "            [ 0.4857,  0.3282,  0.3297,  ...,  0.1501,  0.1781,  0.2602],\n",
       "            [ 0.4310,  0.4279,  0.4642,  ...,  0.1933,  0.3104,  0.3487],\n",
       "            ...,\n",
       "            [-0.2382,  0.2249,  0.0234,  ..., -0.5076, -0.5087, -0.5641],\n",
       "            [-0.0904, -0.1508, -0.0251,  ..., -0.4557, -0.4915, -0.4503],\n",
       "            [ 0.0033, -0.0715, -0.0451,  ..., -0.3240, -0.4808, -0.4255]]],\n",
       " \n",
       " \n",
       "          [[[ 0.6572,  0.7632,  0.5207,  ...,  0.6379,  0.5191,  0.5153],\n",
       "            [ 0.4699,  0.4798,  0.6949,  ...,  0.5551,  0.6388,  0.5320],\n",
       "            [ 0.8051,  1.0002,  0.9930,  ...,  0.7539,  0.6788,  0.4521],\n",
       "            ...,\n",
       "            [-0.2556,  0.0185, -0.0595,  ...,  0.0145, -0.0243, -0.1628],\n",
       "            [ 0.2158, -0.6076,  0.1660,  ...,  0.1643, -0.1717, -0.0757],\n",
       "            [-0.1669, -0.0814,  0.2215,  ..., -0.1917,  0.0638, -0.5833]]],\n",
       " \n",
       " \n",
       "          [[[-0.7549, -0.6213, -1.0215,  ..., -0.9879, -1.0543, -0.8875],\n",
       "            [-0.6052, -0.9585, -0.9612,  ..., -1.1254, -1.0122, -0.9744],\n",
       "            [-0.8946, -0.9029, -1.0744,  ..., -0.8248, -0.8273, -0.7163],\n",
       "            ...,\n",
       "            [ 0.1690, -0.2227, -0.3325,  ..., -0.1022,  0.5846,  0.6036],\n",
       "            [ 0.2840, -0.1027,  0.0283,  ...,  0.1643,  0.3336,  0.7680],\n",
       "            [-0.1670, -0.2220, -0.2612,  ...,  0.1040,  0.1814,  0.2372]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.0537,  0.0647, -0.1825,  ..., -0.1796, -0.2395, -0.0590],\n",
       "            [ 0.8507,  0.3671,  0.5287,  ...,  0.7558,  0.9875,  0.7325],\n",
       "            [-0.5375, -0.5811, -0.6253,  ..., -0.3366, -0.2758, -0.1443],\n",
       "            ...,\n",
       "            [-0.4441, -0.4246, -0.3435,  ..., -0.4169, -0.6518,  0.0872],\n",
       "            [-0.3295, -0.8781, -0.0939,  ..., -0.6289, -0.8420, -0.3944],\n",
       "            [-0.9821, -0.8154, -0.1712,  ..., -0.5205, -0.5913, -0.7619]]],\n",
       " \n",
       " \n",
       "          [[[-0.1620,  0.0488, -0.2601,  ..., -0.2334, -0.2151, -0.1862],\n",
       "            [-0.0892, -0.2937, -0.2728,  ..., -0.1206, -0.1973, -0.1734],\n",
       "            [-0.2170, -0.2189, -0.1440,  ...,  0.0678,  0.0691, -0.0168],\n",
       "            ...,\n",
       "            [-0.3772, -0.2116, -0.2504,  ..., -0.4039, -0.3674, -0.0741],\n",
       "            [-0.0550, -0.5102, -0.1040,  ..., -0.2074, -0.3351, -0.3591],\n",
       "            [-0.4637, -0.2126, -0.2017,  ..., -0.2004, -0.0123, -0.5128]]],\n",
       " \n",
       " \n",
       "          [[[-0.1410, -0.0573, -0.3498,  ..., -0.3328, -0.4440, -0.3750],\n",
       "            [-0.0149, -0.2708, -0.2961,  ..., -0.1505, -0.3095, -0.2977],\n",
       "            [ 0.0141, -0.0307, -0.0062,  ...,  0.0390, -0.0477, -0.1651],\n",
       "            ...,\n",
       "            [ 0.5769,  0.0585,  0.2209,  ...,  1.2752,  1.1943,  0.4877],\n",
       "            [ 0.7542,  0.7171,  0.4171,  ...,  1.2975,  0.9574,  0.8747],\n",
       "            [ 0.5077,  0.6221,  0.4396,  ...,  0.9142,  0.9132,  0.6399]]]]]),\n",
       " 'text_embeddings': tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          ...,\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]),\n",
       " 'seq_lens': tensor([1]),\n",
       " 'mask': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file': ['/workspace/processed_data/video_1753452111/traindata/00016.jpg'],\n",
       " 'caption': [''],\n",
       " 'latents': tensor([[[[[ 5.3613e-01,  5.2792e-01,  5.6895e-01,  ...,  5.6889e-01,\n",
       "              5.6411e-01,  5.0952e-01],\n",
       "            [ 4.4968e-01,  3.3040e-01,  3.2199e-01,  ...,  1.5734e-01,\n",
       "              1.5404e-01,  3.3015e-01],\n",
       "            [ 3.8823e-01,  4.1981e-01,  3.9511e-01,  ...,  2.5009e-01,\n",
       "              2.3648e-01,  3.3086e-01],\n",
       "            ...,\n",
       "            [-2.6776e-01,  2.4441e-01,  2.3819e-02,  ..., -4.6515e-01,\n",
       "             -5.4142e-01, -4.3836e-01],\n",
       "            [-8.0016e-02, -1.6726e-01,  1.2865e-02,  ..., -5.0893e-01,\n",
       "             -4.4473e-01, -3.2156e-01],\n",
       "            [ 1.3643e-02, -6.7854e-02, -4.2846e-02,  ..., -4.9982e-01,\n",
       "             -4.4468e-01, -1.7678e-01]]],\n",
       " \n",
       " \n",
       "          [[[ 6.4636e-01,  6.6739e-01,  4.3858e-01,  ...,  6.7812e-01,\n",
       "              6.1503e-01,  5.2010e-01],\n",
       "            [ 5.5126e-01,  3.9383e-01,  6.1295e-01,  ...,  6.0191e-01,\n",
       "              5.2656e-01,  4.2267e-01],\n",
       "            [ 8.5082e-01,  9.3517e-01,  1.0980e+00,  ...,  7.2316e-01,\n",
       "              6.8867e-01,  4.5109e-01],\n",
       "            ...,\n",
       "            [-2.4949e-01,  2.1667e-02,  8.2236e-03,  ..., -5.4332e-01,\n",
       "              6.7735e-02, -1.4799e-01],\n",
       "            [ 2.2119e-01, -5.7126e-01,  1.9002e-01,  ...,  8.4882e-02,\n",
       "              3.2143e-01, -2.1713e-01],\n",
       "            [-2.0573e-01, -6.8333e-02,  2.4016e-01,  ...,  3.5964e-02,\n",
       "              5.1252e-01, -1.6460e-03]]],\n",
       " \n",
       " \n",
       "          [[[-7.7085e-01, -6.7645e-01, -9.9917e-01,  ..., -1.0730e+00,\n",
       "             -1.0354e+00, -9.3447e-01],\n",
       "            [-5.8823e-01, -1.0286e+00, -1.0272e+00,  ..., -1.0642e+00,\n",
       "             -1.0797e+00, -1.0158e+00],\n",
       "            [-6.7989e-01, -1.0054e+00, -1.0013e+00,  ..., -9.0828e-01,\n",
       "             -9.0816e-01, -7.6571e-01],\n",
       "            ...,\n",
       "            [ 1.4138e-01, -2.3868e-01, -3.9885e-01,  ...,  4.9027e-02,\n",
       "              7.1554e-01,  7.4175e-01],\n",
       "            [ 2.5431e-01, -9.8470e-02,  1.1386e-02,  ...,  5.9111e-02,\n",
       "              5.0116e-01,  5.2620e-01],\n",
       "            [-1.8524e-01, -2.3415e-01, -2.3373e-01,  ...,  6.0206e-01,\n",
       "              3.4869e-01,  3.7477e-01]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 1.1843e-01, -4.2530e-02, -2.6943e-01,  ..., -1.9081e-01,\n",
       "             -1.7728e-02,  2.7266e-02],\n",
       "            [ 8.6008e-01,  4.1312e-01,  5.5545e-01,  ...,  8.6423e-01,\n",
       "              8.7124e-01,  7.3740e-01],\n",
       "            [-6.1791e-01, -7.1871e-01, -5.7705e-01,  ..., -2.4636e-01,\n",
       "             -2.2084e-01, -9.8279e-02],\n",
       "            ...,\n",
       "            [-4.6644e-01, -5.4880e-01, -3.9161e-01,  ..., -1.2443e-02,\n",
       "              1.2266e-01, -6.1934e-01],\n",
       "            [-3.8163e-01, -8.7077e-01, -5.1944e-02,  ..., -1.8160e-01,\n",
       "              1.2337e-01, -3.8092e-01],\n",
       "            [-1.0089e+00, -8.6931e-01, -9.1230e-02,  ...,  1.6786e-01,\n",
       "             -9.1078e-04, -2.5274e-01]]],\n",
       " \n",
       " \n",
       "          [[[-1.2557e-01,  9.9826e-02, -2.0096e-01,  ..., -2.2460e-01,\n",
       "             -2.7561e-01, -2.3949e-01],\n",
       "            [-1.3531e-01, -2.7016e-01, -1.9648e-01,  ..., -5.8305e-02,\n",
       "             -1.5389e-01, -1.3738e-01],\n",
       "            [-1.4597e-01, -1.6185e-01, -2.0658e-01,  ...,  6.4189e-02,\n",
       "              8.8900e-02, -2.6375e-02],\n",
       "            ...,\n",
       "            [-3.5558e-01, -1.8985e-01, -2.0111e-01,  ..., -4.3595e-01,\n",
       "              2.1948e-02, -2.4888e-01],\n",
       "            [-3.6404e-02, -4.8115e-01, -3.1447e-02,  ..., -2.5319e-01,\n",
       "             -3.9019e-03, -7.9307e-02],\n",
       "            [-4.3580e-01, -1.0579e-01, -2.1647e-01,  ..., -1.5728e-01,\n",
       "              9.1865e-02,  1.0714e-02]]],\n",
       " \n",
       " \n",
       "          [[[-1.6181e-01, -9.2035e-02, -3.4514e-01,  ..., -3.6115e-01,\n",
       "             -3.7892e-01, -3.9770e-01],\n",
       "            [ 9.0516e-02, -2.8966e-01, -3.3226e-01,  ..., -1.9148e-01,\n",
       "             -2.9683e-01, -2.6983e-01],\n",
       "            [ 1.1901e-01,  4.1821e-02,  7.1013e-02,  ...,  6.5329e-03,\n",
       "              3.0277e-02, -9.6799e-02],\n",
       "            ...,\n",
       "            [ 5.9674e-01,  9.9457e-02,  2.2828e-01,  ...,  4.6076e-01,\n",
       "              7.9664e-01,  8.2135e-01],\n",
       "            [ 7.2264e-01,  6.4608e-01,  3.6038e-01,  ...,  4.5905e-01,\n",
       "              1.0200e+00,  8.2575e-01],\n",
       "            [ 4.9940e-01,  6.4121e-01,  4.2070e-01,  ...,  4.4386e-01,\n",
       "              6.0617e-01,  5.9035e-01]]]]]),\n",
       " 'text_embeddings': tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          ...,\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "          [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]),\n",
       " 'seq_lens': tensor([1]),\n",
       " 'mask': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)# print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.5560,  0.5190,  0.2835,  ...,  0.4618,  0.4417,  0.5438],\n",
      "           [ 0.3523,  0.4687,  0.4062,  ...,  0.0445,  0.1618,  0.3272],\n",
      "           [ 0.4827,  0.2116,  0.2746,  ...,  0.0225,  0.2956,  0.0927],\n",
      "           ...,\n",
      "           [-0.1939, -0.0960, -0.1837,  ..., -0.5610, -0.5009, -0.4936],\n",
      "           [ 0.0934, -0.1163,  0.2162,  ..., -0.4202, -0.4093, -0.3961],\n",
      "           [ 0.0333, -0.0298, -0.0472,  ..., -0.3373, -0.5945, -0.1551]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4874,  0.6455,  0.4890,  ...,  0.7090,  0.5608,  0.4812],\n",
      "           [ 0.3058,  0.1869,  0.4426,  ...,  0.4327,  0.6946,  0.4421],\n",
      "           [ 0.8983,  0.6940,  0.7258,  ...,  0.6040,  0.4270,  0.5288],\n",
      "           ...,\n",
      "           [-0.3506, -0.0370,  0.0063,  ..., -0.1436, -0.0251, -0.2024],\n",
      "           [ 0.2682, -0.3992,  0.2365,  ..., -0.0665, -0.1306, -0.0611],\n",
      "           [-0.2594,  0.0068,  0.2343,  ..., -0.1181,  0.1598, -0.5005]]],\n",
      "\n",
      "\n",
      "         [[[-0.5898, -0.4113, -1.0634,  ..., -1.0641, -0.8769, -0.6729],\n",
      "           [-0.6078, -0.8159, -0.9582,  ..., -0.9279, -0.7869, -0.8493],\n",
      "           [-0.7430, -0.7007, -1.0854,  ..., -0.5966, -0.8004, -0.5210],\n",
      "           ...,\n",
      "           [ 0.2600, -0.2473, -0.4454,  ..., -0.1158,  0.3617,  0.5658],\n",
      "           [ 0.3674, -0.0499, -0.0376,  ..., -0.0313,  0.1501,  0.8814],\n",
      "           [ 0.0947, -0.1313, -0.1434,  ...,  0.2469,  0.2654,  0.2288]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0271, -0.0545,  0.1163,  ..., -0.2790, -0.3931, -0.1143],\n",
      "           [ 0.6787,  0.3730,  0.4022,  ...,  0.7726,  0.9400,  0.7556],\n",
      "           [-0.4979, -0.5570, -0.7223,  ..., -0.3098, -0.2150,  0.0098],\n",
      "           ...,\n",
      "           [-0.4023, -0.2235, -0.2010,  ..., -0.3128, -0.3152,  0.0336],\n",
      "           [-0.4265, -0.7674, -0.2865,  ..., -0.4389, -0.7769, -0.1641],\n",
      "           [-0.8614, -0.7305, -0.2113,  ..., -0.4390, -0.6971, -0.6341]]],\n",
      "\n",
      "\n",
      "         [[[-0.1888, -0.1943, -0.0873,  ..., -0.3014, -0.4191, -0.0856],\n",
      "           [-0.1969, -0.2789, -0.2908,  ...,  0.0343, -0.2724, -0.2202],\n",
      "           [-0.2579,  0.0294, -0.3405,  ...,  0.0319,  0.0493,  0.0596],\n",
      "           ...,\n",
      "           [-0.4200, -0.0913, -0.1759,  ..., -0.4141, -0.3286, -0.3384],\n",
      "           [ 0.0086, -0.5090, -0.0224,  ..., -0.2073, -0.2420, -0.3084],\n",
      "           [-0.3656, -0.2986, -0.2055,  ..., -0.1255,  0.0142, -0.6863]]],\n",
      "\n",
      "\n",
      "         [[[-0.0345, -0.0100, -0.4591,  ..., -0.2031, -0.4557, -0.2194],\n",
      "           [-0.0403, -0.1914, -0.2014,  ...,  0.0688, -0.3604, -0.1246],\n",
      "           [-0.0769,  0.0297,  0.2037,  ...,  0.1980,  0.0148, -0.1291],\n",
      "           ...,\n",
      "           [ 0.5646,  0.2120,  0.4601,  ...,  1.0885,  1.0413,  0.3405],\n",
      "           [ 0.7673,  0.6264,  0.2040,  ...,  0.8795,  1.0216,  0.7609],\n",
      "           [ 0.2417,  0.7372,  0.3351,  ...,  1.0033,  1.0646,  0.4545]]]]]), tensor([]), tensor([120.8590]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 6.1201e-01, -1.0058e-01, -2.9115e+00,  ..., -1.4217e+00,\n",
      "            -9.2809e-01,  5.9141e-02],\n",
      "           [-1.1037e+00,  1.1629e+00,  6.3351e-01,  ..., -8.7382e-01,\n",
      "            -1.3437e-01,  5.5434e-01],\n",
      "           [ 4.2757e-01, -1.7897e+00, -1.5685e+00,  ..., -1.4138e+00,\n",
      "            -1.2215e-01, -2.1177e+00],\n",
      "           ...,\n",
      "           [ 3.6717e-01, -2.6553e+00, -1.7137e+00,  ..., -4.4211e-01,\n",
      "             6.4303e-02,  5.8266e-01],\n",
      "           [ 1.5214e+00,  2.8523e-01,  1.9960e+00,  ...,  2.9381e-01,\n",
      "             6.8077e-01,  4.4825e-01],\n",
      "           [ 2.4781e-01,  3.4516e-01, -1.7423e-02,  ..., -1.0996e-01,\n",
      "            -9.4094e-01,  2.2374e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.4051e+00, -9.7401e-01, -2.6234e-01,  ...,  5.8784e-01,\n",
      "             3.4456e-01, -2.8178e-01],\n",
      "           [-1.3580e+00, -2.4234e+00, -2.0878e+00,  ..., -1.0126e+00,\n",
      "             4.6193e-01, -7.4442e-01],\n",
      "           [ 7.7117e-01, -2.5335e+00, -2.2111e+00,  ..., -1.2406e+00,\n",
      "            -2.0828e+00,  6.3464e-01],\n",
      "           ...,\n",
      "           [-7.8566e-01, -4.5940e-01,  5.4426e-01,  ..., -1.3084e+00,\n",
      "            -6.8376e-03, -3.2783e-01],\n",
      "           [ 4.3349e-01,  1.7247e+00,  5.8350e-01,  ..., -1.9095e+00,\n",
      "             3.4014e-01,  1.2106e-01],\n",
      "           [-7.6559e-01,  7.3022e-01,  1.0553e-01,  ...,  6.0833e-01,\n",
      "             7.9452e-01,  6.8497e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3654e+00,  1.7378e+00, -3.4699e-01,  ..., -6.3030e-01,\n",
      "             1.4679e+00,  1.7757e+00],\n",
      "           [-2.1189e-02,  1.1798e+00,  2.4776e-02,  ...,  1.6336e+00,\n",
      "             1.8636e+00,  1.0352e+00],\n",
      "           [ 1.2548e+00,  1.6730e+00, -9.1151e-02,  ...,  1.8886e+00,\n",
      "             2.2282e-01,  1.6163e+00],\n",
      "           ...,\n",
      "           [ 7.5321e-01, -2.0328e-01, -9.3402e-01,  ..., -1.1238e-01,\n",
      "            -1.8442e+00, -3.1226e-01],\n",
      "           [ 6.8961e-01,  4.3677e-01, -5.4539e-01,  ..., -1.6184e+00,\n",
      "            -1.5187e+00,  9.3793e-01],\n",
      "           [ 2.1654e+00,  7.5013e-01,  9.7439e-01,  ...,  1.1817e+00,\n",
      "             6.9551e-01, -6.9554e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-6.6855e-01, -9.8636e-01,  2.4725e+00,  ..., -8.2264e-01,\n",
      "            -1.2707e+00, -4.5790e-01],\n",
      "           [-1.4229e+00,  4.8297e-02, -1.0465e+00,  ...,  1.3904e-01,\n",
      "            -3.9331e-01,  1.9143e-01],\n",
      "           [ 3.2782e-01,  1.9935e-01, -8.0188e-01,  ...,  2.2161e-01,\n",
      "             5.0322e-01,  1.2752e+00],\n",
      "           ...,\n",
      "           [ 3.4573e-01,  1.6642e+00,  1.1793e+00,  ...,  8.6161e-01,\n",
      "             2.7843e+00, -4.4353e-01],\n",
      "           [-8.0280e-01,  9.1654e-01, -1.5936e+00,  ...,  1.5725e+00,\n",
      "             5.3853e-01,  1.9059e+00],\n",
      "           [ 9.9871e-01,  7.0274e-01, -3.3205e-01,  ...,  6.7374e-01,\n",
      "            -8.7588e-01,  1.0579e+00]]],\n",
      "\n",
      "\n",
      "         [[[-2.2117e-01, -2.0117e+00,  1.4298e+00,  ..., -5.6299e-01,\n",
      "            -1.6877e+00,  8.3302e-01],\n",
      "           [-8.9168e-01,  1.2219e-01, -1.4846e-01,  ...,  1.2814e+00,\n",
      "            -6.2168e-01, -3.8649e-01],\n",
      "           [-3.3796e-01,  2.0550e+00, -1.6261e+00,  ..., -2.9693e-01,\n",
      "            -1.6334e-01,  6.3174e-01],\n",
      "           ...,\n",
      "           [-3.5416e-01,  9.9516e-01,  6.1650e-01,  ..., -8.3945e-02,\n",
      "             3.2152e-01, -2.1864e+00],\n",
      "           [ 5.2639e-01,  1.0016e-02,  6.7529e-01,  ...,  1.4001e-03,\n",
      "             7.7035e-01,  4.1982e-01],\n",
      "           [ 8.1200e-01, -7.1084e-01, -3.1728e-02,  ...,  6.1985e-01,\n",
      "             2.1876e-01, -1.4359e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 8.8120e-01,  3.9176e-01, -9.0361e-01,  ...,  1.0733e+00,\n",
      "            -9.6206e-02,  1.2876e+00],\n",
      "           [-2.1036e-01,  6.5678e-01,  7.8393e-01,  ...,  1.8145e+00,\n",
      "            -4.2099e-01,  1.4325e+00],\n",
      "           [-7.5282e-01,  4.9956e-01,  1.7364e+00,  ...,  1.3161e+00,\n",
      "             5.1673e-01,  2.9743e-01],\n",
      "           ...,\n",
      "           [-1.0184e-01,  1.2695e+00,  1.9794e+00,  ..., -1.5442e+00,\n",
      "            -1.2658e+00, -1.2180e+00],\n",
      "           [ 1.0863e-01, -7.5029e-01, -1.7632e+00,  ..., -3.4581e+00,\n",
      "             5.3080e-01, -9.4141e-01],\n",
      "           [-2.2012e+00,  9.5294e-01, -8.6437e-01,  ...,  7.3676e-01,\n",
      "             1.2526e+00, -1.5341e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-0.8149, -0.0184,  0.8637,  ...,  0.3648,  0.4875, -0.4337],\n",
      "           [-0.2736,  0.1112,  0.3181,  ..., -0.2055,  0.2384, -0.3243],\n",
      "           [ 0.2230,  0.5910,  0.1624,  ...,  0.3215,  0.1076,  0.4658],\n",
      "           ...,\n",
      "           [ 1.0852,  0.4002,  0.7942,  ..., -0.1916, -0.9207, -0.9353],\n",
      "           [ 0.0086, -0.3357,  0.3907,  ..., -0.7965,  0.1727,  0.8119],\n",
      "           [-0.4194, -0.4373, -0.7949,  ..., -1.3510,  0.9317, -0.2546]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9782,  1.3033, -0.3975,  ...,  1.0190,  0.4527,  0.4809],\n",
      "           [-0.6413,  0.0718,  0.1575,  ..., -0.8704, -1.1198, -0.5996],\n",
      "           [ 1.1433,  0.2414,  0.0445,  ...,  0.2122, -0.5286,  1.8085],\n",
      "           ...,\n",
      "           [-0.7058,  0.4777,  0.1191,  ..., -0.5376, -1.1307,  0.2859],\n",
      "           [ 0.0531, -0.1447, -0.1780,  ...,  0.5604, -0.2873, -0.3755],\n",
      "           [ 0.0222,  0.0233,  0.1547,  ..., -0.1771, -0.7151,  0.1402]]],\n",
      "\n",
      "\n",
      "         [[[-0.1314, -1.1636,  0.3375,  ..., -1.0840,  0.0259, -0.3305],\n",
      "           [-0.0611,  0.3126, -0.3733,  ..., -1.1342, -1.3390, -2.1611],\n",
      "           [ 1.2518, -0.2189, -0.8368,  ..., -0.9222,  0.6093, -1.0008],\n",
      "           ...,\n",
      "           [-0.1289, -1.1710, -0.1222,  ..., -0.1837,  0.3728, -0.5175],\n",
      "           [-0.1268, -0.8941, -0.2806,  ...,  0.8438, -0.2201,  0.4078],\n",
      "           [-0.8249,  0.2687, -0.7731,  ...,  1.1603, -0.3231,  0.3280]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.2522, -0.1370, -0.9160,  ..., -0.5479, -0.9533, -0.4935],\n",
      "           [ 0.4271,  1.1880, -0.2875,  ...,  0.2527, -0.4888, -0.3853],\n",
      "           [ 0.4943,  0.6818,  0.0471,  ...,  0.9037,  0.0589,  0.3778],\n",
      "           ...,\n",
      "           [ 0.8224, -0.2569, -1.2073,  ..., -0.6883, -0.2177, -0.4920],\n",
      "           [-0.2623,  0.3810,  1.1163,  ...,  0.2117, -0.1973,  0.3839],\n",
      "           [ 0.8396, -1.0275, -0.5666,  ..., -1.4024, -0.8855, -0.3600]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9405,  0.2924,  0.3740,  ...,  0.8377, -0.1610, -0.5084],\n",
      "           [-0.5203, -0.1820, -0.3687,  ..., -0.4298,  0.2319, -0.0543],\n",
      "           [-0.0750, -0.3048, -0.6661,  ..., -0.1215, -0.2821,  0.6622],\n",
      "           ...,\n",
      "           [ 0.6561,  0.5636, -0.5496,  ..., -0.1859,  1.3095,  0.1061],\n",
      "           [ 0.2975, -0.3248, -0.3938,  ...,  0.3043, -0.1278, -0.3064],\n",
      "           [-0.5777,  0.4081,  0.0989,  ..., -0.5733,  0.3154,  0.5733]]],\n",
      "\n",
      "\n",
      "         [[[-1.1561,  0.1189, -0.8184,  ..., -0.0566, -0.3116, -0.1613],\n",
      "           [-0.5757,  0.4037,  0.0370,  ...,  0.1387,  0.0174, -0.9962],\n",
      "           [-1.2869,  0.0561,  0.3032,  ...,  0.4483, -0.5339, -0.0345],\n",
      "           ...,\n",
      "           [ 0.6007, -0.2040, -0.4194,  ..., -0.2143,  1.2707,  0.5549],\n",
      "           [-0.0695,  1.0476,  0.4452,  ...,  0.6252, -0.5931, -0.2810],\n",
      "           [ 0.7512, -0.4245,  1.0323,  ..., -1.6704, -0.0242,  0.6468]]]]]), tensor([]), tensor([676.5458]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.9970, -0.8075,  0.4357,  ..., -0.3016, -0.1133, -1.3942],\n",
      "           [-1.0691, -0.3240, -0.0057,  ..., -0.5363,  0.1247, -0.9674],\n",
      "           [-0.2442,  0.2531, -0.3440,  ...,  0.1056, -0.1906,  0.1994],\n",
      "           ...,\n",
      "           [ 1.9999,  0.2302,  1.1387,  ...,  0.4043, -0.5606, -0.7345],\n",
      "           [ 0.1310, -0.2490,  0.5585,  ..., -0.4250,  0.9127,  1.6754],\n",
      "           [-0.6401, -0.5461, -1.1117,  ..., -1.2581,  2.0345, -0.1150]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4905,  0.9399, -1.2358,  ...,  0.5039, -0.2399, -0.0580],\n",
      "           [-1.7627, -0.4759, -0.6732,  ..., -2.1762, -2.4335, -1.5110],\n",
      "           [ 0.4322, -1.0255, -1.5572,  ..., -0.7553, -1.7993,  2.0064],\n",
      "           ...,\n",
      "           [-0.6745,  0.6740,  0.1639,  ...,  0.0084, -1.7715,  0.6413],\n",
      "           [-0.2485,  0.6305, -0.5440,  ...,  0.7029, -0.8997, -0.2341],\n",
      "           [ 0.3369,  0.1355, -0.1264,  ..., -0.3150, -1.8145,  0.2096]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9451, -0.7200,  1.9758,  ..., -0.0164,  1.5687,  0.8927],\n",
      "           [ 0.7791,  1.9824,  0.9666,  ..., -0.1035, -0.3834, -1.6929],\n",
      "           [ 2.8553,  1.1626,  0.2431,  ..., -0.0206,  2.2430, -0.3475],\n",
      "           ...,\n",
      "           [-0.3995, -1.3780,  0.4089,  ..., -0.3441, -0.5066, -1.8614],\n",
      "           [-0.5633, -1.1761, -0.4316,  ...,  1.1598, -1.0661, -0.1750],\n",
      "           [-0.9455,  0.7433, -0.7973,  ...,  0.8251, -0.9929, -0.0692]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1977, -0.1396, -0.9557,  ..., -0.5278, -1.3828, -0.7698],\n",
      "           [-0.6399,  1.1453, -1.2460,  ..., -0.9039, -2.0103, -1.6594],\n",
      "           [ 1.6440,  2.0701,  0.9225,  ...,  1.6999,  0.4134,  0.7037],\n",
      "           ...,\n",
      "           [ 1.9050,  0.4315, -1.2057,  ..., -0.9990, -0.5031,  0.1882],\n",
      "           [ 0.1764,  1.8502,  1.7268,  ...,  0.5813, -0.4740,  1.1304],\n",
      "           [ 2.7323, -0.2339, -0.7027,  ..., -2.3211, -1.3075, -0.1585]]],\n",
      "\n",
      "\n",
      "         [[[ 1.5757,  0.2847,  0.8498,  ...,  1.5702,  0.1694, -0.3975],\n",
      "           [-0.5691,  0.1303, -0.2546,  ..., -0.5490,  0.5702,  0.1227],\n",
      "           [ 0.1049, -0.2114, -0.6793,  ..., -0.2744, -0.5484,  1.0178],\n",
      "           ...,\n",
      "           [ 1.4954,  1.1136, -0.5151,  ...,  0.3696,  1.9031,  0.5247],\n",
      "           [ 0.4935,  0.2311, -0.5356,  ...,  0.8240, -0.1832, -0.3356],\n",
      "           [-0.2097,  0.7596,  0.4662,  ..., -0.6149,  0.3304,  0.8315]]],\n",
      "\n",
      "\n",
      "         [[[-1.4696,  0.3118, -0.6996,  ...,  0.4502,  0.0996,  0.3494],\n",
      "           [-0.9848,  1.0248,  0.5457,  ...,  0.4880,  0.4645, -1.0736],\n",
      "           [-2.0780,  0.0211,  0.3431,  ...,  0.6529, -0.8339,  0.0921],\n",
      "           ...,\n",
      "           [ 0.0059, -0.4486, -0.9574,  ..., -0.9978,  0.7007, -0.3939],\n",
      "           [-1.1708,  0.5934,  0.1254,  ...,  0.2456, -2.3842, -1.6358],\n",
      "           [ 0.3722, -1.5752,  0.9040,  ..., -3.1251, -0.9317,  0.0835]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.6894, -0.3298,  0.9802,  ...,  0.0898,  0.5414, -0.0181],\n",
      "           [ 0.0851,  0.5610, -0.2891,  ...,  0.5302, -0.0331,  0.4746],\n",
      "           [-0.2599,  0.0532,  0.2922,  ...,  0.0967, -0.8264,  0.7604],\n",
      "           ...,\n",
      "           [-0.7789, -0.0177,  0.5149,  ..., -0.8524, -0.0908, -0.4737],\n",
      "           [-0.3169,  0.0491, -0.1454,  ..., -0.4992, -0.1930, -0.2042],\n",
      "           [-0.4054, -0.3171,  0.8947,  ..., -0.1499, -0.8262,  0.0682]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4481, -0.0734,  0.0526,  ...,  0.2813, -0.3823, -0.1697],\n",
      "           [ 0.8463,  1.1095,  0.7730,  ...,  0.1528,  0.5316, -0.6402],\n",
      "           [ 0.9109,  0.4051,  0.1872,  ...,  0.1417,  0.1461,  0.4099],\n",
      "           ...,\n",
      "           [-0.6236,  0.3721,  0.3204,  ..., -0.1072,  0.1262, -0.0682],\n",
      "           [ 0.0597, -0.2123,  0.7679,  ..., -0.3623, -0.7375, -0.1310],\n",
      "           [-0.4439, -0.0340,  0.0680,  ..., -0.0604,  0.3258,  0.4727]]],\n",
      "\n",
      "\n",
      "         [[[-0.0933, -0.9449, -1.1433,  ..., -0.1393, -0.5501, -0.9331],\n",
      "           [ 0.0586, -1.0435, -0.3037,  ..., -0.1955, -0.5249, -0.5301],\n",
      "           [-0.7156, -0.6550, -0.3345,  ...,  0.0845, -0.8534, -0.9366],\n",
      "           ...,\n",
      "           [ 0.2289, -0.6288, -0.8645,  ...,  0.4298,  0.1271,  0.9713],\n",
      "           [-0.4574, -0.2252,  0.1834,  ...,  0.3591,  0.6373,  0.7895],\n",
      "           [-0.3951, -0.3025, -0.1785,  ..., -0.0416,  0.1845,  0.2409]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0502, -0.0778, -0.7321,  ...,  0.1576,  0.2590,  0.1222],\n",
      "           [ 0.5479, -0.2570,  0.0124,  ...,  0.5320,  0.3035,  0.2530],\n",
      "           [-0.2820, -0.4735, -0.9442,  ..., -0.3360, -0.3449, -0.2310],\n",
      "           ...,\n",
      "           [-0.1230, -0.2030, -0.3719,  ..., -0.1724,  0.5656, -0.4905],\n",
      "           [-0.4386, -0.1191,  0.4053,  ..., -0.1131,  0.1592,  0.4376],\n",
      "           [-0.5936, -0.2848,  0.0450,  ..., -0.5705,  0.3863,  0.0295]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0256,  0.2353, -0.2070,  ..., -0.1491, -0.1526, -0.1684],\n",
      "           [-0.1550,  0.3218, -0.9104,  ..., -0.1455, -0.7193, -0.0181],\n",
      "           [ 0.0925, -0.1102,  0.2645,  ..., -0.2122,  0.0589,  0.0655],\n",
      "           ...,\n",
      "           [-0.1603, -0.2596, -0.1708,  ..., -0.3819,  0.2370, -0.1447],\n",
      "           [-0.0306, -0.5562, -0.2982,  ...,  0.2035, -0.4484, -1.0060],\n",
      "           [-0.2463, -0.2166, -0.1297,  ...,  0.3750, -0.3745, -0.3377]]],\n",
      "\n",
      "\n",
      "         [[[-0.3494,  0.6177, -0.1613,  ..., -0.1752, -0.5384, -0.2195],\n",
      "           [ 0.4235,  0.1597, -0.4765,  ...,  0.2222, -0.2243, -0.1072],\n",
      "           [ 0.3796, -0.1927, -0.1951,  ...,  0.7199,  0.5735,  0.3138],\n",
      "           ...,\n",
      "           [ 0.2106,  0.8254,  0.0161,  ...,  0.9975,  0.7367,  0.4662],\n",
      "           [ 0.3345,  0.0711,  0.2579,  ...,  0.3102, -0.0428,  0.8527],\n",
      "           [ 0.5476, -0.4856,  0.5741,  ...,  0.8598,  0.3209,  0.2132]]]]]), tensor([]), tensor([366.7200]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 4.4722e-01, -2.3490e+00,  1.0149e+00,  ..., -1.3098e+00,\n",
      "            -2.4192e-01, -1.5475e+00],\n",
      "           [-9.8397e-01,  7.7240e-01, -1.6627e+00,  ...,  8.8787e-01,\n",
      "            -5.2702e-01,  5.0154e-01],\n",
      "           [-1.6618e+00, -7.5658e-01, -1.7842e-01,  ..., -7.4658e-01,\n",
      "            -2.9903e+00,  1.1168e+00],\n",
      "           ...,\n",
      "           [-1.3400e+00, -8.0798e-01,  1.3386e+00,  ..., -7.4310e-01,\n",
      "             1.2959e+00, -2.4377e-01],\n",
      "           [-4.9384e-01,  6.0843e-01, -3.9602e-01,  ...,  7.7804e-02,\n",
      "             1.0012e+00,  4.2372e-01],\n",
      "           [-1.1684e+00, -6.9160e-01,  2.6499e+00,  ...,  8.0179e-01,\n",
      "            -1.1258e+00,  1.2794e+00]]],\n",
      "\n",
      "\n",
      "         [[[-5.0774e-01, -2.0143e+00, -1.5031e+00,  ..., -1.2052e+00,\n",
      "            -2.5977e+00, -1.9553e+00],\n",
      "           [ 8.6558e-01,  1.5491e+00,  2.0597e-01,  ..., -1.3788e+00,\n",
      "             5.0722e-02, -3.0279e+00],\n",
      "           [-7.0516e-02, -1.9313e+00, -2.8349e+00,  ..., -1.6474e+00,\n",
      "            -1.2657e+00, -3.2549e-01],\n",
      "           ...,\n",
      "           [-8.7244e-01,  1.1280e+00,  1.1372e+00,  ...,  3.6416e-01,\n",
      "            -1.2655e-01,  8.0385e-01],\n",
      "           [-4.9415e-01,  8.2160e-01,  1.2769e+00,  ..., -1.0306e+00,\n",
      "            -2.6941e+00, -4.5241e-01],\n",
      "           [-5.2534e-01,  3.6684e-01, -2.7258e-01,  ..., -6.2035e-01,\n",
      "             9.0751e-01,  1.0071e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8131e+00, -7.1784e-01, -1.5918e-01,  ...,  1.9806e+00,\n",
      "             1.4165e+00, -1.4487e-01],\n",
      "           [ 1.8088e+00,  5.2169e-02,  2.0868e+00,  ...,  2.0864e+00,\n",
      "             1.5041e+00,  1.1308e+00],\n",
      "           [ 2.2913e-01,  5.3566e-01,  1.7638e+00,  ...,  2.5611e+00,\n",
      "             1.5079e-01, -5.1594e-01],\n",
      "           ...,\n",
      "           [ 2.6109e-01, -1.1061e+00, -1.3963e+00,  ...,  7.5772e-01,\n",
      "            -6.1093e-01,  6.1414e-01],\n",
      "           [-1.9783e+00, -3.7421e-01,  3.6096e-01,  ...,  7.7737e-01,\n",
      "             8.8969e-01,  5.2762e-04],\n",
      "           [-5.9323e-01, -1.1208e-01,  2.2024e-01,  ..., -1.0930e+00,\n",
      "             1.1595e-02,  6.9524e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-6.8794e-01, -3.1131e-01, -1.2132e+00,  ...,  7.6345e-01,\n",
      "             3.7196e-01, -7.1548e-02],\n",
      "           [-7.8688e-01, -1.4194e+00, -1.1320e+00,  ..., -7.6518e-01,\n",
      "            -1.3276e+00, -8.0301e-01],\n",
      "           [ 2.2203e-02,  7.5977e-02, -1.4176e+00,  ..., -1.8377e-01,\n",
      "             3.0144e-01, -2.9312e-01],\n",
      "           ...,\n",
      "           [ 8.2848e-01,  5.6098e-01, -1.3614e-02,  ...,  2.4325e-01,\n",
      "             1.3344e+00,  9.8422e-02],\n",
      "           [-5.2299e-02,  1.9939e+00,  1.4644e+00,  ...,  1.5142e-01,\n",
      "             1.0585e+00,  2.0423e+00],\n",
      "           [ 1.0750e+00,  1.4899e+00,  3.9923e-01,  ..., -1.4286e+00,\n",
      "             1.3902e+00,  2.8922e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.7293e-01,  4.4485e-01, -3.0413e-02,  ...,  7.4874e-02,\n",
      "             3.5536e-01,  1.9786e-01],\n",
      "           [ 1.8760e-03,  1.4353e+00, -2.0549e+00,  ..., -7.0307e-02,\n",
      "            -1.1172e+00,  5.4810e-01],\n",
      "           [ 5.7932e-01,  3.7031e-02,  9.2138e-01,  ..., -9.4811e-01,\n",
      "             1.6674e-02,  8.1447e-02],\n",
      "           ...,\n",
      "           [ 5.9976e-01, -1.0150e-01,  1.8002e-01,  ..., -2.1974e-01,\n",
      "             7.1477e-01,  5.4367e-01],\n",
      "           [ 1.7349e-02, -1.7000e-01, -6.6744e-01,  ...,  8.7297e-01,\n",
      "            -1.0604e+00, -1.9269e+00],\n",
      "           [ 4.2565e-01, -2.5034e-01,  3.0759e-01,  ...,  1.1212e+00,\n",
      "            -7.7439e-01, -5.1226e-02]]],\n",
      "\n",
      "\n",
      "         [[[-5.6707e-01,  1.6759e+00,  5.7444e-01,  ...,  2.7290e-01,\n",
      "            -4.3522e-01,  4.6725e-01],\n",
      "           [ 8.6522e-01,  1.0846e+00, -6.3467e-01,  ...,  1.0305e+00,\n",
      "             6.1628e-02,  3.4589e-01],\n",
      "           [ 5.1730e-01, -1.0091e+00, -7.6114e-01,  ...,  2.1761e+00,\n",
      "             1.7652e+00,  9.0089e-01],\n",
      "           ...,\n",
      "           [-1.0432e+00,  2.0195e+00, -5.5361e-01,  ...,  3.0149e-01,\n",
      "             1.8130e-01, -8.9677e-01],\n",
      "           [-1.2359e+00, -1.6205e+00, -3.3797e-01,  ..., -5.3005e-01,\n",
      "            -2.2272e+00,  4.4646e-01],\n",
      "           [ 2.1616e-01, -3.0668e+00,  3.8951e-01,  ...,  8.4521e-01,\n",
      "            -6.6455e-01, -3.4144e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 1.8724, -0.4246,  0.5064,  ..., -2.3096,  0.7753,  0.0652],\n",
      "           [-0.2381,  0.5280,  0.2955,  ...,  1.2923,  0.4414, -0.3122],\n",
      "           [-0.2929, -0.2697,  0.6611,  ...,  1.3802,  0.5483,  0.6870],\n",
      "           ...,\n",
      "           [-0.7536,  0.3718, -1.2391,  ...,  0.6710,  0.3611, -1.3030],\n",
      "           [ 0.8686,  0.7389, -0.1720,  ..., -1.4597,  0.2313, -0.4595],\n",
      "           [ 0.4929,  0.2986, -1.2798,  ..., -0.4335,  0.9964, -0.0955]]],\n",
      "\n",
      "\n",
      "         [[[-0.6785, -0.5353, -0.8358,  ..., -0.9140,  0.2870, -1.2270],\n",
      "           [ 0.5292, -1.0026, -0.9257,  ...,  0.0275, -0.0453, -0.4212],\n",
      "           [-0.3440, -0.1971, -0.4325,  ...,  0.0667,  0.0869,  0.3971],\n",
      "           ...,\n",
      "           [-0.1379, -0.0672, -1.6444,  ..., -0.3890, -0.7381, -1.0298],\n",
      "           [ 1.0403,  0.4219,  0.2649,  ...,  0.4679,  2.4970, -1.5434],\n",
      "           [ 1.2183,  0.2703,  0.5938,  ...,  1.1335,  0.3213, -1.3480]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0132,  0.8901, -0.1383,  ..., -0.1461, -0.8111,  0.2491],\n",
      "           [-0.3928,  1.1742,  0.8045,  ..., -0.3203,  0.4990, -1.5346],\n",
      "           [-0.2422,  0.1832, -0.7726,  ...,  0.2414, -0.5648, -0.5126],\n",
      "           ...,\n",
      "           [ 0.2832,  0.9923, -0.2764,  ..., -0.9657, -0.2821, -0.6724],\n",
      "           [-0.4561, -0.7298,  0.8298,  ...,  0.8591,  0.4280,  0.0950],\n",
      "           [-0.2337, -0.0631, -0.0511,  ...,  0.2274, -0.8948, -1.0045]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.1372, -1.3018,  1.5511,  ..., -0.5505, -0.7828,  0.8662],\n",
      "           [ 0.3840, -0.9008, -0.9954,  ...,  0.8849, -0.6270, -0.6757],\n",
      "           [ 0.9170,  0.3785, -1.4907,  ...,  0.7864,  0.4497, -0.4276],\n",
      "           ...,\n",
      "           [-0.9051,  0.2662, -1.1204,  ...,  0.4545, -0.6749,  0.2323],\n",
      "           [-0.1469, -0.2418,  0.3073,  ...,  0.4459, -0.3304,  0.0295],\n",
      "           [ 0.3083, -0.0077,  0.3856,  ...,  0.4606, -0.2509, -1.1850]]],\n",
      "\n",
      "\n",
      "         [[[-0.9956,  0.5749, -0.1523,  ...,  1.7042, -0.5121,  1.5495],\n",
      "           [-1.0932,  1.1559,  0.4184,  ...,  0.9985, -0.5615,  0.2669],\n",
      "           [ 0.5592, -0.0681, -1.0851,  ...,  0.2285,  0.3253, -0.4295],\n",
      "           ...,\n",
      "           [ 0.3357,  0.5137,  0.7589,  ...,  0.8175,  1.9228,  2.1791],\n",
      "           [ 1.5209, -1.0757,  1.1281,  ..., -0.7023,  0.0384,  0.8366],\n",
      "           [ 0.3182,  0.1168, -0.0253,  ..., -0.0288, -1.1290, -1.5841]]],\n",
      "\n",
      "\n",
      "         [[[-0.5965, -0.8670,  0.5421,  ..., -0.4285,  0.2687, -1.2992],\n",
      "           [-0.7280,  1.1926, -0.9816,  ...,  0.7685,  0.8101, -0.2862],\n",
      "           [ 0.0764, -0.0758,  0.9360,  ...,  0.5371, -0.5203, -0.3084],\n",
      "           ...,\n",
      "           [ 1.0476,  0.9270, -0.8734,  ...,  0.7853,  0.2347,  1.4013],\n",
      "           [-0.7699, -0.2705,  1.0677,  ...,  0.7209,  0.5261, -0.9704],\n",
      "           [-1.0620,  0.7499,  0.2751,  ...,  0.4417, -1.4670,  0.0451]]]]]), tensor([]), tensor([839.6549]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.6200, -1.1762, -0.0841,  ..., -3.5166,  0.2669, -0.5701],\n",
      "           [-0.9119,  0.2248, -0.0692,  ...,  1.3602,  0.3143, -0.6975],\n",
      "           [-0.9164, -0.9101,  0.2682,  ...,  1.3034,  0.2371,  0.4010],\n",
      "           ...,\n",
      "           [-0.6092,  0.1451, -1.5112,  ...,  1.4506,  0.8965, -0.8035],\n",
      "           [ 1.1658,  1.0650, -0.1612,  ..., -1.1343,  0.9872,  0.0505],\n",
      "           [ 0.5668,  0.4212, -1.4810,  ..., -0.0043,  1.7077,  0.4173]]],\n",
      "\n",
      "\n",
      "         [[[-1.5750, -1.5741, -1.5813,  ..., -1.8566, -0.2921, -2.0979],\n",
      "           [ 0.0577, -1.7113, -1.7843,  ..., -0.6202, -0.7546, -1.1102],\n",
      "           [-1.3168, -1.3362, -1.6681,  ..., -0.7964, -0.8048, -0.2163],\n",
      "           ...,\n",
      "           [ 0.1474, -0.0476, -1.9166,  ..., -0.7402, -0.9797, -1.2437],\n",
      "           [ 0.9862,  1.2734,  0.0774,  ..., -0.3098,  3.2615, -1.7658],\n",
      "           [ 1.6757,  0.4763,  0.4388,  ...,  1.1170, -0.3075, -1.3705]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9186,  1.8038,  1.0372,  ...,  1.0255,  0.2965,  1.3534],\n",
      "           [ 0.2621,  2.5899,  2.1540,  ...,  0.9391,  1.8416, -0.6375],\n",
      "           [ 0.7877,  1.3325,  0.3354,  ...,  1.2970,  0.3478,  0.2426],\n",
      "           ...,\n",
      "           [ 0.1466,  1.4749,  0.0703,  ..., -1.1255, -1.1228, -1.5427],\n",
      "           [-0.8721, -0.7225,  0.9779,  ...,  0.7474, -0.1246, -0.5669],\n",
      "           [-0.0752,  0.1923,  0.2448,  ...,  0.1784, -1.1607, -1.8164]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.2263, -1.7166,  2.1120,  ..., -0.4401, -0.5955,  1.1527],\n",
      "           [-0.6355, -1.5573, -1.8827,  ...,  0.1930, -1.9419, -1.6259],\n",
      "           [ 1.5205,  1.0405, -1.3829,  ...,  1.2128,  0.7564, -0.2761],\n",
      "           ...,\n",
      "           [-0.5857,  0.7712, -0.9112,  ...,  1.2078,  0.1359,  0.1908],\n",
      "           [ 0.2174,  0.7122,  0.4685,  ...,  0.7486,  0.6705,  0.0931],\n",
      "           [ 1.5334,  0.9358,  0.6459,  ...,  0.9974, -0.1613,  0.1254]]],\n",
      "\n",
      "\n",
      "         [[[-0.9844,  0.6365,  0.1475,  ...,  2.3050, -0.3451,  2.0732],\n",
      "           [-1.1692,  1.7790,  0.8193,  ...,  1.3485, -0.4316,  0.5032],\n",
      "           [ 0.9663,  0.1872, -0.9932,  ...,  0.2284,  0.2914, -0.5411],\n",
      "           ...,\n",
      "           [ 0.8465,  0.8539,  1.1629,  ...,  1.0755,  2.5497,  2.5858],\n",
      "           [ 1.8827, -0.6902,  1.4201,  ..., -0.9957,  0.1861,  1.0697],\n",
      "           [ 0.9237,  0.3741,  0.1987,  ...,  0.2211, -1.4333, -1.4855]]],\n",
      "\n",
      "\n",
      "         [[[-0.4919, -0.9462,  1.1349,  ..., -0.1121,  0.8645, -1.0972],\n",
      "           [-0.8133,  1.8625, -0.8015,  ...,  1.1335,  1.3123,  0.0231],\n",
      "           [ 0.0793, -0.0594,  1.1295,  ...,  0.6165, -0.5744, -0.2416],\n",
      "           ...,\n",
      "           [ 0.5608,  1.0256, -1.2889,  ..., -0.0945, -0.9994,  0.9331],\n",
      "           [-1.8294, -1.0891,  0.8051,  ..., -0.2421, -0.7257, -2.4563],\n",
      "           [-1.8670,  0.1443, -0.1784,  ..., -0.3373, -2.3136, -1.2181]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.0921,  0.6557,  0.8875,  ..., -0.9840, -0.1512,  0.5558],\n",
      "           [-0.1907,  0.9865,  0.3524,  ...,  0.1533,  0.8115,  0.1538],\n",
      "           [ 0.1238, -0.0818, -0.1491,  ..., -0.1814, -1.6136,  1.0928],\n",
      "           ...,\n",
      "           [ 2.4167, -0.9899, -0.6936,  ..., -0.2234,  0.6239, -0.3325],\n",
      "           [-0.5542, -0.3162,  0.0666,  ..., -1.6036, -1.5503, -1.3838],\n",
      "           [-0.1920, -0.1526, -0.3523,  ..., -0.0850, -1.1037, -0.8495]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1113, -0.6061, -0.5953,  ..., -0.7038,  1.4143, -0.5910],\n",
      "           [ 0.9196,  1.6765, -0.9344,  ..., -0.6131,  0.1609,  0.5061],\n",
      "           [-0.7422, -1.8045,  0.1224,  ...,  0.6044, -0.0106,  0.5514],\n",
      "           ...,\n",
      "           [ 1.2824, -0.7610,  0.1557,  ..., -0.3720,  0.5346, -0.5667],\n",
      "           [ 0.3085, -0.6888,  0.3727,  ..., -0.1387, -0.4160,  0.8628],\n",
      "           [ 0.3926, -0.1966,  0.9169,  ..., -0.3184,  0.2396,  0.1854]]],\n",
      "\n",
      "\n",
      "         [[[-0.0394,  0.3131,  0.7480,  ..., -1.8315, -0.5367, -0.5704],\n",
      "           [ 0.2283, -0.8302, -0.2585,  ..., -1.9240,  0.4539, -1.1085],\n",
      "           [-0.4982, -1.3558, -0.9409,  ..., -1.0085, -0.5985, -0.6695],\n",
      "           ...,\n",
      "           [-0.3567, -0.1353, -0.1510,  ..., -0.7576, -0.1920, -0.7661],\n",
      "           [-0.2426, -1.0724,  0.3931,  ..., -0.9023, -0.2975, -0.9207],\n",
      "           [-1.3421,  1.1167, -1.5525,  ..., -0.2515,  0.4090,  0.5764]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.8769, -0.2321,  1.0765,  ...,  1.3275, -1.5387, -0.5081],\n",
      "           [-0.0493, -0.5354,  0.0522,  ..., -1.1436, -0.3523, -0.3048],\n",
      "           [-1.0918,  0.0460,  0.0859,  ..., -0.4729,  0.1135, -2.2667],\n",
      "           ...,\n",
      "           [ 0.1165,  0.4329,  0.0715,  ...,  1.0827, -1.3568,  1.0506],\n",
      "           [ 1.0561,  0.3674, -1.0500,  ..., -0.4706, -0.1222, -1.6447],\n",
      "           [-0.7905, -0.3137, -1.7013,  ..., -1.2940, -2.3722, -0.1537]]],\n",
      "\n",
      "\n",
      "         [[[-0.6268, -0.8955,  0.0354,  ..., -0.1501, -1.5847,  0.9687],\n",
      "           [ 0.4276,  1.0290, -1.2494,  ...,  0.6808,  0.7836, -0.3112],\n",
      "           [ 0.6123,  0.6195,  0.7313,  ...,  0.6634, -1.0680, -1.3395],\n",
      "           ...,\n",
      "           [ 0.3147,  0.1730, -0.1908,  ..., -0.0804,  0.2440, -0.5304],\n",
      "           [ 0.7289, -0.5898,  0.3009,  ..., -0.6985,  0.8934,  0.2896],\n",
      "           [-0.9332,  0.1763, -0.1509,  ..., -0.5566, -1.6012, -0.8041]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1993,  0.4214, -1.9552,  ..., -0.2864, -1.5620, -0.4544],\n",
      "           [ 1.4674,  0.1515,  0.0240,  ...,  2.1674,  0.6913, -0.2852],\n",
      "           [-0.5786,  0.3348, -0.7314,  ..., -1.4185, -1.5105,  0.6905],\n",
      "           ...,\n",
      "           [ 1.1329,  0.1163,  0.7374,  ..., -0.0486, -0.0476,  0.9841],\n",
      "           [-0.8831, -0.2377, -0.6515,  ..., -1.3477, -1.1931,  1.4615],\n",
      "           [-1.0259, -0.0517, -1.2418,  ...,  1.4284, -0.1109,  0.3902]]]]]), tensor([]), tensor([806.8425]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-5.1782e-01,  2.2275e-01,  3.6313e-01,  ..., -2.0173e+00,\n",
      "            -1.0181e+00, -5.0354e-02],\n",
      "           [-7.7348e-01,  8.2062e-01,  1.1536e-01,  ..., -6.5423e-02,\n",
      "             7.4104e-01, -1.5061e-01],\n",
      "           [-3.3408e-01, -5.9863e-01, -5.9616e-01,  ..., -6.0389e-01,\n",
      "            -2.2876e+00,  9.9578e-01],\n",
      "           ...,\n",
      "           [ 3.3267e+00, -1.5957e+00, -8.7580e-01,  ...,  2.7017e-01,\n",
      "             1.4505e+00,  1.3456e-01],\n",
      "           [-5.6169e-01, -2.5524e-01,  7.2645e-02,  ..., -1.2782e+00,\n",
      "            -1.2863e+00, -1.1308e+00],\n",
      "           [-2.2835e-01, -1.2564e-01, -3.7370e-01,  ...,  5.2739e-01,\n",
      "            -7.8450e-01, -6.7929e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.5294e-01, -1.5407e+00, -1.5460e+00,  ..., -1.7096e+00,\n",
      "             9.8663e-01, -1.4117e+00],\n",
      "           [ 4.1860e-01,  1.4409e+00, -1.8386e+00,  ..., -1.6668e+00,\n",
      "            -6.9572e-01, -3.4689e-02],\n",
      "           [-1.9515e+00, -3.3758e+00, -1.2554e+00,  ...,  1.4142e-03,\n",
      "            -6.1581e-01,  1.3689e-01],\n",
      "           ...,\n",
      "           [ 2.0221e+00, -9.5203e-01,  2.6803e-01,  ...,  1.7652e-01,\n",
      "             7.1459e-01, -1.4751e-01],\n",
      "           [ 1.3886e-01, -5.6482e-02,  1.9534e-01,  ...,  1.4313e-01,\n",
      "            -5.3636e-01,  1.1771e+00],\n",
      "           [ 8.1494e-01, -1.5960e-01,  8.6568e-01,  ..., -3.6252e-01,\n",
      "             3.1807e-01, -2.9379e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.8254e-01,  1.1701e+00,  2.2837e+00,  ..., -1.0524e+00,\n",
      "             6.5393e-01,  4.7068e-01],\n",
      "           [ 1.0302e+00,  2.3773e-01,  8.7496e-01,  ..., -1.1309e+00,\n",
      "             1.9136e+00, -1.8617e-01],\n",
      "           [ 4.3446e-01, -4.7807e-01,  7.0520e-02,  ..., -2.1357e-01,\n",
      "             3.1561e-01,  5.1489e-02],\n",
      "           ...,\n",
      "           [-6.0462e-01,  1.0977e-01,  2.6140e-01,  ..., -1.3836e+00,\n",
      "            -1.1287e+00, -1.9509e+00],\n",
      "           [-6.4671e-01, -1.2046e+00,  4.8782e-01,  ..., -1.3465e+00,\n",
      "            -1.0027e+00, -2.1387e+00],\n",
      "           [-1.4916e+00,  1.6966e+00, -1.6353e+00,  ..., -1.0481e+00,\n",
      "             2.0767e-01,  3.5909e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.2359e+00, -3.8415e-01,  1.4238e+00,  ...,  1.6599e+00,\n",
      "            -2.1377e+00, -9.7203e-01],\n",
      "           [-1.0588e+00, -1.2016e+00, -3.0821e-01,  ..., -2.4167e+00,\n",
      "            -1.5598e+00, -1.1912e+00],\n",
      "           [-9.1513e-01,  8.2615e-01,  6.5463e-01,  ..., -2.9772e-01,\n",
      "             4.6620e-01, -2.4872e+00],\n",
      "           ...,\n",
      "           [ 7.6055e-01,  1.1820e+00,  5.7153e-01,  ...,  1.7443e+00,\n",
      "            -1.3613e+00,  2.0599e+00],\n",
      "           [ 1.8184e+00,  1.5623e+00, -1.2508e+00,  ..., -7.9231e-01,\n",
      "             2.8035e-01, -1.1999e+00],\n",
      "           [ 4.3453e-01,  6.4671e-01, -1.9281e+00,  ..., -1.4187e+00,\n",
      "            -2.5535e+00, -1.0464e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.4636e-01, -1.2282e+00,  1.7351e-01,  ...,  7.2136e-02,\n",
      "            -1.6871e+00,  1.5007e+00],\n",
      "           [ 6.3944e-01,  1.4805e+00, -1.3322e+00,  ...,  9.5594e-01,\n",
      "             1.2059e+00, -2.9247e-03],\n",
      "           [ 9.7799e-01,  9.9411e-01,  1.0780e+00,  ...,  6.5967e-01,\n",
      "            -1.4879e+00, -1.6969e+00],\n",
      "           ...,\n",
      "           [ 8.5832e-01,  5.3144e-01,  4.7208e-02,  ...,  5.8179e-01,\n",
      "             4.9322e-01, -3.1148e-01],\n",
      "           [ 9.9756e-01, -9.4230e-02,  4.7039e-01,  ..., -4.7531e-01,\n",
      "             1.3854e+00,  6.9675e-01],\n",
      "           [-5.6957e-01,  3.9819e-01,  1.1206e-01,  ..., -4.2608e-01,\n",
      "            -1.6062e+00, -1.0374e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 4.2699e-01,  5.9925e-01, -1.9765e+00,  ...,  8.2519e-03,\n",
      "            -1.5174e+00, -1.1558e-01],\n",
      "           [ 1.7452e+00,  4.9865e-01,  2.8615e-01,  ...,  2.8003e+00,\n",
      "             1.1373e+00, -3.6119e-02],\n",
      "           [-7.3210e-01,  4.0003e-01, -9.7638e-01,  ..., -1.6375e+00,\n",
      "            -1.8761e+00,  9.6566e-01],\n",
      "           ...,\n",
      "           [ 6.6699e-01,  8.3270e-02,  6.6540e-01,  ..., -1.0994e+00,\n",
      "            -1.4771e+00,  3.7603e-01],\n",
      "           [-2.0177e+00, -1.1191e+00, -1.2689e+00,  ..., -2.6959e+00,\n",
      "            -2.7033e+00,  8.9730e-01],\n",
      "           [-1.8502e+00, -8.7224e-01, -2.0548e+00,  ...,  9.7272e-01,\n",
      "            -1.0259e+00,  2.5400e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.7969, -0.3600,  0.2724,  ...,  0.3532,  0.2216, -0.4443],\n",
      "           [ 0.8929, -0.1254,  0.3765,  ...,  0.2893, -0.1075, -1.0763],\n",
      "           [-0.7411, -0.1568, -0.3178,  ...,  0.3237, -0.7969,  0.1346],\n",
      "           ...,\n",
      "           [-0.4985,  0.6974, -0.0958,  ..., -0.4364,  0.0277, -0.7220],\n",
      "           [ 0.2392,  0.3462, -0.9258,  ..., -0.0254,  0.7179, -0.2284],\n",
      "           [-0.3410, -0.2169, -0.3744,  ...,  0.5121, -1.2169, -0.5992]]],\n",
      "\n",
      "\n",
      "         [[[-0.3331,  0.1520, -0.1089,  ...,  0.7749,  0.0244,  0.1048],\n",
      "           [ 0.4753, -0.0615, -0.0712,  ...,  0.7112,  0.4324,  0.4887],\n",
      "           [-0.0020,  0.5263, -0.2604,  ..., -0.0918,  0.5371,  0.0253],\n",
      "           ...,\n",
      "           [ 0.0393, -0.0826,  0.4344,  ...,  0.5243,  0.5100, -0.5829],\n",
      "           [ 1.0214,  0.1874, -0.1329,  ..., -0.8360,  0.5341,  0.5527],\n",
      "           [ 0.6626,  0.0404, -0.0419,  ..., -0.5411, -0.3894, -0.5949]]],\n",
      "\n",
      "\n",
      "         [[[-1.2164, -1.2223, -0.8363,  ..., -0.7050, -1.7642, -1.2480],\n",
      "           [-0.3628, -0.3193, -0.2006,  ..., -0.1967,  0.3714, -0.7949],\n",
      "           [-0.8689, -0.0678, -0.6441,  ..., -1.0562, -1.0155,  0.4401],\n",
      "           ...,\n",
      "           [-0.6929,  0.0991, -0.0193,  ..., -0.1874, -0.2396,  0.6438],\n",
      "           [-0.0959, -0.2376,  0.1447,  ...,  0.4563,  0.1759,  0.9422],\n",
      "           [ 0.2366, -1.5733, -0.0203,  ..., -0.3902, -0.2731, -0.5578]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.7273, -0.6115, -1.3537,  ..., -0.7641,  0.8825,  0.2837],\n",
      "           [ 0.4542, -1.0899,  0.6339,  ...,  1.2384, -0.6197, -0.2749],\n",
      "           [ 0.3854, -0.8855, -0.4781,  ..., -0.2515,  0.2529, -0.0299],\n",
      "           ...,\n",
      "           [ 1.3658, -0.0541,  0.3982,  ..., -1.1333,  0.6590, -0.2924],\n",
      "           [-0.5619, -0.1597,  1.0162,  ...,  0.1812, -1.0672, -0.3556],\n",
      "           [ 0.1102, -0.2436,  0.5497,  ...,  0.2042, -0.8575, -0.4280]]],\n",
      "\n",
      "\n",
      "         [[[-0.4638, -0.6415,  0.7461,  ..., -1.1588, -0.0607, -0.6624],\n",
      "           [-0.3124,  0.2134,  0.9710,  ...,  0.0755, -0.7112,  0.2432],\n",
      "           [ 0.1009, -0.8999, -0.8275,  ...,  0.4780, -1.0257, -0.3073],\n",
      "           ...,\n",
      "           [ 0.0406, -0.4298, -0.3647,  ...,  0.2348, -0.6352,  0.0662],\n",
      "           [-0.8067, -0.8466,  0.2639,  ..., -0.2066,  0.2011, -0.3715],\n",
      "           [-0.1849, -0.4912,  0.1199,  ..., -0.0671, -0.1995,  0.0062]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7627,  0.5348,  0.1736,  ..., -0.4062,  0.8831,  0.1690],\n",
      "           [ 0.5326, -0.7123,  0.0178,  ...,  0.4336,  1.1676, -0.0986],\n",
      "           [-0.0931, -0.3392,  0.5653,  ...,  0.1855,  0.0691, -0.2358],\n",
      "           ...,\n",
      "           [ 0.5234, -0.0377, -0.2082,  ..., -0.1974, -0.6816,  1.0036],\n",
      "           [ 0.9515,  0.1727,  0.0658,  ..., -0.8363,  0.5167,  0.9690],\n",
      "           [ 0.4380,  1.1821, -0.0217,  ...,  1.2780, -0.2166,  0.8095]]]]]), tensor([]), tensor([549.1276]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.4316, -1.5758, -0.5497,  ..., -0.4775, -0.6385, -1.7394],\n",
      "           [ 0.7295, -0.9565, -0.0200,  ...,  0.2178, -0.4731, -2.4467],\n",
      "           [-2.0479, -1.1424, -1.2526,  ...,  0.0039, -1.9947, -0.3827],\n",
      "           ...,\n",
      "           [-0.3865,  0.8462, -0.2377,  ..., -0.1826,  1.2097, -0.2418],\n",
      "           [ 0.6079,  0.9256, -1.6916,  ...,  0.8332,  2.1213,  0.4398],\n",
      "           [-0.6697, -0.2178, -0.5436,  ...,  1.7678, -1.5014, -0.4583]]],\n",
      "\n",
      "\n",
      "         [[[-1.8570, -0.8852, -1.0721,  ...,  0.1791, -1.0966, -0.7995],\n",
      "           [-0.0683, -0.9687, -1.3279,  ...,  0.1804, -0.0744,  0.2313],\n",
      "           [-1.2531, -0.7795, -2.6086,  ..., -1.5144, -0.3785, -0.7755],\n",
      "           ...,\n",
      "           [ 0.3665, -0.2536,  0.6840,  ...,  1.1594,  0.5951, -0.2635],\n",
      "           [ 1.3324,  1.2993, -0.6187,  ..., -1.1377,  0.7456,  1.4048],\n",
      "           [ 1.6882,  0.2586, -0.4528,  ..., -1.3490, -1.3080, -1.4419]]],\n",
      "\n",
      "\n",
      "         [[[-0.8003, -1.0361,  0.3143,  ...,  0.5847, -1.2927, -0.5653],\n",
      "           [ 0.4342,  1.3890,  1.6591,  ...,  1.5733,  2.5898,  0.4389],\n",
      "           [-0.1094,  1.7261,  0.6520,  ..., -0.3383, -0.2659,  2.1822],\n",
      "           ...,\n",
      "           [-1.4983,  0.5855,  0.6256,  ..., -0.0454, -1.3786, -0.0092],\n",
      "           [-0.6188, -0.2559,  0.1974,  ...,  0.7033, -0.9071,  0.4714],\n",
      "           [ 0.7954, -2.4048,  0.3784,  ..., -1.6582, -1.0607, -1.2420]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.9946, -0.8888, -1.5810,  ..., -0.8392,  1.5563,  0.4923],\n",
      "           [-0.7934, -2.8748,  0.0998,  ...,  0.7852, -2.5733, -1.6282],\n",
      "           [ 1.1789, -0.5947,  0.0651,  ..., -0.2146,  0.9325,  0.1463],\n",
      "           ...,\n",
      "           [ 3.2841,  0.8828,  1.4651,  ..., -1.8506,  2.2261,  0.9273],\n",
      "           [-0.4312,  1.2129,  1.9491,  ...,  0.2494, -0.9061,  0.5596],\n",
      "           [ 2.0293,  1.1440,  1.1886,  ...,  0.8845, -0.7835,  0.2933]]],\n",
      "\n",
      "\n",
      "         [[[-0.6343, -1.2510,  2.0206,  ..., -1.6708,  0.4740, -0.7669],\n",
      "           [-0.3365,  0.7123,  1.8943,  ...,  0.2849, -0.9497,  0.6797],\n",
      "           [ 0.5233, -1.3850, -1.3343,  ...,  0.7008, -2.0033, -0.3703],\n",
      "           ...,\n",
      "           [ 0.7194, -0.3993, -0.3045,  ...,  1.4830, -1.1413,  0.4947],\n",
      "           [-1.3902, -0.6552,  0.5469,  ...,  0.3422,  0.3037, -0.3971],\n",
      "           [ 0.5002, -0.6787,  0.6635,  ...,  0.0416, -0.2743,  0.2801]]],\n",
      "\n",
      "\n",
      "         [[[ 1.6712,  1.1243,  0.9819,  ..., -0.0726,  2.5331,  1.0971],\n",
      "           [ 0.8806, -0.6251,  0.5892,  ...,  1.0181,  2.6583,  0.2683],\n",
      "           [-0.4288, -0.6933,  1.1278,  ...,  0.2680, -0.0238, -0.2958],\n",
      "           ...,\n",
      "           [-0.0121, -0.2378, -0.8053,  ..., -0.5085, -3.2370,  0.4531],\n",
      "           [ 0.4651, -0.8518, -0.5112,  ..., -2.4408, -0.8277,  0.1293],\n",
      "           [-0.0496,  1.0227, -0.7558,  ...,  0.9794, -2.0482,  0.3745]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 6.2891e-01,  5.8114e-01,  5.8248e-01,  ...,  4.5949e-01,\n",
      "             2.4614e-01,  6.6935e-01],\n",
      "           [ 2.4599e-01,  3.0125e-01,  2.5772e-01,  ...,  4.5267e-02,\n",
      "             2.6637e-03,  7.4757e-02],\n",
      "           [ 5.8731e-01,  3.7804e-01,  6.6117e-02,  ...,  2.8563e-01,\n",
      "             2.7371e-01,  4.5478e-01],\n",
      "           ...,\n",
      "           [-1.8219e-01,  2.6924e-01,  1.7185e-01,  ..., -6.0275e-01,\n",
      "            -6.2652e-01, -4.3508e-01],\n",
      "           [-2.5948e-01, -8.9749e-03, -3.0316e-01,  ..., -2.4187e-01,\n",
      "            -3.4619e-01, -4.0388e-01],\n",
      "           [ 6.1114e-02, -7.8000e-02, -9.0832e-02,  ..., -4.6476e-01,\n",
      "            -3.7748e-01, -4.1190e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.2359e-01,  4.2430e-01,  5.0366e-01,  ...,  7.2265e-01,\n",
      "             6.1780e-01,  4.1659e-01],\n",
      "           [ 4.9570e-01,  3.9731e-01,  6.0432e-01,  ...,  3.9335e-01,\n",
      "             8.3686e-01,  4.0028e-01],\n",
      "           [ 9.3443e-01,  7.9615e-01,  7.7047e-01,  ...,  4.6903e-01,\n",
      "             5.0101e-01,  4.7818e-01],\n",
      "           ...,\n",
      "           [-3.0699e-01,  6.8654e-02,  1.7901e-02,  ..., -1.4335e-01,\n",
      "            -2.2552e-01, -1.0859e-01],\n",
      "           [ 1.7201e-01, -5.2375e-01,  1.0400e-01,  ...,  2.5318e-01,\n",
      "            -1.7484e-02,  7.1023e-02],\n",
      "           [-1.3931e-01, -4.0112e-04,  1.8488e-01,  ..., -1.4914e-01,\n",
      "            -8.4161e-02,  5.2271e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.3470e-01, -5.3127e-01, -1.1176e+00,  ..., -1.0166e+00,\n",
      "            -9.2509e-01, -9.1970e-01],\n",
      "           [-5.8768e-01, -9.8124e-01, -9.5198e-01,  ..., -9.0426e-01,\n",
      "            -1.0439e+00, -9.0103e-01],\n",
      "           [-8.6513e-01, -7.1019e-01, -8.5021e-01,  ..., -6.8675e-01,\n",
      "            -7.0063e-01, -6.9616e-01],\n",
      "           ...,\n",
      "           [ 8.5667e-02, -2.0555e-01, -3.1261e-01,  ..., -9.7252e-02,\n",
      "             3.1354e-01,  6.4085e-01],\n",
      "           [ 2.0228e-01, -1.3670e-01,  1.0537e-01,  ...,  1.0905e-01,\n",
      "             6.8736e-01,  3.8649e-01],\n",
      "           [-9.9045e-02, -3.3045e-01, -2.8676e-01,  ...,  6.6886e-01,\n",
      "            -1.7083e-02,  3.4040e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.3181e-03, -6.9308e-02, -4.0932e-01,  ...,  8.5768e-02,\n",
      "             9.4171e-02,  4.7827e-01],\n",
      "           [ 7.6066e-01,  1.4983e-01,  2.6196e-01,  ...,  7.4261e-01,\n",
      "             8.8923e-01,  4.7615e-01],\n",
      "           [-4.5986e-01, -6.6387e-01, -6.1203e-01,  ..., -4.8010e-02,\n",
      "            -3.7235e-01, -1.3509e-01],\n",
      "           ...,\n",
      "           [-4.3137e-01, -3.1792e-01, -2.2861e-01,  ...,  2.7395e-01,\n",
      "            -2.6771e-01, -6.9305e-01],\n",
      "           [-4.0015e-01, -8.8594e-01,  4.2622e-02,  ...,  2.5008e-01,\n",
      "            -4.6033e-02, -2.6638e-01],\n",
      "           [-8.2007e-01, -9.3108e-01, -8.7538e-02,  ..., -1.2232e-01,\n",
      "            -2.7191e-01,  3.4401e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.2156e-01, -5.2907e-02, -2.7751e-01,  ...,  8.7008e-02,\n",
      "            -3.1729e-01, -6.4792e-02],\n",
      "           [-1.7328e-01, -1.5488e-01, -6.0987e-02,  ...,  1.8851e-01,\n",
      "            -2.8914e-01, -3.2698e-01],\n",
      "           [-7.5559e-02, -6.2777e-02, -1.7120e-01,  ...,  1.1552e-01,\n",
      "             1.9247e-02, -1.0034e-01],\n",
      "           ...,\n",
      "           [-2.4193e-01,  3.7454e-02, -2.7321e-01,  ..., -5.0957e-01,\n",
      "            -7.7438e-02, -2.6082e-01],\n",
      "           [-1.0425e-01, -4.5423e-01,  1.0723e-01,  ..., -2.0730e-01,\n",
      "             7.0426e-02, -6.3604e-02],\n",
      "           [-5.4906e-01, -3.9880e-01,  9.0086e-02,  ..., -1.9845e-01,\n",
      "            -3.1388e-01,  3.4132e-01]]],\n",
      "\n",
      "\n",
      "         [[[-5.1920e-02, -1.8895e-01, -3.3023e-01,  ..., -1.9702e-01,\n",
      "            -1.4153e-01, -4.4790e-01],\n",
      "           [ 1.6135e-01, -3.4266e-01, -1.8606e-01,  ..., -1.5596e-01,\n",
      "            -3.0696e-01, -2.4355e-01],\n",
      "           [ 1.7613e-01,  3.0694e-02,  4.1979e-01,  ..., -1.7291e-01,\n",
      "             1.1675e-01, -8.9920e-02],\n",
      "           ...,\n",
      "           [ 7.6573e-01,  2.7099e-01,  1.1164e-01,  ..., -1.0212e-01,\n",
      "             9.1324e-01,  5.5301e-01],\n",
      "           [ 7.4131e-01,  4.9510e-01,  2.7050e-01,  ...,  4.9839e-01,\n",
      "             7.8040e-01,  7.9458e-01],\n",
      "           [ 5.5447e-01,  5.9174e-01,  1.6822e-01,  ...,  4.3939e-01,\n",
      "             4.6137e-01,  3.9322e-01]]]]]), tensor([]), tensor([127.5088]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.7895,  0.7666, -0.3869,  ..., -1.0785, -2.9114,  0.7464],\n",
      "           [-1.6711,  0.0656, -0.2348,  ..., -0.3858, -0.9950, -1.2449],\n",
      "           [ 1.5815, -0.2161, -2.4532,  ..., -0.4388, -0.1313,  1.0737],\n",
      "           ...,\n",
      "           [ 0.7703, -0.2746,  1.4120,  ..., -2.0303, -0.0883,  0.1813],\n",
      "           [-1.1886,  0.7549, -2.3555,  ...,  1.4419,  1.6326,  1.8871],\n",
      "           [ 0.4374, -0.0749, -0.2057,  ..., -0.6236,  0.6151, -0.2860]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1231, -1.9194, -0.6805,  ..., -0.4681, -0.2961, -0.7319],\n",
      "           [-0.4421, -0.7693, -0.3313,  ..., -1.6836,  2.0252, -0.2657],\n",
      "           [ 0.1868, -1.9363, -0.8406,  ..., -1.5389, -0.7658,  0.5628],\n",
      "           ...,\n",
      "           [ 0.5524,  0.5263,  0.7620,  ..., -0.1399, -2.0712,  0.4246],\n",
      "           [-0.3169,  0.5888, -0.9192,  ...,  0.2489, -0.3496,  0.2516],\n",
      "           [ 0.9011,  0.5162, -0.1158,  ..., -1.4297, -1.5134,  0.2180]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1225,  0.8515, -0.4024,  ..., -0.2023,  1.3221,  0.4835],\n",
      "           [ 0.1075,  0.8888,  0.2468,  ...,  1.2856,  0.6646,  0.7115],\n",
      "           [-0.4763,  2.2995,  1.6056,  ...,  0.9207,  1.3392,  0.2771],\n",
      "           ...,\n",
      "           [-0.3264,  0.1002,  0.3398,  ..., -0.7065, -1.8861, -0.0198],\n",
      "           [-0.7146, -0.2489,  0.7318,  ...,  0.0316,  0.9845,  0.1539],\n",
      "           [ 0.5053, -0.5913, -0.4230,  ..., -1.0157, -0.4917, -0.5218]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.6707, -0.4728, -1.0779,  ...,  0.1019,  0.0675,  1.2117],\n",
      "           [-0.0628, -1.6873, -0.6465,  ...,  0.4673,  1.0472, -0.1002],\n",
      "           [ 0.4080,  0.5622, -1.1687,  ...,  0.5892, -0.6672,  0.6226],\n",
      "           ...,\n",
      "           [ 0.4617,  0.9812,  1.2893,  ...,  3.1719, -0.1022,  0.8343],\n",
      "           [ 0.1928, -0.3039,  0.8856,  ...,  0.3489, -0.3001,  1.4313],\n",
      "           [ 2.1395, -0.4168,  0.6264,  ...,  0.1024,  0.2064,  2.3242]]],\n",
      "\n",
      "\n",
      "         [[[-0.9454, -1.0874, -0.5157,  ...,  1.8393, -0.6056,  1.5331],\n",
      "           [-0.6689,  0.0723,  0.5644,  ...,  1.5244, -1.2695, -0.7544],\n",
      "           [ 0.5800,  0.2058, -0.4253,  ...,  0.3270, -0.6598, -0.3306],\n",
      "           ...,\n",
      "           [ 1.1074,  2.1056, -0.4545,  ..., -0.0423,  0.4532,  0.2736],\n",
      "           [-0.2544,  0.4949,  1.4463,  ...,  0.3841, -0.4323,  0.9652],\n",
      "           [-0.8659, -2.1421,  2.5262,  ...,  0.3833, -0.8289,  1.1896]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8053, -0.9569, -0.0606,  ...,  1.3716,  1.8207, -0.9648],\n",
      "           [ 0.8545, -0.3298,  0.7891,  ...,  0.1183, -0.6577,  0.0924],\n",
      "           [-0.3016,  0.0446,  2.5755,  ..., -0.4627,  0.8161,  0.4376],\n",
      "           ...,\n",
      "           [ 1.1337,  1.6842, -0.5865,  ..., -1.1859, -0.4012, -1.7170],\n",
      "           [-0.0799, -1.3140, -0.8471,  ...,  1.3517, -0.4114, -0.6161],\n",
      "           [ 0.6409, -0.4457, -1.9918,  ..., -1.4361, -1.5639, -0.6218]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-8.0692e-03, -5.2901e-01,  3.9447e-01,  ...,  2.5212e-01,\n",
      "            -7.5790e-01, -6.9977e-01],\n",
      "           [ 5.1900e-01,  3.1263e-01, -5.8769e-01,  ..., -8.5980e-02,\n",
      "             4.5892e-01,  6.7532e-02],\n",
      "           [-9.8302e-02,  6.3970e-02, -1.4907e-01,  ...,  5.2186e-01,\n",
      "            -1.5388e-01,  5.7328e-01],\n",
      "           ...,\n",
      "           [ 4.3861e-02, -3.1545e-01,  2.6003e-01,  ..., -6.6924e-01,\n",
      "            -1.9939e-01, -3.6627e-01],\n",
      "           [-1.0886e-01, -2.8401e-01,  6.1393e-01,  ..., -8.8237e-01,\n",
      "            -2.2054e-01, -4.4074e-01],\n",
      "           [ 1.9487e-01,  4.5916e-01, -8.3357e-02,  ..., -1.6654e-02,\n",
      "            -6.7838e-01, -2.8193e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.3240e-01,  3.1871e-02,  3.0422e-01,  ...,  4.7738e-01,\n",
      "             3.8540e-01,  1.1534e+00],\n",
      "           [-5.5835e-02,  7.1111e-01,  8.3949e-01,  ..., -2.9602e-01,\n",
      "            -4.3652e-02,  1.3815e-01],\n",
      "           [ 5.7243e-01,  7.6838e-01,  1.2448e+00,  ..., -2.1081e-01,\n",
      "            -3.2708e-01, -7.8259e-01],\n",
      "           ...,\n",
      "           [-1.6691e-01,  2.2568e-01,  1.0861e-01,  ..., -4.3886e-01,\n",
      "            -3.7238e-01, -7.6398e-01],\n",
      "           [ 2.8044e-01, -2.2049e-02,  5.2344e-01,  ...,  5.3696e-01,\n",
      "            -1.4210e-01, -7.0972e-01],\n",
      "           [-1.6412e-02, -5.6021e-01,  4.2195e-01,  ..., -4.0691e-01,\n",
      "             8.8234e-01,  1.7400e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.1323e-01, -2.3073e-01, -8.3953e-01,  ..., -5.9499e-01,\n",
      "            -1.5995e+00, -3.3721e-01],\n",
      "           [-2.7141e-01, -1.3458e-01,  3.6278e-01,  ..., -8.4764e-01,\n",
      "            -1.0256e-01,  4.8633e-01],\n",
      "           [-7.4269e-01, -8.0932e-01, -4.1096e-01,  ..., -9.5411e-01,\n",
      "            -5.1233e-02, -2.4522e-01],\n",
      "           ...,\n",
      "           [ 4.4621e-02, -2.0229e-01, -5.5323e-02,  ...,  3.5118e-01,\n",
      "             6.4219e-01,  5.2863e-01],\n",
      "           [ 7.8143e-01,  7.9645e-02, -1.4869e-01,  ..., -1.0661e-01,\n",
      "            -1.3140e-01, -1.1097e-01],\n",
      "           [ 1.2368e-01, -4.4254e-01, -9.5833e-02,  ..., -4.6131e-03,\n",
      "             1.5832e-01, -2.1658e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.6661e-01,  1.4857e-01, -6.8209e-02,  ...,  4.7267e-01,\n",
      "            -2.3802e-01,  2.3572e-02],\n",
      "           [ 9.7865e-01,  7.9774e-01,  3.2206e-01,  ..., -1.7799e-01,\n",
      "             1.4613e+00,  5.0700e-01],\n",
      "           [-4.3356e-01, -1.0576e+00, -1.1811e-01,  ...,  8.0492e-01,\n",
      "             8.8849e-01, -4.4193e-01],\n",
      "           ...,\n",
      "           [-9.8223e-01, -2.4155e-01, -3.8060e-01,  ..., -1.1881e+00,\n",
      "             1.1347e-01, -5.5690e-01],\n",
      "           [ 3.8247e-02, -4.7253e-01,  4.5456e-01,  ...,  2.4361e-03,\n",
      "             7.4626e-01, -3.2235e-01],\n",
      "           [-4.5088e-01, -6.0641e-01, -9.1057e-02,  ..., -9.6823e-01,\n",
      "            -2.8804e-01,  8.2411e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.5477e-03,  7.1909e-01,  3.3361e-01,  ..., -4.0128e-01,\n",
      "            -9.7629e-03, -2.0273e-01],\n",
      "           [ 4.8234e-02, -7.7914e-01, -2.7804e-01,  ...,  2.5124e-01,\n",
      "            -8.9120e-01,  2.5198e-01],\n",
      "           [ 3.6259e-01, -7.6452e-01,  4.1608e-01,  ..., -1.7913e-01,\n",
      "            -4.5032e-02,  3.0109e-01],\n",
      "           ...,\n",
      "           [-7.7849e-01, -1.6832e-01, -5.8060e-01,  ..., -1.0137e-01,\n",
      "             2.1473e-01, -1.1979e-03],\n",
      "           [ 1.2157e+00, -1.1600e-01,  2.7948e-01,  ..., -1.5352e-01,\n",
      "            -1.1812e-01, -3.8342e-01],\n",
      "           [-5.8840e-01, -1.3559e+00,  7.7778e-01,  ..., -4.5796e-01,\n",
      "             2.7065e-01, -4.6282e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 4.3194e-02, -2.3683e-01, -1.4458e-01,  ...,  2.9601e-01,\n",
      "            -2.3845e-01,  2.3067e-02],\n",
      "           [ 4.7574e-01, -8.7148e-01,  1.3911e-02,  ...,  4.2465e-02,\n",
      "            -1.4207e-01, -1.3541e-01],\n",
      "           [ 1.4589e-01, -3.5153e-02,  5.2084e-01,  ...,  2.3754e-01,\n",
      "            -4.6392e-01,  7.1775e-01],\n",
      "           ...,\n",
      "           [ 2.1785e-01,  2.2701e-01, -6.5323e-01,  ...,  2.0349e-02,\n",
      "             9.3360e-02,  6.5567e-01],\n",
      "           [ 3.9356e-01, -6.3874e-02,  5.0152e-01,  ...,  1.0116e+00,\n",
      "             3.2474e-01,  3.9930e-01],\n",
      "           [-8.5984e-03, -3.0250e-01, -1.1810e-01,  ...,  8.6601e-01,\n",
      "             5.2807e-01, -2.2018e-01]]]]]), tensor([]), tensor([451.1358]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.1482e+00, -2.3099e+00, -4.7075e-01,  ..., -7.0225e-01,\n",
      "            -3.0226e+00, -2.7565e+00],\n",
      "           [ 1.9499e-01, -9.2339e-03, -2.1105e+00,  ..., -6.0817e-01,\n",
      "             7.0046e-01, -4.6381e-01],\n",
      "           [-1.1505e+00, -7.0325e-01, -1.1517e+00,  ...,  3.6582e-01,\n",
      "            -9.2431e-01,  5.5997e-01],\n",
      "           ...,\n",
      "           [ 7.2611e-01, -1.2917e+00,  5.4341e-01,  ..., -4.9229e-01,\n",
      "             7.4809e-01,  1.2380e-01],\n",
      "           [ 8.0332e-02, -1.9972e-01,  1.3727e+00,  ..., -7.7970e-01,\n",
      "             7.6041e-01,  2.5246e-01],\n",
      "           [ 3.8947e-01,  1.1302e+00, -5.6479e-02,  ...,  1.0326e+00,\n",
      "            -4.7014e-01,  1.1888e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.4147e-01, -1.4686e+00, -7.2314e-01,  ..., -5.5194e-01,\n",
      "            -3.8003e-01,  1.3425e+00],\n",
      "           [-1.2177e+00,  5.3485e-01,  4.4750e-01,  ..., -2.0830e+00,\n",
      "            -1.2264e+00, -8.1070e-01],\n",
      "           [-4.9408e-01, -4.2053e-01,  1.6004e-01,  ..., -1.8794e+00,\n",
      "            -2.0130e+00, -2.7039e+00],\n",
      "           ...,\n",
      "           [ 2.9572e-01,  5.6433e-01,  4.4544e-01,  ..., -4.8524e-02,\n",
      "            -1.0027e+00, -4.9672e-01],\n",
      "           [ 2.0785e-01,  1.2233e+00,  6.0019e-01,  ...,  1.1531e+00,\n",
      "            -5.8009e-01, -1.7947e+00],\n",
      "           [ 4.5820e-01, -9.6251e-01,  4.9645e-01,  ..., -1.4487e+00,\n",
      "             1.7409e+00, -1.2987e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3471e+00,  8.9529e-01,  4.5432e-01,  ...,  6.2032e-01,\n",
      "            -1.2050e+00,  1.1904e+00],\n",
      "           [ 7.1238e-01,  1.9615e+00,  3.1743e+00,  ...,  1.7314e-01,\n",
      "             2.0358e+00,  3.0588e+00],\n",
      "           [ 2.8115e-01,  3.3900e-01,  1.3170e+00,  ..., -2.5160e-01,\n",
      "             1.8980e+00,  1.1266e+00],\n",
      "           ...,\n",
      "           [-1.9473e-01,  7.3509e-02,  6.7961e-01,  ...,  1.0402e+00,\n",
      "             6.0538e-02, -4.5725e-01],\n",
      "           [ 1.1430e+00,  4.1313e-01, -3.9195e-01,  ..., -3.7003e-01,\n",
      "            -6.1367e-01, -1.3501e+00],\n",
      "           [ 6.6126e-01, -3.7905e-01,  3.7963e-01,  ..., -1.0540e+00,\n",
      "             2.1707e-01, -3.5782e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.0731e+00,  2.6966e-01,  6.3170e-01,  ...,  1.3551e+00,\n",
      "            -7.2347e-01, -2.3172e-01],\n",
      "           [ 3.9750e-01,  1.0657e+00, -1.6912e-01,  ..., -2.0509e+00,\n",
      "             1.5799e+00,  2.3069e-03],\n",
      "           [-2.3657e-01, -1.1540e+00,  4.6271e-01,  ...,  2.1356e+00,\n",
      "             2.5560e+00, -7.7092e-01],\n",
      "           ...,\n",
      "           [-1.2294e+00,  4.4603e-01, -4.3242e-02,  ..., -2.5403e+00,\n",
      "             9.9219e-02, -6.5360e-01],\n",
      "           [ 8.5404e-01,  7.7581e-01,  1.2056e+00,  ...,  3.1928e-01,\n",
      "             1.7883e+00, -4.9910e-01],\n",
      "           [ 1.2959e+00,  6.2366e-01,  2.0546e-02,  ..., -1.4201e+00,\n",
      "             3.0325e-02,  8.7687e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3694e-01,  1.4106e+00,  1.0952e+00,  ..., -5.7031e-01,\n",
      "             5.8741e-01,  7.1681e-02],\n",
      "           [ 4.9007e-01, -1.2206e+00, -2.3142e-01,  ...,  7.9540e-01,\n",
      "            -1.3546e+00,  1.0594e+00],\n",
      "           [ 1.1641e+00, -1.2634e+00,  1.1897e+00,  ..., -5.9928e-01,\n",
      "            -1.9446e-01,  6.1133e-01],\n",
      "           ...,\n",
      "           [-8.8332e-01,  7.8627e-02, -7.5564e-01,  ...,  6.1851e-01,\n",
      "             5.5004e-01,  2.4697e-01],\n",
      "           [ 2.8075e+00,  8.2372e-01,  7.2540e-01,  ...,  1.0031e-01,\n",
      "             2.6438e-01, -5.8227e-01],\n",
      "           [-3.9664e-01, -2.7211e+00,  2.2539e+00,  ..., -1.0048e+00,\n",
      "             1.1035e+00, -6.3534e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 4.1249e-01, -5.0615e-01,  4.5172e-01,  ...,  1.2526e+00,\n",
      "             3.1031e-01,  8.6660e-01],\n",
      "           [ 9.6797e-01, -1.2489e+00,  6.4099e-01,  ...,  3.8600e-01,\n",
      "             1.6523e-01,  2.1633e-01],\n",
      "           [ 3.5715e-02, -2.7040e-01,  1.0569e+00,  ...,  8.1377e-01,\n",
      "            -6.7660e-01,  1.7494e+00],\n",
      "           ...,\n",
      "           [-8.1054e-01,  3.3855e-01, -1.8554e+00,  ..., -3.9815e-01,\n",
      "            -1.9435e+00, -5.0074e-02],\n",
      "           [-8.4297e-01, -1.5766e+00,  2.6197e-01,  ...,  1.3266e+00,\n",
      "            -9.0477e-01, -1.1063e+00],\n",
      "           [-1.1208e+00, -2.1717e+00, -1.2145e+00,  ...,  5.3409e-01,\n",
      "            -6.1090e-02, -6.1858e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 7.3527e-01,  3.3506e-01,  4.4887e-01,  ...,  4.2315e-01,\n",
      "             5.8193e-01,  2.3031e-01],\n",
      "           [ 2.4599e-01,  3.2587e-01,  4.8334e-02,  ..., -2.3328e-01,\n",
      "             1.5873e-02, -1.4194e-01],\n",
      "           [ 3.9036e-01,  4.1776e-01,  7.7958e-01,  ...,  1.5663e-01,\n",
      "             4.9382e-01,  2.7108e-01],\n",
      "           ...,\n",
      "           [-3.4940e-01,  3.2878e-01,  3.9919e-03,  ..., -2.3054e-01,\n",
      "            -4.1435e-01, -1.7648e-01],\n",
      "           [-2.2792e-01, -1.3749e-01, -2.5864e-01,  ..., -2.9658e-01,\n",
      "            -3.2710e-01, -3.8693e-01],\n",
      "           [-4.0831e-01, -2.5035e-01, -1.6493e-01,  ..., -2.3362e-01,\n",
      "             1.8996e-02, -9.6614e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 6.6683e-01,  4.5454e-01,  1.2717e-01,  ...,  6.3392e-01,\n",
      "             6.1504e-01,  6.6975e-01],\n",
      "           [ 6.7577e-01,  7.9359e-01,  5.6480e-01,  ...,  1.2607e+00,\n",
      "             4.3133e-01,  4.1404e-01],\n",
      "           [ 6.1755e-01,  9.4429e-01,  4.8268e-01,  ...,  4.2506e-01,\n",
      "             3.7543e-01,  6.2548e-01],\n",
      "           ...,\n",
      "           [-2.2767e-01,  1.2733e-01, -5.8647e-01,  ...,  2.4187e-01,\n",
      "            -1.2054e-01, -5.7545e-01],\n",
      "           [-4.5515e-01, -2.6630e-01,  1.3504e-01,  ...,  1.1021e-01,\n",
      "             2.1339e-01, -1.6254e-01],\n",
      "           [ 2.3728e-03, -1.2709e-01, -5.6182e-02,  ...,  3.6853e-01,\n",
      "            -1.3792e-01,  3.8018e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.0471e+00, -5.7796e-01, -1.2901e+00,  ..., -1.0231e+00,\n",
      "            -6.8981e-01, -6.8294e-01],\n",
      "           [-3.8696e-01, -7.5902e-01, -8.8319e-01,  ..., -1.0030e+00,\n",
      "            -7.6892e-01, -5.7103e-01],\n",
      "           [-5.2665e-01, -7.6513e-01, -5.7256e-01,  ..., -7.1944e-01,\n",
      "            -5.1099e-01, -2.1918e-01],\n",
      "           ...,\n",
      "           [ 2.7630e-01, -5.0468e-01, -2.8853e-01,  ...,  2.5693e-01,\n",
      "             3.0005e-01,  6.8547e-01],\n",
      "           [ 4.2212e-01, -5.2456e-02, -2.5845e-01,  ...,  1.8982e-01,\n",
      "             4.3964e-01,  1.3932e-01],\n",
      "           [-4.1644e-01, -6.3394e-02, -1.4903e-01,  ...,  4.8446e-03,\n",
      "            -1.1624e-01,  7.7344e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.1604e-01,  6.5212e-01, -2.2191e-02,  ..., -4.3074e-01,\n",
      "            -2.3680e-02, -3.2610e-02],\n",
      "           [ 2.4080e-01,  1.5519e-01,  4.4472e-01,  ...,  4.3398e-01,\n",
      "             5.4624e-01,  3.2083e-01],\n",
      "           [-1.0050e+00, -3.7857e-01, -1.5226e-01,  ...,  2.3912e-02,\n",
      "            -3.5788e-01, -1.8294e-01],\n",
      "           ...,\n",
      "           [-1.9341e-01, -3.4241e-01, -2.0094e-01,  ..., -5.7389e-01,\n",
      "            -1.0897e+00, -5.3650e-01],\n",
      "           [-2.7680e-01, -5.7806e-01,  1.8408e-01,  ..., -4.1729e-01,\n",
      "            -6.8661e-02,  3.1197e-01],\n",
      "           [-8.3726e-01, -6.9304e-01, -3.1586e-01,  ..., -3.1301e-01,\n",
      "            -2.5712e-02, -2.5626e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 9.7053e-02, -2.4851e-01, -2.4580e-01,  ..., -9.1190e-02,\n",
      "            -5.2175e-01,  1.5960e-01],\n",
      "           [ 3.7257e-01,  3.6992e-01, -2.6092e-01,  ..., -8.7991e-02,\n",
      "            -1.9278e-02, -5.5396e-01],\n",
      "           [-2.6347e-01, -1.6025e-01, -1.6708e-01,  ..., -3.3596e-01,\n",
      "             1.8485e-01, -5.8068e-01],\n",
      "           ...,\n",
      "           [-1.2676e-01, -5.1860e-01,  7.6758e-02,  ..., -2.9993e-01,\n",
      "            -4.5740e-01, -3.5739e-01],\n",
      "           [-7.1686e-03, -6.9742e-01,  1.1754e-01,  ..., -3.8647e-01,\n",
      "            -4.0891e-01, -3.7282e-01],\n",
      "           [-1.9059e-01,  7.6011e-02,  9.2562e-02,  ...,  7.6083e-02,\n",
      "            -1.8832e-01, -1.1638e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.8463e-02, -3.3070e-01, -2.3497e-01,  ..., -7.3726e-01,\n",
      "             2.3629e-02,  2.3385e-01],\n",
      "           [ 1.4752e-01, -4.1852e-01, -1.9873e-01,  ...,  2.5800e-01,\n",
      "            -1.7885e-01, -3.2609e-01],\n",
      "           [ 2.8929e-02, -1.9936e-02,  3.3962e-01,  ..., -8.3818e-02,\n",
      "             3.7123e-01, -9.4785e-04],\n",
      "           ...,\n",
      "           [ 4.1837e-01, -5.1518e-02,  2.1170e-01,  ...,  5.8040e-01,\n",
      "             1.0016e+00,  9.7948e-01],\n",
      "           [ 4.9386e-01,  7.8358e-01,  1.6263e-03,  ...,  8.6547e-01,\n",
      "             7.0897e-01,  3.6530e-01],\n",
      "           [ 3.4528e-01, -6.7729e-02,  3.8496e-01,  ...,  8.6554e-01,\n",
      "             4.8699e-01,  1.4586e-01]]]]]), tensor([]), tensor([214.9563]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.1829e+00, -7.1811e-01, -3.9275e-01,  ..., -7.8261e-01,\n",
      "             1.5856e-01, -1.4388e+00],\n",
      "           [-9.5510e-01, -1.4502e-01, -1.2877e+00,  ..., -2.0225e+00,\n",
      "            -8.8200e-01, -2.0726e+00],\n",
      "           [-4.5636e-01, -2.6117e-01,  1.6182e+00,  ..., -5.0464e-01,\n",
      "             6.4880e-01, -2.7999e-01],\n",
      "           ...,\n",
      "           [-4.0634e-01,  3.1954e-01,  1.1214e-01,  ...,  1.3470e+00,\n",
      "             6.9345e-01,  1.4934e+00],\n",
      "           [-5.5620e-01,  2.7941e-02, -1.1722e+00,  ...,  1.1595e+00,\n",
      "             1.1754e+00,  4.5530e-01],\n",
      "           [-2.1050e+00, -8.5741e-01, -5.5027e-01,  ...,  1.7455e+00,\n",
      "             1.7601e+00,  1.2213e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 8.1009e-02, -1.2349e+00, -1.7969e+00,  ..., -3.8119e-01,\n",
      "            -6.5395e-02,  2.8972e-01],\n",
      "           [ 5.1935e-01,  9.2743e-01, -6.9860e-01,  ...,  2.6439e+00,\n",
      "            -6.4055e-01, -3.4373e-01],\n",
      "           [-9.3870e-01, -2.4184e-01, -2.1758e+00,  ..., -1.2735e+00,\n",
      "            -1.4065e+00,  5.9362e-01],\n",
      "           ...,\n",
      "           [-5.3517e-02,  5.0441e-01, -2.3519e+00,  ...,  2.0241e-01,\n",
      "             5.4220e-01, -3.9086e-01],\n",
      "           [-3.2093e+00,  1.6254e+00, -4.8663e-02,  ..., -5.7082e-01,\n",
      "             4.9435e-01, -1.5751e+00],\n",
      "           [ 1.0935e+00,  1.8853e-01, -1.2628e+00,  ...,  6.5627e-01,\n",
      "            -2.2428e-01,  3.8986e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.3004e+00,  5.2404e-01, -1.0576e+00,  ..., -6.4015e-05,\n",
      "             1.6436e+00,  9.4714e-01],\n",
      "           [ 8.8554e-01,  1.2777e+00,  5.9402e-01,  ...,  2.9252e-01,\n",
      "             1.6587e+00,  1.8548e+00],\n",
      "           [ 1.2529e+00,  6.5000e-01,  1.8266e+00,  ...,  9.5567e-01,\n",
      "             1.8312e+00,  2.3870e+00],\n",
      "           ...,\n",
      "           [ 5.8469e-01, -1.3493e+00,  2.7399e-01,  ..., -4.8004e-01,\n",
      "            -1.8572e+00,  2.3028e-01],\n",
      "           [ 7.9467e-01,  1.6381e-01, -1.2094e+00,  ...,  1.2266e-01,\n",
      "             1.1226e+00, -2.0688e+00],\n",
      "           [-1.1168e+00,  1.0823e+00,  4.4527e-01,  ..., -1.2477e+00,\n",
      "             9.4645e-02,  1.5220e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-4.6382e-01,  1.7673e+00,  1.3451e+00,  ..., -5.7629e-01,\n",
      "            -2.0960e-01, -5.6611e-01],\n",
      "           [-2.5170e+00, -1.6661e+00, -6.6775e-01,  ..., -1.2017e+00,\n",
      "            -1.5018e+00, -1.6342e+00],\n",
      "           [-2.5488e+00,  1.5354e+00,  1.1154e+00,  ...,  1.3037e+00,\n",
      "            -3.5550e-01,  3.6495e-02],\n",
      "           ...,\n",
      "           [ 8.5985e-01,  2.5536e-01,  9.8002e-01,  ...,  9.2126e-01,\n",
      "            -2.1330e+00,  4.5310e-01],\n",
      "           [ 2.9910e-01,  1.1169e+00,  1.1924e+00,  ..., -3.3234e-02,\n",
      "             9.9537e-01,  2.1450e+00],\n",
      "           [ 9.3763e-01,  1.2729e+00, -4.9561e-01,  ..., -5.2405e-01,\n",
      "             1.6493e+00,  1.5242e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 9.5224e-01, -1.6935e+00, -2.0799e-01,  ...,  2.6421e-01,\n",
      "            -1.2743e+00,  1.7320e+00],\n",
      "           [ 2.2422e+00,  2.4057e+00, -3.6790e-01,  ..., -8.1122e-02,\n",
      "             5.9336e-01, -1.9457e+00],\n",
      "           [-2.8295e-01, -4.6672e-02,  2.2211e-01,  ..., -1.4567e+00,\n",
      "             9.2255e-01, -2.3417e+00],\n",
      "           ...,\n",
      "           [ 1.1583e+00, -1.4690e+00,  1.2997e+00,  ...,  2.3866e-01,\n",
      "            -9.3477e-01,  4.1212e-01],\n",
      "           [ 2.0696e-01, -7.7788e-01,  7.1339e-01,  ..., -7.1319e-01,\n",
      "            -9.4783e-01, -9.0921e-01],\n",
      "           [ 1.1073e+00,  9.1025e-01,  1.5385e+00,  ...,  9.0098e-01,\n",
      "             3.1537e-01,  8.5373e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8808e-01, -1.4498e+00,  2.6431e-01,  ..., -1.7177e+00,\n",
      "             2.1764e+00,  2.9566e+00],\n",
      "           [ 3.7309e-01, -7.2136e-01,  1.8348e-01,  ...,  2.0621e+00,\n",
      "             1.5619e-01, -3.0948e-01],\n",
      "           [-2.8739e-01, -2.7712e-01,  1.2147e+00,  ..., -2.7568e-01,\n",
      "             1.8114e+00,  4.9460e-01],\n",
      "           ...,\n",
      "           [-7.5623e-01, -9.3063e-01, -2.5034e-02,  ..., -1.7760e+00,\n",
      "            -5.3730e-01,  8.0419e-01],\n",
      "           [-1.2293e+00,  4.1860e-01, -1.6784e+00,  ..., -6.3226e-01,\n",
      "            -1.1467e+00, -2.3791e+00],\n",
      "           [-6.6479e-01, -3.3353e+00, -2.6209e-01,  ...,  5.9354e-01,\n",
      "             6.5320e-01, -1.4491e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.1015,  0.2380,  0.8350,  ...,  0.2861,  0.4783,  0.1332],\n",
      "           [ 0.1292, -0.0719,  0.2774,  ..., -0.1720,  0.0485,  0.7066],\n",
      "           [ 0.0472,  0.2730,  0.4886,  ...,  0.3464,  0.4040,  0.1353],\n",
      "           ...,\n",
      "           [-0.4087, -0.1584, -0.0742,  ..., -0.3415, -0.4329, -0.2360],\n",
      "           [-0.3347, -0.0326,  0.2635,  ..., -0.6782, -0.6156, -0.7874],\n",
      "           [-0.1635,  0.4474, -0.3907,  ..., -0.0710, -0.3634, -0.2425]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7046,  0.3491,  0.2888,  ...,  0.5021,  0.2898,  0.3112],\n",
      "           [ 0.6387,  0.3030,  0.4424,  ...,  0.5511,  0.3524,  0.1862],\n",
      "           [ 1.0978,  1.0019,  0.8919,  ...,  0.5479,  0.1307,  0.1415],\n",
      "           ...,\n",
      "           [-0.0217,  0.2000, -0.3135,  ..., -0.1466,  0.3523, -0.1564],\n",
      "           [ 0.4537, -0.2785,  0.5093,  ...,  0.4735,  0.1956, -0.2890],\n",
      "           [ 0.0131,  0.4528,  0.1271,  ...,  0.1489,  0.0578,  0.2448]]],\n",
      "\n",
      "\n",
      "         [[[-0.9514, -0.7288, -0.7952,  ..., -0.4027, -0.6667, -0.5660],\n",
      "           [-0.4154, -0.6293, -0.6133,  ..., -0.8445, -0.7920, -0.5402],\n",
      "           [-0.8634, -0.5956, -0.7707,  ..., -0.7197, -0.5995, -0.7123],\n",
      "           ...,\n",
      "           [-0.0127, -0.0674, -0.0814,  ..., -0.1786,  0.9210,  0.1527],\n",
      "           [-0.0143, -0.0460,  0.5209,  ...,  0.5382,  0.7755,  0.0631],\n",
      "           [ 0.2010, -0.1507, -0.1527,  ...,  0.8835, -0.1072,  0.6432]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1938, -0.1783, -0.3696,  ..., -0.2430,  0.3063, -0.0735],\n",
      "           [ 0.7980,  0.3580,  0.2843,  ...,  0.6301,  0.6440,  0.0705],\n",
      "           [-0.7019, -0.6794, -0.4266,  ..., -0.0393, -0.5373, -0.2400],\n",
      "           ...,\n",
      "           [-0.3657, -0.2837,  0.0313,  ...,  0.0045, -0.2810, -0.5350],\n",
      "           [-0.4564, -1.0269,  0.2668,  ...,  0.0887,  0.3806, -0.1706],\n",
      "           [-0.7113, -0.3174, -0.2784,  ..., -0.0164, -0.3513,  0.0540]]],\n",
      "\n",
      "\n",
      "         [[[-0.0563,  0.2944, -0.2399,  ..., -0.1265, -0.2482, -0.4507],\n",
      "           [ 0.0661,  0.0346,  0.2008,  ...,  0.2498, -0.4385, -0.4305],\n",
      "           [ 0.0081, -0.1068,  0.2219,  ...,  0.1135,  0.0695, -0.0749],\n",
      "           ...,\n",
      "           [-0.2470,  0.0981, -0.0941,  ..., -0.8445,  0.0625, -0.4857],\n",
      "           [-0.0122, -0.8554, -0.1802,  ...,  0.2223,  0.4423, -0.7298],\n",
      "           [-0.2174, -0.0880, -0.0917,  ...,  0.4062, -0.1957,  0.5188]]],\n",
      "\n",
      "\n",
      "         [[[-0.0171, -0.1387,  0.0081,  ..., -0.3960,  0.2299, -0.2106],\n",
      "           [-0.0476, -0.4975, -0.3690,  ...,  0.0059, -0.5150, -0.6201],\n",
      "           [ 0.1983,  0.1115,  0.2821,  ...,  0.0022,  0.4833, -0.0757],\n",
      "           ...,\n",
      "           [ 0.7916,  0.1642,  0.3486,  ..., -0.3269,  1.0851,  0.5047],\n",
      "           [ 0.3506,  0.5634,  0.5257,  ..., -0.0723,  0.6691,  0.6010],\n",
      "           [ 0.5065,  0.2957,  0.5469,  ...,  0.4226,  0.4852,  0.3152]]]]]), tensor([]), tensor([230.7237]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.8164, -0.9918,  0.9458,  ..., -1.3593, -0.6188, -1.8844],\n",
      "           [-1.4696, -1.6637, -0.0667,  ..., -1.1622, -0.2828,  2.0192],\n",
      "           [-1.4078, -0.6469,  0.4299,  ...,  0.0335,  0.5891, -0.7482],\n",
      "           ...,\n",
      "           [-0.5569, -1.9423, -0.3104,  ..., -0.0943,  0.6988,  0.9452],\n",
      "           [-0.9714,  0.3389,  1.1593,  ..., -1.0727, -0.3288, -0.6574],\n",
      "           [-0.7282,  2.2459, -1.4020,  ...,  1.4132,  0.2295,  0.6993]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3048, -1.3223, -1.1426,  ..., -1.1725, -1.6000, -0.8745],\n",
      "           [ 0.4230, -0.7261, -0.8019,  ..., -0.2616, -1.0494, -1.0051],\n",
      "           [ 0.8717, -0.3587, -0.3493,  ..., -0.4862, -2.0227, -0.9978],\n",
      "           ...,\n",
      "           [ 1.4118,  0.8248, -1.0325,  ..., -0.1619,  1.1968,  0.1407],\n",
      "           [ 1.0513,  1.3913,  1.2300,  ...,  1.5170,  0.6650, -1.6434],\n",
      "           [ 1.1646,  2.2521, -0.2992,  ...,  0.4570, -0.3353, -1.3464]]],\n",
      "\n",
      "\n",
      "         [[[-0.7650, -0.4279,  1.1858,  ...,  2.5432,  1.8692,  1.7990],\n",
      "           [ 0.8210,  1.9716,  1.6580,  ...,  0.9795,  1.4255,  1.9982],\n",
      "           [-0.3213,  1.4335,  1.1650,  ...,  0.3464,  1.1557,  0.0039],\n",
      "           ...,\n",
      "           [-0.6064,  0.6487,  1.2150,  ..., -0.6381,  1.3559, -2.2310],\n",
      "           [-1.3322,  0.2489,  2.1789,  ...,  1.7682,  1.0704, -1.3764],\n",
      "           [ 1.5879,  0.4544,  0.3395,  ...,  0.4131, -0.6768,  1.0029]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.5513, -0.6794, -0.2448,  ..., -1.4298,  0.9349, -1.6583],\n",
      "           [ 0.0290,  0.2885, -0.1899,  ..., -0.2618, -0.4298, -1.8497],\n",
      "           [-0.5204, -0.0134, -0.6354,  ...,  0.5751, -1.0633,  0.1636],\n",
      "           ...,\n",
      "           [ 0.6282,  0.8238,  1.7786,  ...,  0.4634, -0.1035,  1.1369],\n",
      "           [-0.0798, -0.7331,  1.3708,  ..., -0.3846,  1.8176,  1.1327],\n",
      "           [ 1.6641,  2.4117, -0.4770,  ...,  0.9228, -0.4753,  0.5218]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1878,  0.8814, -0.1705,  ...,  0.0940, -0.0469, -0.8310],\n",
      "           [ 0.6764,  0.6837,  1.2705,  ...,  1.1018, -1.3227, -0.9498],\n",
      "           [ 0.7793, -0.1405,  1.1294,  ...,  0.2186, -0.1482, -0.0537],\n",
      "           ...,\n",
      "           [ 0.5558,  1.4012,  0.4881,  ..., -1.3896,  0.6989, -0.8570],\n",
      "           [ 0.2520, -1.4674, -0.4426,  ...,  2.2351,  1.3461, -2.3556],\n",
      "           [ 0.9542,  0.1557,  0.6180,  ...,  2.9015,  0.0796,  1.4481]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5735, -0.3382,  1.3626,  ..., -0.1262,  2.6435,  0.5321],\n",
      "           [-0.4823, -0.8586, -0.3725,  ...,  0.7433, -1.2498, -1.6542],\n",
      "           [-0.0808,  0.1251,  0.8409,  ...,  0.4177,  2.0668,  0.2016],\n",
      "           ...,\n",
      "           [ 0.7880,  0.5078,  0.6674,  ..., -1.5466,  0.7775, -1.1045],\n",
      "           [-1.7284, -0.4194,  0.6468,  ..., -1.9201, -0.5860, -1.4553],\n",
      "           [ 0.1507, -1.5320,  0.5188,  ..., -0.8999, -0.7242, -0.5631]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.4574,  0.8224,  0.3763,  ...,  0.8356, -0.0765,  0.2815],\n",
      "           [-0.2289, -0.0259,  1.1603,  ...,  0.3207,  0.1463,  0.8407],\n",
      "           [ 0.1766,  0.0512,  0.7529,  ...,  0.5472, -0.2525, -0.3285],\n",
      "           ...,\n",
      "           [ 0.1152,  1.2308,  0.6142,  ..., -0.8294, -0.0524,  0.3348],\n",
      "           [-0.3527,  0.0446,  0.3013,  ...,  0.5182, -0.3860, -0.0809],\n",
      "           [ 0.1458, -0.3021, -0.2415,  ..., -0.4540, -0.0250,  0.2203]]],\n",
      "\n",
      "\n",
      "         [[[-0.6185,  0.8200,  0.4593,  ...,  0.0151,  0.6732,  0.5352],\n",
      "           [ 0.7331,  0.0781,  0.1528,  ...,  0.1041,  0.4732, -0.0646],\n",
      "           [ 0.7967,  1.0077,  0.2968,  ..., -0.0199,  0.6563,  0.1718],\n",
      "           ...,\n",
      "           [-0.6418,  0.1078,  0.3846,  ..., -0.2571, -0.1644, -0.6086],\n",
      "           [ 0.3760, -0.2416, -0.0937,  ...,  0.4978, -0.1944,  0.1092],\n",
      "           [-0.3535,  0.2283, -0.3631,  ...,  0.9469,  0.3574, -0.0442]]],\n",
      "\n",
      "\n",
      "         [[[-0.5486, -0.0540, -0.8815,  ..., -0.6337, -0.7231, -0.5070],\n",
      "           [ 0.0511, -0.8636, -0.8777,  ..., -0.8905, -0.8136, -0.9607],\n",
      "           [-0.0680, -0.8190, -0.3187,  ..., -0.3925, -0.7330,  0.1560],\n",
      "           ...,\n",
      "           [ 0.1286,  0.2523, -0.1403,  ...,  0.6238,  0.3515,  0.0882],\n",
      "           [ 0.0746, -0.3023,  0.2489,  ...,  0.5504, -0.0046,  0.3938],\n",
      "           [-0.0078,  0.5439, -0.6437,  ..., -0.4084,  0.6235,  0.3072]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.7308,  0.2288,  0.5763,  ..., -0.5705,  0.0843,  0.4127],\n",
      "           [ 0.5338, -0.0944,  0.1436,  ...,  0.4366,  0.6493,  0.0644],\n",
      "           [-0.4610, -0.0383,  0.1700,  ...,  0.3484,  0.1268, -0.6831],\n",
      "           ...,\n",
      "           [-0.3950, -0.1515,  0.0031,  ..., -0.3043, -0.1508, -0.1687],\n",
      "           [-0.6382, -0.6433, -0.2780,  ..., -0.4088,  0.2415, -0.1398],\n",
      "           [-0.1241, -0.1643, -0.5997,  ...,  0.9975,  0.0523, -0.2960]]],\n",
      "\n",
      "\n",
      "         [[[-0.3189,  0.0784, -0.1042,  ..., -0.0524,  0.2834, -0.7725],\n",
      "           [ 0.0221,  0.1994, -0.6061,  ..., -0.6361, -0.2120,  0.0969],\n",
      "           [ 0.3882,  0.3378,  0.0449,  ...,  0.4706,  0.0398,  0.0508],\n",
      "           ...,\n",
      "           [ 0.0641, -0.7735,  0.1561,  ..., -0.8402, -0.2745, -0.6299],\n",
      "           [-0.0401,  0.1234, -0.3413,  ..., -1.0829, -0.2926,  0.0526],\n",
      "           [-0.6545,  0.3419, -0.2408,  ...,  0.3003, -0.0428, -0.2548]]],\n",
      "\n",
      "\n",
      "         [[[-0.2521, -0.1185,  0.0385,  ..., -0.0991, -0.0224, -0.1336],\n",
      "           [ 0.3983, -0.3707, -0.4047,  ..., -0.1031,  0.0623,  0.2587],\n",
      "           [ 0.1365,  0.1399, -0.1678,  ...,  0.4468, -0.4693, -0.0884],\n",
      "           ...,\n",
      "           [ 0.2519,  0.1532,  0.3917,  ...,  0.1434,  0.6711,  0.7338],\n",
      "           [ 1.0190, -0.0338,  0.9046,  ...,  0.6377,  0.6902,  0.2228],\n",
      "           [ 0.0804,  0.3589,  0.9643,  ...,  0.6809,  0.3758, -0.3521]]]]]), tensor([]), tensor([362.0320]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.1830,  0.9383, -0.6348,  ...,  0.5364, -1.8676, -0.7835],\n",
      "           [-1.8385, -0.8692,  2.4103,  ...,  0.4904, -0.0092,  1.5940],\n",
      "           [-0.5569, -1.0121,  0.8253,  ...,  1.0157, -1.2068, -1.7943],\n",
      "           ...,\n",
      "           [ 1.1353,  2.6179,  1.7232,  ..., -1.0634,  1.3651,  2.1915],\n",
      "           [-0.6907,  0.4472,  0.8030,  ...,  2.8991,  0.3099,  0.6719],\n",
      "           [ 0.3835, -0.6795, -0.4760,  ...,  0.3438,  1.0906,  1.2591]]],\n",
      "\n",
      "\n",
      "         [[[-3.5627,  0.4230, -0.5981,  ..., -1.8571,  0.4024,  0.1907],\n",
      "           [ 0.4426, -1.1164, -1.0967,  ..., -1.9482, -0.6150, -1.6259],\n",
      "           [-0.4649,  0.1505, -2.3098,  ..., -2.0067,  0.3057, -0.7865],\n",
      "           ...,\n",
      "           [-0.7850,  0.3064,  1.2360,  ...,  0.4315, -0.8011, -1.0049],\n",
      "           [ 0.5126,  1.0287, -0.8436,  ...,  0.8611, -1.3891, -0.1850],\n",
      "           [-0.3645,  0.7400, -1.4857,  ...,  1.7954, -0.8946, -1.2759]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5787,  1.6014,  0.5957,  ...,  1.0206,  0.8942,  1.2996],\n",
      "           [ 1.8295,  0.4787,  0.2712,  ...,  0.4609,  0.6599,  0.2008],\n",
      "           [ 1.7242,  0.3338,  1.9787,  ...,  1.5184,  0.3427,  2.5670],\n",
      "           ...,\n",
      "           [-0.0403,  1.3130,  0.6439,  ...,  1.3280, -0.8540, -1.7706],\n",
      "           [-0.5646, -0.5967,  0.6973,  ...,  1.3474, -0.9999,  0.0052],\n",
      "           [ 0.4440,  2.1128, -1.1033,  ..., -2.4202,  0.9546,  0.1620]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.6230,  0.4250,  1.6197,  ..., -1.5188, -0.0146,  0.2351],\n",
      "           [-0.7999, -1.0957, -0.6424,  ..., -0.8971, -0.8725, -2.0561],\n",
      "           [ 0.2046,  1.3471,  1.1961,  ...,  1.2893,  1.3601, -1.4033],\n",
      "           ...,\n",
      "           [ 0.3134,  1.0472,  1.1447,  ..., -0.3677, -0.9144,  0.8057],\n",
      "           [-0.6670,  0.7405, -0.5661,  ..., -0.6213,  0.7535,  0.5021],\n",
      "           [ 2.6962,  1.8849, -1.0955,  ...,  2.3192,  0.9600, -0.0122]]],\n",
      "\n",
      "\n",
      "         [[[-0.6305, -0.0286,  0.0932,  ...,  0.4568,  1.6431, -1.2477],\n",
      "           [ 0.3276,  1.0894, -1.1829,  ..., -1.5892,  0.1951,  0.9570],\n",
      "           [ 1.6857,  1.3634,  0.5467,  ...,  0.9862, -0.2279,  0.0458],\n",
      "           ...,\n",
      "           [ 1.2407, -1.4571,  1.0358,  ..., -0.9668, -0.6516, -1.1233],\n",
      "           [ 0.0495,  1.7344, -0.7495,  ..., -2.3099, -0.7087,  0.4697],\n",
      "           [-0.6256,  1.3581,  0.0066,  ...,  1.0471, -0.1125, -0.9245]]],\n",
      "\n",
      "\n",
      "         [[[-0.3312, -0.1577,  1.0776,  ...,  0.7023,  1.0302,  0.8523],\n",
      "           [ 0.9159, -0.3079, -0.6637,  ...,  0.0352,  1.0482,  1.6111],\n",
      "           [ 0.1688,  0.3077, -0.2524,  ...,  1.3841, -1.1646, -0.0393],\n",
      "           ...,\n",
      "           [-0.9960,  0.2788,  0.5485,  ..., -1.1655, -0.7042, -0.2021],\n",
      "           [ 0.7407, -1.9032,  1.4539,  ...,  0.1738, -0.9147, -1.3300],\n",
      "           [-1.0606, -0.7855,  1.4406,  ...,  1.0811, -0.2126, -2.7451]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 7.0997e-01, -3.0434e-01,  6.4369e-01,  ...,  5.8865e-01,\n",
      "             6.1417e-01,  4.4722e-01],\n",
      "           [-8.3753e-01,  9.7361e-01, -1.0308e-01,  ...,  5.9476e-02,\n",
      "             3.8443e-01,  4.1519e-01],\n",
      "           [-4.1441e-01, -4.6070e-01, -3.1243e-01,  ..., -4.0059e-01,\n",
      "            -7.5627e-01,  4.5095e-01],\n",
      "           ...,\n",
      "           [-1.2711e-01, -4.2604e-02, -2.9547e-01,  ...,  2.3648e-01,\n",
      "            -4.2957e-01,  2.8415e-01],\n",
      "           [ 7.6669e-01,  6.1295e-01, -1.2072e+00,  ..., -2.3203e-01,\n",
      "             2.5489e-01, -5.0508e-01],\n",
      "           [-6.9648e-01,  6.0956e-02, -6.0775e-01,  ...,  2.6711e-01,\n",
      "            -1.5063e-01, -6.7936e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.4450e-02,  8.1250e-02,  3.0888e-01,  ..., -2.7910e-02,\n",
      "             1.7791e-01,  3.2443e-01],\n",
      "           [-9.0568e-01, -2.5283e-01,  1.2591e+00,  ...,  8.1613e-02,\n",
      "             1.1172e+00,  1.2302e+00],\n",
      "           [ 8.4202e-01,  3.0382e-01,  6.5002e-01,  ..., -1.5680e-01,\n",
      "            -3.1056e-01,  9.7238e-01],\n",
      "           ...,\n",
      "           [-7.6270e-01, -2.6069e-01,  9.8218e-01,  ..., -7.2960e-01,\n",
      "            -7.2327e-01,  4.6615e-01],\n",
      "           [ 4.5335e-01, -8.8490e-01, -4.6356e-01,  ...,  6.8987e-01,\n",
      "            -2.8489e-01, -4.3652e-01],\n",
      "           [-3.2933e-01,  1.5315e-01, -4.5556e-01,  ...,  1.1056e-01,\n",
      "            -2.3588e-01,  2.3057e-01]]],\n",
      "\n",
      "\n",
      "         [[[-8.3117e-02, -9.6376e-02, -8.0120e-01,  ..., -1.0575e+00,\n",
      "            -8.2287e-01, -6.7819e-01],\n",
      "           [-1.1463e-01, -4.1844e-01, -5.6495e-02,  ..., -3.2661e-01,\n",
      "            -6.4924e-02, -4.9369e-01],\n",
      "           [-3.0685e-01,  5.6576e-01, -5.0883e-02,  ..., -5.3584e-01,\n",
      "            -4.2490e-01, -1.5798e+00],\n",
      "           ...,\n",
      "           [ 4.3292e-01, -2.1917e-01, -3.5835e-01,  ..., -9.4429e-01,\n",
      "             3.8041e-01,  3.6859e-01],\n",
      "           [ 5.0689e-01, -7.7446e-01, -3.3862e-01,  ...,  4.5928e-01,\n",
      "             4.0595e-01,  3.6588e-01],\n",
      "           [-2.5160e-01,  1.9111e-01,  7.6472e-01,  ...,  2.4416e-02,\n",
      "            -1.4273e-01,  5.4433e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.0485e+00, -3.2527e-01,  1.3765e-01,  ..., -2.8512e-01,\n",
      "            -2.3985e-01, -1.6321e-01],\n",
      "           [-3.3714e-02, -7.4532e-01,  5.2838e-01,  ...,  8.2461e-02,\n",
      "             5.7903e-01,  5.1906e-01],\n",
      "           [-9.7769e-01,  5.3166e-01, -1.6493e+00,  ...,  3.1556e-01,\n",
      "             1.6795e-01, -2.2208e-01],\n",
      "           ...,\n",
      "           [ 1.1021e-01, -3.7808e-01,  1.9899e-01,  ...,  2.6390e-01,\n",
      "            -4.6441e-01, -1.4319e+00],\n",
      "           [-5.6934e-02, -1.7929e-01, -1.6608e-01,  ..., -4.2411e-01,\n",
      "            -2.4594e-03, -7.6643e-01],\n",
      "           [-1.8526e+00, -1.8331e-01,  7.8137e-01,  ..., -1.1356e-01,\n",
      "            -6.3307e-01, -3.7220e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.6416e-03, -1.5603e-01,  2.2344e-02,  ...,  1.1502e+00,\n",
      "            -1.6991e-01,  4.7245e-01],\n",
      "           [-1.1930e-01, -1.2854e+00, -6.7754e-01,  ..., -9.7451e-02,\n",
      "             4.1192e-01,  6.4328e-01],\n",
      "           [-4.0633e-01, -7.0515e-02, -8.8125e-01,  ...,  4.2640e-01,\n",
      "            -6.1841e-01, -7.8198e-01],\n",
      "           ...,\n",
      "           [-1.0618e+00, -6.3744e-02, -6.4828e-01,  ..., -6.1700e-01,\n",
      "            -4.3844e-02, -1.3776e+00],\n",
      "           [-9.3192e-02, -9.4074e-01,  3.6215e-01,  ..., -6.8210e-01,\n",
      "            -1.3427e-01,  3.6399e-01],\n",
      "           [ 3.4892e-01, -1.2274e-01,  3.2794e-01,  ...,  6.8368e-02,\n",
      "             6.1343e-01,  7.7380e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.6842e-01,  4.6972e-01, -7.2817e-01,  ...,  8.2203e-01,\n",
      "            -6.0985e-03, -4.9480e-01],\n",
      "           [ 6.2908e-01,  4.3025e-01,  5.7511e-02,  ..., -4.3922e-01,\n",
      "             7.5582e-01,  1.6324e+00],\n",
      "           [ 3.0953e-01,  9.6519e-01,  2.5601e-01,  ..., -6.3820e-01,\n",
      "            -4.0353e-01, -1.7187e-01],\n",
      "           ...,\n",
      "           [ 5.6920e-01,  9.9360e-01, -1.9053e-01,  ...,  2.8845e-01,\n",
      "            -9.1205e-01, -1.3015e-01],\n",
      "           [ 3.3451e-01, -1.2867e+00,  1.1146e-01,  ..., -6.8229e-01,\n",
      "             7.0624e-01,  6.2945e-01],\n",
      "           [-8.0732e-01,  5.8240e-02,  4.0453e-01,  ...,  4.4101e-04,\n",
      "             6.0126e-01,  3.0943e-01]]]]]), tensor([]), tensor([526.3354]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.3276, -1.5919,  0.1245,  ...,  0.0305,  0.0772, -0.1217],\n",
      "           [-2.4828,  1.2186, -0.8120,  ..., -0.2733,  0.4217,  0.1577],\n",
      "           [-1.6000, -1.6152, -1.4422,  ..., -1.3124, -1.9510,  0.1787],\n",
      "           ...,\n",
      "           [ 0.2830, -0.5314, -0.6026,  ...,  1.1366,  0.2470,  1.3165],\n",
      "           [ 1.6130,  1.4792, -2.3152,  ...,  0.4769,  1.3945, -0.1556],\n",
      "           [-1.3526,  0.2515, -1.0786,  ...,  1.4887,  0.6365, -0.8563]]],\n",
      "\n",
      "\n",
      "         [[[-1.1476, -1.1195, -0.2724,  ..., -1.2807, -0.7894, -0.3549],\n",
      "           [-2.7134, -1.2822,  1.3142,  ..., -1.0787,  1.1227,  1.5229],\n",
      "           [ 0.0167, -1.3492, -0.7151,  ..., -1.6018, -1.7906,  1.0681],\n",
      "           ...,\n",
      "           [-1.0761, -0.5408,  1.8462,  ..., -0.5405, -1.5349,  1.3183],\n",
      "           [ 0.3953, -0.6229, -1.2498,  ...,  1.2070, -0.7270, -0.5662],\n",
      "           [-0.2369,  0.4338, -1.3146,  ...,  0.3098, -0.9627, -0.0243]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3193,  1.1254,  0.3608,  ...,  0.0600,  0.4161,  0.4927],\n",
      "           [ 0.9196,  1.1566,  1.8913,  ...,  1.4218,  1.9135,  0.9909],\n",
      "           [ 0.8728,  2.7418,  1.7207,  ...,  0.7088,  0.8631, -1.5498],\n",
      "           ...,\n",
      "           [ 0.5558,  0.0492,  0.0742,  ..., -1.6313, -0.6833, -0.7920],\n",
      "           [ 0.4711, -1.2852, -0.6622,  ...,  0.7239, -0.4994, -0.3342],\n",
      "           [-0.1133,  0.8184,  1.9072,  ..., -1.3305, -0.6570,  0.6029]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.7530, -0.4865,  0.8853,  ..., -0.1588, -0.3931, -0.3715],\n",
      "           [-1.8088, -2.1766, -0.1042,  ..., -1.3990, -0.5258, -0.3983],\n",
      "           [-1.2107,  2.1778, -2.0966,  ...,  1.1004,  0.7461, -0.2432],\n",
      "           ...,\n",
      "           [ 1.1006,  0.3122,  1.0902,  ...,  0.1017, -1.0760, -1.4151],\n",
      "           [ 0.6678,  1.2907, -0.2181,  ..., -0.9172, -0.2114, -0.3973],\n",
      "           [-1.5959,  1.2693,  1.6366,  ..., -0.1481, -0.8686, -0.2070]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2276, -0.4902,  0.4501,  ...,  2.6019,  0.1885,  1.3525],\n",
      "           [ 0.0167, -1.9444, -0.8560,  ..., -0.1286,  1.0997,  1.4667],\n",
      "           [-0.3921,  0.2722, -1.3693,  ...,  0.6907, -1.2902, -1.3711],\n",
      "           ...,\n",
      "           [-1.3234,  0.2573, -0.8519,  ..., -0.2503, -0.1771, -1.9810],\n",
      "           [-0.0941, -0.8684,  0.7498,  ..., -0.6790, -0.2288,  0.8582],\n",
      "           [ 1.4815, -0.0322,  1.0298,  ...,  0.4559,  1.1581,  1.4046]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0058,  1.1204, -0.6835,  ...,  2.2463,  0.6950, -0.1726],\n",
      "           [ 1.0571,  1.3797,  0.7559,  ..., -0.5127,  2.0244,  3.5883],\n",
      "           [ 0.3259,  1.8442,  0.3874,  ..., -1.2543, -0.7955, -0.1277],\n",
      "           ...,\n",
      "           [ 0.0092,  1.7285, -0.7931,  ..., -0.1201, -3.3314, -1.8029],\n",
      "           [-0.7259, -3.6788, -0.4642,  ..., -2.0168, -0.7078, -0.3613],\n",
      "           [-2.4866, -1.0838, -0.0233,  ..., -1.3488, -0.1799, -0.9582]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.2657,  0.3077,  0.6224,  ...,  0.5191,  0.4011,  0.4025],\n",
      "           [ 0.4341,  0.5808,  0.1782,  ...,  0.0611,  0.1168,  0.3029],\n",
      "           [ 0.3852,  0.2035,  0.5071,  ..., -0.0420,  0.3547,  0.1912],\n",
      "           ...,\n",
      "           [-0.6110,  0.3098, -0.1359,  ..., -0.3275, -0.4351, -0.3744],\n",
      "           [-0.1154, -0.1346, -0.0318,  ..., -0.3415, -0.4302,  0.1364],\n",
      "           [-0.1306, -0.3584, -0.0326,  ..., -0.4290, -0.1282, -0.0650]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1896,  0.6841,  0.1488,  ...,  0.5779,  0.2221,  0.3208],\n",
      "           [ 0.5740,  0.5179,  0.7414,  ...,  0.7187,  0.6193,  0.2664],\n",
      "           [ 0.6998,  0.6295,  0.7511,  ...,  0.6072,  0.4763,  0.7624],\n",
      "           ...,\n",
      "           [-0.3450,  0.0455, -0.0441,  ..., -0.2461, -0.2240, -0.5938],\n",
      "           [ 0.1427, -0.2046,  0.1365,  ...,  0.0021,  0.4771,  0.0931],\n",
      "           [-0.3213, -0.1248,  0.2248,  ...,  0.5934,  0.7838,  0.0554]]],\n",
      "\n",
      "\n",
      "         [[[-0.6009, -0.5367, -0.9110,  ..., -0.8264, -0.7148, -1.1125],\n",
      "           [-0.8375, -1.1359, -0.8874,  ..., -0.9226, -0.7892, -0.6583],\n",
      "           [-0.6945, -0.6485, -1.0523,  ..., -0.4107, -0.5756, -0.8107],\n",
      "           ...,\n",
      "           [-0.1462, -0.3410, -0.7240,  ...,  0.1665,  0.5222,  0.6198],\n",
      "           [-0.0074,  0.1534, -0.0132,  ...,  0.2398,  0.4411,  0.5816],\n",
      "           [-0.0320, -0.3935, -0.1250,  ...,  0.2380, -0.1419,  0.2513]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1087, -0.0592,  0.0298,  ..., -0.1450,  0.0633,  0.1312],\n",
      "           [ 0.4181,  0.1953,  0.5976,  ...,  0.5310,  0.5220,  0.6802],\n",
      "           [ 0.0327, -0.4752, -0.2084,  ..., -0.2541, -0.3905, -0.1821],\n",
      "           ...,\n",
      "           [-0.4665, -0.5893, -0.5468,  ...,  0.0273,  0.0266, -0.3834],\n",
      "           [-0.2536, -0.6070,  0.2419,  ..., -0.1849,  0.2867, -0.2642],\n",
      "           [-0.8324, -0.4705, -0.0965,  ..., -0.1594, -0.1523,  0.3818]]],\n",
      "\n",
      "\n",
      "         [[[-0.1893, -0.0290,  0.1518,  ..., -0.1505, -0.0542, -0.1404],\n",
      "           [-0.1425, -0.1123, -0.0994,  ..., -0.2290, -0.0257, -0.2724],\n",
      "           [-0.2476, -0.1915, -0.0329,  ...,  0.2083, -0.0838,  0.0504],\n",
      "           ...,\n",
      "           [-0.4191, -0.2195, -0.0566,  ..., -0.1750, -0.0398, -0.1370],\n",
      "           [-0.0111, -0.4413,  0.0079,  ..., -0.2148,  0.2283, -0.0121],\n",
      "           [-0.6978,  0.1319,  0.0806,  ..., -0.3259, -0.3024, -0.0677]]],\n",
      "\n",
      "\n",
      "         [[[-0.2606, -0.0740, -0.4497,  ..., -0.2494, -0.5350, -0.5741],\n",
      "           [ 0.1319, -0.2225,  0.0835,  ...,  0.1390,  0.1148, -0.1319],\n",
      "           [ 0.4399, -0.0589,  0.0667,  ..., -0.2449, -0.0335, -0.0097],\n",
      "           ...,\n",
      "           [ 0.8080,  0.0479, -0.0040,  ...,  0.4457,  0.6613,  0.8030],\n",
      "           [ 0.4454,  0.2499,  0.1776,  ...,  0.7053,  0.7359,  0.4374],\n",
      "           [ 0.7300,  0.7853,  0.5157,  ...,  0.6335,  0.3855,  0.7719]]]]]), tensor([]), tensor([169.2356]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.5524, -1.1517, -0.0838,  ..., -0.4486, -1.1887, -0.8757],\n",
      "           [-0.1794,  1.4318, -0.6174,  ..., -0.5979, -0.3377,  0.4323],\n",
      "           [ 0.1167, -1.0264,  0.5058,  ..., -1.7949,  0.8775, -0.7327],\n",
      "           ...,\n",
      "           [-1.9386,  0.1501, -0.7772,  ...,  0.8859,  1.1737,  0.9451],\n",
      "           [-0.0475, -0.1067, -0.2772,  ...,  1.3207,  0.4586,  3.1419],\n",
      "           [-0.6816, -1.7329,  0.1623,  ...,  0.5418,  1.1967,  0.9202]]],\n",
      "\n",
      "\n",
      "         [[[-2.5076,  0.2171, -2.7928,  ..., -0.5127, -1.7274, -0.7453],\n",
      "           [ 0.2724, -0.0297,  0.6369,  ..., -0.2881, -0.6534, -1.4954],\n",
      "           [-1.1990, -1.1072, -0.2419,  ..., -0.8334, -0.4011,  2.0130],\n",
      "           ...,\n",
      "           [-0.0109,  0.2004,  0.1798,  ...,  0.9012, -1.4159, -2.3386],\n",
      "           [-0.2881,  2.4579, -0.5556,  ..., -0.2851,  0.5205, -0.1972],\n",
      "           [-0.3565, -0.2890,  0.0459,  ...,  1.3123,  0.4806, -1.7545]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9145,  0.5952,  1.0372,  ...,  0.7614,  1.9911, -0.9771],\n",
      "           [-1.2723, -0.4859,  0.5639,  ...,  0.2845,  1.5335,  1.8189],\n",
      "           [ 0.9611,  1.8135,  0.8755,  ...,  2.5212,  1.5637, -0.3546],\n",
      "           ...,\n",
      "           [-1.6596, -0.7384, -2.0878,  ..., -0.3395, -1.3578, -0.1521],\n",
      "           [-1.7595,  1.4752, -0.0984,  ...,  0.7969,  0.6143,  0.8834],\n",
      "           [ 0.6239, -0.8934,  0.5355,  ..., -0.5420, -2.0992, -0.2172]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0826, -0.9309,  0.3497,  ..., -0.7075, -0.4289, -0.7540],\n",
      "           [-2.4731, -1.1120,  1.1264,  ..., -1.4453, -2.1569, -0.1886],\n",
      "           [ 1.9280,  0.3399,  1.2360,  ...,  0.0698,  0.0625,  0.2000],\n",
      "           ...,\n",
      "           [ 0.2078, -0.4439, -0.7360,  ...,  1.0818,  0.0756,  0.1498],\n",
      "           [ 1.0112,  1.9034,  1.7451,  ..., -1.2861,  1.6335, -1.8689],\n",
      "           [ 1.8795,  2.4245,  0.3041,  ..., -1.7902, -0.0434,  2.4532]]],\n",
      "\n",
      "\n",
      "         [[[-0.4440, -0.6890,  1.6784,  ...,  0.4758,  1.2854,  0.9335],\n",
      "           [-0.4201,  0.4714,  0.6974,  ..., -0.6728,  1.2650, -0.1264],\n",
      "           [-0.4874, -0.2007, -0.1395,  ...,  0.4659, -0.9674,  0.3338],\n",
      "           ...,\n",
      "           [-0.2436,  0.1424,  0.9971,  ...,  1.5257,  0.5271,  0.4051],\n",
      "           [ 0.3486,  0.3572,  0.4619,  ...,  0.8920,  1.3977,  0.5626],\n",
      "           [-1.3867,  1.6312,  1.9644,  ..., -1.1659, -1.5941, -0.7501]]],\n",
      "\n",
      "\n",
      "         [[[-0.7004, -0.1395, -0.5972,  ...,  0.2664, -0.8041, -0.8061],\n",
      "           [ 0.3978,  0.3265,  1.9696,  ...,  1.4344,  2.3069,  1.1063],\n",
      "           [ 2.3110, -0.2691,  0.2665,  ..., -0.7050,  0.2845,  0.2793],\n",
      "           ...,\n",
      "           [ 1.1994,  0.0481, -1.1553,  ..., -2.0130, -1.7662,  0.1476],\n",
      "           [-1.7591, -2.4555, -1.2006,  ..., -0.5774, -0.6210, -1.2075],\n",
      "           [ 1.5265,  0.7644,  0.4940,  ...,  0.8854, -0.4493, -0.0926]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.5407,  0.4244,  0.5127,  ...,  0.5555,  0.4608,  0.5203],\n",
      "           [ 0.3732,  0.0529,  0.1886,  ...,  0.3012,  0.0831,  0.1731],\n",
      "           [ 0.3126,  0.2524,  0.2879,  ...,  0.2194,  0.1708,  0.2179],\n",
      "           ...,\n",
      "           [-0.2133,  0.1993,  0.0204,  ..., -0.4094, -0.6425, -0.4266],\n",
      "           [-0.1772, -0.1369,  0.0313,  ..., -0.3550, -0.5345, -0.2895],\n",
      "           [-0.0402, -0.0770, -0.0261,  ..., -0.4973, -0.3515, -0.3316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3961,  0.5462,  0.4365,  ...,  0.6365,  0.3589,  0.6394],\n",
      "           [ 0.4397,  0.3396,  0.5646,  ...,  0.5318,  0.5081,  0.4985],\n",
      "           [ 0.7568,  0.8107,  0.9910,  ...,  0.5886,  0.4606,  0.2539],\n",
      "           ...,\n",
      "           [-0.1770,  0.1477,  0.1533,  ..., -0.3611,  0.1761, -0.1790],\n",
      "           [ 0.2273, -0.6925,  0.1216,  ...,  0.0083,  0.0243, -0.3444],\n",
      "           [-0.2184, -0.0424,  0.0730,  ...,  0.2533,  0.1832,  0.0573]]],\n",
      "\n",
      "\n",
      "         [[[-0.8487, -0.5335, -0.8402,  ..., -1.1037, -0.7620, -0.8826],\n",
      "           [-0.5914, -0.9655, -1.0149,  ..., -0.8500, -0.9132, -0.8595],\n",
      "           [-0.9990, -0.8893, -0.7329,  ..., -0.6974, -0.7208, -0.7169],\n",
      "           ...,\n",
      "           [ 0.1460, -0.1381, -0.4761,  ...,  0.0776,  0.6205,  0.7898],\n",
      "           [ 0.2164, -0.1677,  0.1713,  ...,  0.1586,  0.6959,  0.5166],\n",
      "           [-0.2960, -0.2851, -0.2810,  ...,  0.5629,  0.3148,  0.0708]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1636, -0.0417, -0.1934,  ..., -0.0448,  0.0814,  0.1120],\n",
      "           [ 0.7170,  0.2460,  0.6738,  ...,  0.8738,  0.7590,  0.6352],\n",
      "           [-0.4057, -0.4690, -0.4925,  ..., -0.2386, -0.2879,  0.0587],\n",
      "           ...,\n",
      "           [-0.5574, -0.4931, -0.3181,  ...,  0.0789,  0.0057, -0.8778],\n",
      "           [-0.5940, -0.8440, -0.0318,  ...,  0.0972, -0.1449, -0.6514],\n",
      "           [-0.9498, -0.7370,  0.0817,  ..., -0.1491, -0.2870, -0.2318]]],\n",
      "\n",
      "\n",
      "         [[[-0.0945,  0.1826, -0.2303,  ..., -0.2838, -0.1977, -0.2608],\n",
      "           [-0.0488, -0.2331, -0.1984,  ...,  0.0041, -0.1724, -0.1560],\n",
      "           [-0.2403, -0.2684, -0.1825,  ...,  0.1644,  0.0513, -0.0822],\n",
      "           ...,\n",
      "           [-0.4186, -0.3225, -0.1562,  ..., -0.4482,  0.0797, -0.2330],\n",
      "           [-0.0041, -0.4849, -0.0633,  ..., -0.4037,  0.1613, -0.5005],\n",
      "           [-0.2686, -0.0173, -0.2338,  ..., -0.0969, -0.1630, -0.1510]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0057,  0.0298, -0.3871,  ..., -0.3508, -0.2255, -0.2432],\n",
      "           [ 0.2016, -0.3390, -0.1138,  ..., -0.2053, -0.2530, -0.4108],\n",
      "           [ 0.1468, -0.1392,  0.0613,  ...,  0.0129, -0.0961, -0.0071],\n",
      "           ...,\n",
      "           [ 0.6938,  0.0044,  0.0785,  ...,  0.0053,  0.8858,  0.9073],\n",
      "           [ 0.6511,  0.5341,  0.2694,  ...,  0.2459,  0.8614,  0.9653],\n",
      "           [ 0.5292,  0.5897,  0.3355,  ...,  0.7592,  0.9245,  0.9014]]]]]), tensor([]), tensor([86.8546]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.0239, -0.9707, -1.2013,  ..., -0.0943, -0.8068,  0.0748],\n",
      "           [-0.8570, -2.9043, -0.9436,  ...,  1.1568, -0.8077, -1.2703],\n",
      "           [-1.2819, -1.3565, -0.9738,  ..., -0.5899, -1.2282, -1.1157],\n",
      "           ...,\n",
      "           [ 0.7308, -0.4894, -0.0510,  ...,  0.4498,  0.0729, -0.0817],\n",
      "           [-1.0798,  0.2960,  0.2131,  ...,  1.3069, -0.4721,  2.7290],\n",
      "           [-0.6392, -0.0393,  0.1206,  ...,  0.3449,  1.7646,  0.2976]]],\n",
      "\n",
      "\n",
      "         [[[-2.4337, -1.4449, -0.9921,  ..., -0.1967, -2.3088,  1.6901],\n",
      "           [-1.0594, -0.1597,  0.0645,  ..., -1.0058, -0.0120,  1.0615],\n",
      "           [-1.0807, -1.2149, -0.0770,  ..., -1.0401, -1.9152, -1.2369],\n",
      "           ...,\n",
      "           [ 0.3344,  1.4012,  1.7258,  ..., -0.7105, -0.5422,  1.1739],\n",
      "           [-0.2055, -1.5425, -0.7919,  ..., -0.5592, -0.8517, -0.0606],\n",
      "           [-0.1492,  0.3843, -1.9060,  ...,  1.4525,  0.2379,  1.3466]]],\n",
      "\n",
      "\n",
      "         [[[-0.8898,  1.4534,  2.2215,  ..., -0.3677,  3.2308,  0.8717],\n",
      "           [ 0.2451,  1.0837,  0.8840,  ...,  2.5197,  1.6099,  1.6081],\n",
      "           [-2.2997,  1.3039,  2.0195,  ...,  2.4476,  1.6940,  0.4612],\n",
      "           ...,\n",
      "           [ 0.0268,  1.1282, -0.9750,  ...,  2.1530, -0.5285,  0.3357],\n",
      "           [-0.5158, -0.8337,  1.8377,  ...,  1.4145,  0.0158,  0.2986],\n",
      "           [-1.2049, -0.5314, -0.4698,  ..., -1.7177,  0.3746,  0.0363]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.3321,  0.3825, -0.1090,  ...,  1.8093,  1.2241,  0.5617],\n",
      "           [-2.1461, -1.5164,  0.1511,  ...,  0.7388, -0.4298, -0.3270],\n",
      "           [ 0.6789,  0.9133,  0.4906,  ...,  1.0207,  0.5961,  1.5584],\n",
      "           ...,\n",
      "           [-0.9624,  0.5035,  0.8272,  ...,  0.1361, -0.4058,  1.5415],\n",
      "           [-2.0867,  0.2176,  0.3086,  ...,  0.0514,  0.4584, -0.2046],\n",
      "           [ 0.7449,  1.3570,  1.9109,  ...,  1.0054,  1.3191,  1.2489]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6260,  1.2032,  0.2932,  ..., -0.5573,  1.0664, -0.3722],\n",
      "           [ 1.1954,  0.5234,  0.8784,  ...,  0.4873, -0.0685,  0.1748],\n",
      "           [-0.3626, -0.5292,  0.4104,  ...,  1.3742, -0.1400,  0.2516],\n",
      "           ...,\n",
      "           [-0.6292, -1.3695,  0.5621,  ...,  0.0743,  0.7524,  1.2851],\n",
      "           [ 0.4468, -0.0105, -0.3721,  ..., -0.5331,  1.9898, -2.0609],\n",
      "           [ 1.8548,  1.0062, -0.2247,  ...,  0.8924, -0.6379,  0.3553]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8572,  1.4545, -0.6947,  ...,  0.0427,  1.8423,  1.7561],\n",
      "           [ 1.7897, -0.5603,  1.7971,  ..., -0.6534,  0.3720, -1.7416],\n",
      "           [-0.4871, -1.8508,  0.4083,  ..., -0.2615, -1.3328,  0.5155],\n",
      "           ...,\n",
      "           [ 1.4493, -0.9516, -1.7303,  ..., -0.4642, -1.7145, -0.2188],\n",
      "           [-0.7298, -1.3503, -1.0018,  ..., -1.6219, -2.9561, -0.9534],\n",
      "           [ 0.3119, -0.4668, -0.8588,  ..., -0.7102, -0.3804, -0.6894]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-1.4506,  0.3613,  1.2487,  ..., -0.2229, -0.3195, -0.2012],\n",
      "           [ 0.9740,  2.6503,  0.5181,  ...,  0.1828,  1.1801,  0.3299],\n",
      "           [ 1.2372, -0.0963,  0.3580,  ..., -0.0170, -0.6358,  0.0167],\n",
      "           ...,\n",
      "           [-0.6401, -0.7765, -2.2386,  ..., -0.1659, -1.2065,  0.8353],\n",
      "           [ 0.4499, -1.4612,  0.4947,  ...,  0.3640, -1.4401, -0.1385],\n",
      "           [-0.1541,  0.1852, -1.1065,  ..., -1.4536, -0.2844, -1.3673]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6850,  0.3630, -1.7083,  ...,  0.9061,  0.9264, -0.5250],\n",
      "           [ 0.7308,  0.6390,  0.5444,  ...,  1.5371, -0.7575,  1.2480],\n",
      "           [-0.5797, -0.3179,  0.0875,  ...,  0.2755,  1.5627,  0.1229],\n",
      "           ...,\n",
      "           [-1.6069,  1.2795,  0.5975,  ...,  0.3407, -1.9328, -0.4754],\n",
      "           [-1.2728, -0.6709,  1.5220,  ..., -0.9628, -0.9461,  0.8606],\n",
      "           [-0.0265, -0.2690, -1.0393,  ..., -1.1176, -0.5660, -0.2926]]],\n",
      "\n",
      "\n",
      "         [[[-1.5744,  1.0351, -1.0788,  ..., -0.1050,  0.6315, -0.5564],\n",
      "           [ 1.7211, -0.2748,  0.4046,  ..., -0.0770,  0.0875, -0.0074],\n",
      "           [ 0.5595,  1.2704, -0.5623,  ..., -0.8577, -0.2059,  0.1703],\n",
      "           ...,\n",
      "           [-1.4890, -0.2461,  0.0203,  ..., -0.6822, -1.0246, -0.4541],\n",
      "           [-0.1220,  0.3623,  1.2378,  ...,  0.0493, -0.3650,  0.0343],\n",
      "           [ 1.5237, -1.3056, -0.0842,  ..., -0.6306, -1.0749,  0.0107]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.1814,  0.6121, -2.2341,  ...,  0.9173,  0.5739, -0.4329],\n",
      "           [ 0.6148,  1.6145,  1.4256,  ..., -0.4511, -2.2023, -0.5128],\n",
      "           [ 0.5668,  0.0611,  2.8155,  ..., -1.2359,  1.2295, -0.0523],\n",
      "           ...,\n",
      "           [ 0.8087, -1.0125, -0.7286,  ..., -0.9357,  0.8292, -0.0279],\n",
      "           [ 1.0414,  0.2096, -0.7459,  ..., -0.0192, -0.0624,  0.3367],\n",
      "           [-1.9155,  0.2339,  1.0119,  ...,  0.8492,  1.3495, -1.1402]]],\n",
      "\n",
      "\n",
      "         [[[-0.6332, -0.3533,  0.8215,  ...,  0.1258,  0.6207, -2.3895],\n",
      "           [ 0.0964,  0.5882,  1.8705,  ...,  0.4324,  1.2017,  0.6168],\n",
      "           [-1.0609,  0.2791,  0.7330,  ...,  0.1578, -0.3278, -0.2682],\n",
      "           ...,\n",
      "           [ 0.8053,  0.6905,  0.3366,  ...,  0.8468,  0.7265, -0.0303],\n",
      "           [-0.6066,  0.4075, -0.1315,  ...,  1.6438, -1.0121,  0.7099],\n",
      "           [ 1.3492, -0.7998, -0.0296,  ..., -0.9924,  0.1259,  1.0337]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6238, -1.2184, -0.9837,  ...,  0.2039,  0.3901, -0.1187],\n",
      "           [-0.5543, -0.2846, -0.2386,  ...,  1.2906, -0.2649,  0.0723],\n",
      "           [ 1.8604, -0.4937, -0.4422,  ...,  0.5759, -0.6466, -1.7148],\n",
      "           ...,\n",
      "           [ 1.6472, -0.2391,  1.1134,  ...,  1.3392, -1.1815, -0.4713],\n",
      "           [ 0.3866,  0.2253, -1.0536,  ...,  0.3321, -0.0353,  0.3113],\n",
      "           [-0.0740, -0.6505,  0.0755,  ..., -0.3583, -0.0081,  1.7803]]]]]), tensor([]), tensor([939.8582]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-2.0815, -0.1759,  0.6765,  ..., -0.9007, -0.9911, -0.8534],\n",
      "           [ 0.5957,  2.4587,  0.1823,  ..., -0.0054,  1.0131,  0.0931],\n",
      "           [ 0.8687, -0.5320, -0.0979,  ..., -0.3985, -0.9925, -0.3019],\n",
      "           ...,\n",
      "           [-0.4105, -1.1392, -2.4087,  ...,  0.2389, -0.6580,  1.3930],\n",
      "           [ 0.5766, -1.4198,  0.5231,  ...,  0.8785, -1.1256,  0.3783],\n",
      "           [-0.1474,  0.2428, -1.1183,  ..., -1.0472,  0.1656, -1.1165]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0554, -0.3749, -2.5643,  ...,  0.2694,  0.3076, -1.1800],\n",
      "           [ 0.1772,  0.0712, -0.1616,  ...,  1.0453, -1.5310,  0.8349],\n",
      "           [-1.6117, -1.4269, -1.1058,  ..., -0.3167,  1.0113, -0.3244],\n",
      "           ...,\n",
      "           [-1.4263,  1.3425,  0.6415,  ...,  0.3116, -2.0448, -0.2323],\n",
      "           [-1.5914, -0.0882,  1.2863,  ..., -1.3397, -0.9064,  0.9586],\n",
      "           [ 0.3058, -0.1805, -1.2974,  ..., -1.1136, -0.7417, -0.5521]]],\n",
      "\n",
      "\n",
      "         [[[-0.8691,  1.7709, -0.0832,  ...,  0.9551,  1.7585,  0.3627],\n",
      "           [ 2.4581,  0.8144,  1.5517,  ...,  1.0128,  1.1776,  0.9547],\n",
      "           [ 1.4380,  2.3716,  0.4569,  ...,  0.0317,  0.6683,  0.9975],\n",
      "           ...,\n",
      "           [-1.7406,  0.0136,  0.4371,  ..., -0.4575, -1.4731, -1.1656],\n",
      "           [-0.3804,  0.4790,  1.2877,  ..., -0.0254, -0.9088, -0.4412],\n",
      "           [ 1.8500, -1.1146,  0.1693,  ..., -1.5165, -1.4939, -0.1871]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.2912,  0.6439, -2.1010,  ...,  1.2816,  0.5841, -0.6548],\n",
      "           [-0.1276,  1.2611,  0.7892,  ..., -1.2274, -3.3218, -1.1686],\n",
      "           [ 1.1387,  0.9001,  3.5227,  ..., -0.9741,  1.6592,  0.1650],\n",
      "           ...,\n",
      "           [ 1.3263, -0.5955, -0.4027,  ..., -0.8809,  1.3532,  0.2461],\n",
      "           [ 1.4674,  1.2405, -0.6831,  ...,  0.1903,  0.3572,  0.2400],\n",
      "           [-0.8742,  1.1645,  1.1588,  ...,  1.2061,  1.2635, -1.0088]]],\n",
      "\n",
      "\n",
      "         [[[-0.5673, -0.5053,  1.1249,  ...,  0.3433,  0.9808, -2.2986],\n",
      "           [ 0.2480,  0.8380,  2.1260,  ...,  0.5144,  1.4705,  1.0098],\n",
      "           [-0.9905,  0.4780,  0.9732,  ...,  0.1154, -0.4500, -0.3020],\n",
      "           ...,\n",
      "           [ 1.2716,  0.9687,  0.5571,  ...,  1.2328,  1.0231,  0.2495],\n",
      "           [-0.5991,  0.9915, -0.0354,  ...,  1.8570, -0.9307,  0.8947],\n",
      "           [ 1.8815, -0.7691,  0.2041,  ..., -0.8969,  0.1395,  1.2279]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8389, -1.2355, -0.6495,  ...,  0.5900,  0.9390,  0.2901],\n",
      "           [-0.6725,  0.0191,  0.0206,  ...,  1.5299, -0.0057,  0.3868],\n",
      "           [ 1.7929, -0.5698, -0.4651,  ...,  0.7548, -0.5657, -1.7082],\n",
      "           ...,\n",
      "           [ 1.1332, -0.3150,  0.9970,  ...,  1.1778, -2.3889, -1.3425],\n",
      "           [-0.4203, -0.4852, -1.5109,  ..., -0.2617, -1.1921, -0.3642],\n",
      "           [-0.5672, -1.4090, -0.3790,  ..., -1.0375, -0.7048,  1.5667]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.6533,  0.2798,  0.7030,  ...,  0.5287,  0.2212,  0.1397],\n",
      "           [ 0.9585,  0.3740,  0.3121,  ...,  0.2928,  0.1332,  0.7322],\n",
      "           [ 0.4815,  0.7962,  0.0293,  ..., -0.0853,  0.4661,  0.2445],\n",
      "           ...,\n",
      "           [-0.5158, -0.0179,  0.0800,  ..., -0.5811, -0.7897, -0.4555],\n",
      "           [-0.2859, -0.1856,  0.1579,  ..., -0.6799, -0.1445,  0.1683],\n",
      "           [-0.1410,  0.6140, -0.0176,  ..., -0.5098, -0.3936, -0.3467]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4941,  0.3146,  0.6311,  ...,  0.4771,  0.1394,  0.8831],\n",
      "           [ 0.9706,  0.1819,  0.4554,  ...,  0.9926,  0.8632,  1.2390],\n",
      "           [ 0.6440,  0.9850,  0.7473,  ...,  0.5464,  0.6753,  0.7759],\n",
      "           ...,\n",
      "           [ 0.0815,  0.5243, -0.0931,  ...,  0.0018,  0.6800,  0.1260],\n",
      "           [ 0.4052, -0.2299,  0.3266,  ..., -0.2251,  0.0125,  0.3037],\n",
      "           [ 0.1150,  0.2765,  0.5569,  ...,  0.7371,  0.7983,  0.3662]]],\n",
      "\n",
      "\n",
      "         [[[-0.3870, -1.2033, -0.2597,  ..., -0.3650, -0.3861, -0.1725],\n",
      "           [-0.4705, -0.7551, -0.4947,  ..., -0.7978, -0.4059, -0.5052],\n",
      "           [-0.5138, -1.2558, -0.8953,  ..., -0.9394, -0.5238, -0.6182],\n",
      "           ...,\n",
      "           [ 0.6117,  0.0339, -0.2221,  ...,  0.0725,  0.4778,  0.8399],\n",
      "           [-0.0920, -0.1375, -0.0036,  ...,  0.1462,  0.3876,  0.9224],\n",
      "           [-0.4941, -0.2396,  0.0880,  ...,  0.3655,  0.0419,  0.1548]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.3557, -0.2188, -0.3022,  ..., -0.0535,  0.2933,  0.3906],\n",
      "           [ 0.7458,  0.1392,  0.5103,  ...,  0.8880,  0.2716,  0.4529],\n",
      "           [-0.5344, -0.9023, -0.8682,  ..., -0.1309,  0.0889, -0.4308],\n",
      "           ...,\n",
      "           [-0.1681, -0.3855, -0.2753,  ..., -0.2308, -0.4484, -0.8519],\n",
      "           [-0.1874, -1.1233,  0.2499,  ..., -0.8958, -0.3790, -0.1570],\n",
      "           [-0.8101, -0.4193,  0.3225,  ..., -0.3375, -0.3975, -0.4366]]],\n",
      "\n",
      "\n",
      "         [[[-0.2180, -0.1513, -0.8161,  ..., -0.6749,  0.2342, -0.0526],\n",
      "           [ 0.3552,  0.3862, -0.6899,  ..., -0.0642, -0.3546, -0.2696],\n",
      "           [ 0.1200, -0.4231, -0.1433,  ..., -0.0864, -0.0246, -0.1376],\n",
      "           ...,\n",
      "           [-0.3435, -0.4816,  0.0080,  ..., -0.3187,  0.2415, -0.2898],\n",
      "           [-0.1242, -0.4244, -0.3031,  ..., -0.5400,  0.1377, -0.0953],\n",
      "           [-0.7913, -0.2316,  0.1061,  ..., -0.5070,  0.0046, -0.0925]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0169, -0.4424, -0.6752,  ...,  0.1204, -0.4639, -0.5348],\n",
      "           [-0.1747,  0.0685,  0.0156,  ..., -0.2571, -0.0619, -0.3468],\n",
      "           [-0.0755, -0.1781, -0.1589,  ...,  0.2093,  0.1733, -0.1241],\n",
      "           ...,\n",
      "           [ 0.4119, -0.0162,  0.4749,  ...,  0.0308,  0.6127,  0.1815],\n",
      "           [ 0.7961,  0.2924, -0.1703,  ...,  0.5843,  1.2397,  0.6787],\n",
      "           [ 0.6467,  0.4766,  0.4618,  ...,  1.3226,  0.0812,  0.3896]]]]]), tensor([]), tensor([285.4871]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.3100, -0.8441,  0.2864,  ..., -0.1102, -1.2298, -1.3419],\n",
      "           [ 1.7133,  0.0890,  0.0995,  ...,  0.3752, -0.0770,  1.6537],\n",
      "           [ 0.3442,  1.3718, -1.4376,  ..., -1.2359,  0.7257, -0.2201],\n",
      "           ...,\n",
      "           [-0.7954, -0.8737,  0.1570,  ..., -0.8197, -0.5074,  0.4598],\n",
      "           [-0.6952, -0.0579,  0.5501,  ..., -0.4033,  1.4058,  2.0516],\n",
      "           [-0.6002,  2.4859,  0.2146,  ...,  0.0529,  0.1918,  0.1091]]],\n",
      "\n",
      "\n",
      "         [[[-0.5172, -1.2248,  0.3643,  ..., -0.9157, -1.4575,  1.2548],\n",
      "           [ 1.6619, -0.6807, -0.3326,  ...,  1.3172,  1.2863,  2.9868],\n",
      "           [-0.6476, -0.2011, -1.4137,  ..., -0.6025,  0.0278,  1.0957],\n",
      "           ...,\n",
      "           [ 0.8649,  1.5909, -0.5247,  ...,  0.2938,  1.8495,  1.0150],\n",
      "           [ 0.4182,  1.0769,  0.4489,  ..., -0.3423, -0.8169,  2.2991],\n",
      "           [ 1.3170,  1.3471,  1.2441,  ...,  0.7255,  1.2825,  0.2426]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4460, -1.9008,  2.7520,  ...,  2.1809,  2.4524,  2.7335],\n",
      "           [ 0.4704,  1.1569,  2.2306,  ...,  0.9797,  2.3506,  1.8304],\n",
      "           [ 1.1466, -1.2114,  0.5572,  ..., -0.1799,  1.3580,  0.5701],\n",
      "           ...,\n",
      "           [ 1.6857,  0.8892,  0.5017,  ...,  0.7510,  0.5046,  1.4536],\n",
      "           [-1.1784, -0.1575, -0.1208,  ...,  0.1414, -0.8820,  0.7846],\n",
      "           [-1.0125,  0.0794,  1.1246,  ..., -0.9050, -0.7337,  0.3930]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.6765, -0.4961,  0.0281,  ...,  0.5126,  1.1729,  1.3535],\n",
      "           [-0.6255, -1.0103, -0.1897,  ..., -0.1092, -1.7149, -0.4552],\n",
      "           [-0.4700, -0.8927, -1.7079,  ...,  0.2633,  0.9451, -0.9236],\n",
      "           ...,\n",
      "           [ 0.9620,  0.5063,  0.3975,  ..., -0.4034,  0.1146, -0.6941],\n",
      "           [ 0.4807, -1.0545,  1.0443,  ..., -2.3755,  0.4694,  1.9196],\n",
      "           [ 0.6082,  1.5404,  1.4892,  ..., -0.7370,  0.8715,  0.4124]]],\n",
      "\n",
      "\n",
      "         [[[-0.3327, -0.7693, -1.8204,  ..., -1.8325,  1.9212,  0.7223],\n",
      "           [ 1.7585,  2.2596, -1.6486,  ..., -0.1527, -0.6139, -0.4540],\n",
      "           [ 0.9721, -0.6847,  0.0158,  ..., -0.5091, -0.2915, -0.1115],\n",
      "           ...,\n",
      "           [ 0.0345, -0.9458,  0.7420,  ...,  0.1342,  0.5082, -0.9136],\n",
      "           [-0.2504,  0.2455, -0.9107,  ..., -0.7702,  0.2362,  0.1038],\n",
      "           [-1.1793, -0.3922,  1.2464,  ..., -1.6009,  0.0361, -0.1710]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6499, -1.1186, -1.0936,  ...,  1.7067, -0.1441, -0.4399],\n",
      "           [-0.8266,  1.3919,  1.2783,  ..., -0.6716,  0.6804, -0.3694],\n",
      "           [-0.7598, -0.6385, -0.4936,  ...,  0.6260,  0.3668, -0.3545],\n",
      "           ...,\n",
      "           [-0.3996, -0.3153,  0.8733,  ...,  0.1818, -1.6586, -1.9300],\n",
      "           [ 0.3624, -1.2265, -1.8078,  ...,  0.5946,  0.5990, -0.4533],\n",
      "           [ 0.6424, -0.5077,  0.2215,  ...,  1.8418, -2.8072, -1.1391]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 9.6721e-01, -7.6490e-01, -8.8826e-02,  ...,  1.0243e+00,\n",
      "             1.0672e+00,  1.8495e-01],\n",
      "           [ 9.4574e-01, -3.2921e-01,  1.8924e-01,  ...,  1.0397e-01,\n",
      "             6.9937e-01, -1.2165e+00],\n",
      "           [-1.3309e-01,  2.6763e-01,  1.5157e-01,  ..., -2.2166e-01,\n",
      "             1.8286e-01,  8.5077e-01],\n",
      "           ...,\n",
      "           [-9.0154e-01,  3.2157e-01, -7.7243e-01,  ..., -8.3983e-01,\n",
      "             3.7518e-01, -4.9008e-01],\n",
      "           [-1.1670e+00,  3.5718e-02,  3.7220e-01,  ...,  5.9534e-01,\n",
      "            -3.6941e-01,  2.4847e-01],\n",
      "           [-3.3708e-01, -2.0150e-01,  1.2112e+00,  ..., -8.2353e-01,\n",
      "             3.6899e-01,  1.0656e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8084e-01,  1.3843e+00, -5.0232e-01,  ..., -1.2734e-01,\n",
      "             4.1519e-01, -1.2477e-01],\n",
      "           [ 1.4905e-01, -1.2628e+00,  7.6762e-01,  ...,  2.2607e+00,\n",
      "            -1.7738e-03, -1.6049e+00],\n",
      "           [-1.4346e+00,  9.2262e-01, -7.4508e-01,  ...,  2.0535e-01,\n",
      "            -2.7612e-01,  7.9072e-01],\n",
      "           ...,\n",
      "           [-6.4033e-01,  1.3607e+00, -2.3485e-01,  ..., -6.8786e-02,\n",
      "            -9.0063e-01,  2.2113e-01],\n",
      "           [ 1.4382e+00,  5.7322e-01, -2.5518e-01,  ..., -6.0317e-01,\n",
      "            -6.1277e-01,  2.0471e-01],\n",
      "           [ 1.1675e+00,  9.1214e-01,  2.1396e-01,  ...,  1.3004e+00,\n",
      "            -8.8024e-01, -4.4945e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0745e+00,  8.8083e-01,  3.6648e-01,  ..., -4.7960e-01,\n",
      "            -7.7057e-01,  6.1660e-01],\n",
      "           [-2.7185e+00,  2.9428e-01, -1.8751e+00,  ...,  1.1449e-01,\n",
      "            -4.0164e-01,  3.9750e-01],\n",
      "           [ 1.5912e-01, -5.6382e-01, -1.1970e+00,  ..., -1.3066e+00,\n",
      "            -6.2858e-01, -1.9530e-01],\n",
      "           ...,\n",
      "           [ 1.1263e-01,  2.2436e-01,  1.1005e+00,  ...,  2.0469e-01,\n",
      "             2.1134e-02, -3.0001e-01],\n",
      "           [-8.1120e-01, -9.6535e-01, -7.9316e-01,  ...,  1.0568e-01,\n",
      "            -9.0521e-02,  4.4938e-01],\n",
      "           [ 1.0030e+00, -1.6598e-01,  7.6588e-01,  ...,  1.8663e-01,\n",
      "             8.8481e-01, -1.5437e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.4234e+00, -6.7228e-02,  2.3204e-01,  ..., -9.7329e-01,\n",
      "            -2.8786e-02,  3.2204e-01],\n",
      "           [ 1.9778e-01, -1.8205e-01,  1.5975e+00,  ...,  9.7340e-01,\n",
      "             1.0840e+00,  8.3444e-01],\n",
      "           [ 1.5352e+00, -1.1508e-01, -6.1595e-01,  ..., -1.0044e+00,\n",
      "             3.9789e-01, -1.7566e-01],\n",
      "           ...,\n",
      "           [-1.9560e+00,  5.8451e-01, -2.5567e-01,  ...,  7.3409e-01,\n",
      "             3.7272e-02, -3.2023e-01],\n",
      "           [ 4.4597e-01,  8.4487e-01,  8.6975e-01,  ..., -3.6319e-01,\n",
      "            -1.4346e-01,  3.1431e-01],\n",
      "           [ 6.0100e-01, -3.9483e-01,  1.6867e-01,  ...,  3.6421e-01,\n",
      "            -1.5473e+00, -7.5153e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1985e-01,  8.2919e-01, -2.1189e+00,  ...,  1.6008e-01,\n",
      "            -3.8565e-01, -5.0054e-02],\n",
      "           [ 6.4353e-01, -1.0465e+00, -1.1396e+00,  ...,  1.9851e+00,\n",
      "            -8.0306e-01, -1.3118e+00],\n",
      "           [-2.6061e-03, -1.0103e-04,  2.8460e-01,  ...,  2.2599e-02,\n",
      "            -1.8789e-01, -1.0198e+00],\n",
      "           ...,\n",
      "           [ 1.5722e+00, -9.7217e-01, -3.2397e-01,  ...,  1.8730e-01,\n",
      "            -8.7316e-01, -8.0266e-04],\n",
      "           [-5.1728e-01, -3.4885e-02,  1.3481e+00,  ..., -3.2466e-01,\n",
      "            -1.1952e-01,  1.6156e+00],\n",
      "           [-7.7096e-01,  1.1319e+00,  9.5653e-01,  ..., -4.9057e-02,\n",
      "             2.3128e-01,  2.9127e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.5484e-01,  1.5121e+00, -4.3580e-01,  ..., -6.2276e-01,\n",
      "            -1.3613e+00, -4.3999e-01],\n",
      "           [ 1.4753e+00,  1.1103e+00,  2.2007e-01,  ...,  1.5625e+00,\n",
      "            -5.9964e-01, -1.4418e-02],\n",
      "           [ 7.0759e-01, -1.0582e+00, -5.0074e-02,  ..., -5.6726e-01,\n",
      "            -2.0447e-02,  6.6921e-01],\n",
      "           ...,\n",
      "           [ 1.0727e+00,  6.5322e-01,  2.6205e-02,  ..., -5.9628e-01,\n",
      "             1.0240e+00,  6.3956e-01],\n",
      "           [ 1.1164e+00,  1.0750e+00, -1.3743e-01,  ...,  3.3456e-01,\n",
      "             5.0109e-01, -2.1544e-01],\n",
      "           [ 2.3122e-01,  7.8725e-01,  1.3395e+00,  ..., -1.8780e-01,\n",
      "             2.8532e-01,  6.3899e-03]]]]]), tensor([]), tensor([780.4852]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.5881, -1.6547, -0.9120,  ...,  0.5797,  0.5421, -0.5310],\n",
      "           [ 0.6660, -0.8650, -0.2612,  ..., -0.0674,  0.6215, -1.8838],\n",
      "           [-0.6850, -0.1936, -0.3773,  ..., -0.7217, -0.1140,  0.7151],\n",
      "           ...,\n",
      "           [-0.8336,  0.0461, -1.0267,  ..., -0.5947,  1.3323,  0.0074],\n",
      "           [-1.3757,  0.2037,  0.4708,  ...,  1.3342,  0.1275,  0.8159],\n",
      "           [-0.4139, -0.1983,  1.6362,  ..., -0.4934,  1.0991,  1.7198]]],\n",
      "\n",
      "\n",
      "         [[[-0.5701,  0.8458, -1.5375,  ..., -1.0000, -0.2790, -0.9285],\n",
      "           [-0.4788, -2.2407,  0.1297,  ...,  2.1628, -0.8224, -2.6600],\n",
      "           [-3.0122, -0.0205, -2.2293,  ..., -0.4454, -1.0750,  0.5055],\n",
      "           ...,\n",
      "           [-0.4804,  1.7427, -0.2834,  ..., -0.0496, -0.8352,  0.8588],\n",
      "           [ 1.5608,  1.4817, -0.7342,  ..., -1.4126, -0.9005,  0.2038],\n",
      "           [ 1.9037,  1.2993,  0.0032,  ...,  1.5259, -1.4059, -0.7890]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3716,  1.9246,  1.7533,  ...,  0.7253,  0.3535,  1.9976],\n",
      "           [-2.7363,  1.7030, -1.0978,  ...,  1.4654,  0.9010,  1.7613],\n",
      "           [ 1.2195,  0.5409, -0.2153,  ..., -0.5398,  0.2293,  0.6685],\n",
      "           ...,\n",
      "           [-0.0424,  0.6113,  1.9031,  ...,  0.7047, -0.5402, -1.2479],\n",
      "           [-1.3412, -1.1065, -1.0390,  ...,  0.1041, -0.7262,  0.1843],\n",
      "           [ 1.5616,  0.1337,  1.3111,  ..., -0.5972,  1.0852, -0.7206]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.7199, -0.1053,  0.6098,  ..., -0.9151, -0.1389,  0.1113],\n",
      "           [-0.7257, -0.7865,  1.2832,  ...,  0.4118,  0.2135,  0.1603],\n",
      "           [ 2.5513,  0.9212,  0.1748,  ..., -0.7829,  1.0037,  0.0440],\n",
      "           ...,\n",
      "           [-1.9612,  1.3692,  0.1825,  ...,  0.8054,  0.3089,  0.1131],\n",
      "           [ 0.9889,  2.2625,  1.2478,  ..., -0.5799, -0.0635,  0.5356],\n",
      "           [ 2.1557,  0.5780,  0.2458,  ...,  0.4631, -1.8537, -0.7729]]],\n",
      "\n",
      "\n",
      "         [[[-0.0272,  0.9139, -2.4080,  ...,  0.4517, -0.1419,  0.2306],\n",
      "           [ 1.0104, -1.0713, -1.2834,  ...,  2.6374, -0.8133, -1.3557],\n",
      "           [ 0.2332,  0.2639,  0.5949,  ..., -0.0507, -0.3772, -1.3198],\n",
      "           ...,\n",
      "           [ 2.5049, -0.9565, -0.1623,  ...,  0.6303, -0.8770,  0.3591],\n",
      "           [-0.6078,  0.6148,  1.8290,  ..., -0.2920, -0.2651,  2.1311],\n",
      "           [-0.4479,  1.5429,  1.4518,  ...,  0.1459,  0.3448,  0.2676]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0598,  1.9994, -0.0621,  ..., -0.2586, -1.0693, -0.0088],\n",
      "           [ 1.8012,  1.8095,  0.6270,  ...,  2.2050, -0.4766,  0.2958],\n",
      "           [ 0.7207, -1.3386, -0.1117,  ..., -0.5854,  0.0707,  0.9575],\n",
      "           ...,\n",
      "           [ 0.6246,  0.7789, -0.2027,  ..., -1.0135, -0.1983, -0.2514],\n",
      "           [ 0.4461,  0.5098, -0.6542,  ..., -0.2917, -0.5788, -1.3297],\n",
      "           [-0.3034,  0.1737,  1.1790,  ..., -1.0403, -0.6186, -0.7495]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.4335,  0.3416,  0.3168,  ...,  0.5191,  0.4629,  0.4684],\n",
      "           [ 0.2927,  0.3418,  0.0278,  ...,  0.0796,  0.3621,  0.3586],\n",
      "           [ 0.1578,  0.5581,  0.6823,  ...,  0.2390, -0.0398,  0.2004],\n",
      "           ...,\n",
      "           [-0.2273,  0.2417, -0.1703,  ..., -0.3981, -0.5973, -0.2190],\n",
      "           [ 0.0022, -0.1607, -0.0840,  ..., -0.6459, -0.4486, -0.3645],\n",
      "           [ 0.0044, -0.2821,  0.1194,  ..., -0.3280, -0.3138, -0.2104]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3342,  0.7127,  0.5719,  ...,  0.4196,  0.4885,  0.3319],\n",
      "           [ 0.5703,  0.3477,  0.5639,  ...,  0.8399,  0.5027,  0.5300],\n",
      "           [ 0.5090,  0.8361,  1.1079,  ...,  0.7003,  0.4422,  0.4145],\n",
      "           ...,\n",
      "           [-0.2938, -0.0626, -0.2503,  ..., -0.3561, -0.0397, -0.3120],\n",
      "           [ 0.2517, -0.6268,  0.0718,  ..., -0.1634,  0.1804, -0.1147],\n",
      "           [-0.1814, -0.0062,  0.0240,  ...,  0.3183,  0.4870,  0.4050]]],\n",
      "\n",
      "\n",
      "         [[[-0.6782, -0.3924, -1.0104,  ..., -0.7507, -0.9992, -0.6092],\n",
      "           [-0.4777, -0.9652, -1.0368,  ..., -0.7917, -1.0617, -0.7565],\n",
      "           [-0.6654, -0.8752, -0.9360,  ..., -0.7399, -0.7693, -0.7438],\n",
      "           ...,\n",
      "           [-0.0331, -0.1941, -0.4246,  ...,  0.3179,  0.6741,  0.6603],\n",
      "           [ 0.3650, -0.0308, -0.0150,  ..., -0.0190,  0.5073,  0.3123],\n",
      "           [-0.0127, -0.0119, -0.3447,  ...,  0.3222,  0.3267,  0.2900]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.0832,  0.0087,  0.1966,  ...,  0.0265,  0.1533,  0.1792],\n",
      "           [ 0.7122,  0.2597,  0.4442,  ...,  0.8292,  0.6334,  0.5787],\n",
      "           [-0.3366, -0.4965, -0.3209,  ..., -0.1094, -0.3871, -0.4309],\n",
      "           ...,\n",
      "           [-0.5045, -0.4684, -0.3005,  ..., -0.1877, -0.0328, -0.3829],\n",
      "           [-0.4909, -0.8838, -0.0376,  ...,  0.1550, -0.1187,  0.0853],\n",
      "           [-1.1605, -0.5396, -0.2959,  ...,  0.2440, -0.1394,  0.0719]]],\n",
      "\n",
      "\n",
      "         [[[-0.2653,  0.1809, -0.1534,  ..., -0.2632, -0.4960, -0.4380],\n",
      "           [-0.0100, -0.3761, -0.1259,  ..., -0.0431, -0.1531, -0.2142],\n",
      "           [-0.0160, -0.1633, -0.1586,  ...,  0.0190,  0.0351,  0.0570],\n",
      "           ...,\n",
      "           [-0.4110, -0.0544, -0.0642,  ..., -0.4270, -0.1108, -0.0496],\n",
      "           [ 0.0637, -0.4084, -0.1209,  ..., -0.4047,  0.1097, -0.0129],\n",
      "           [-0.5394, -0.3787, -0.2605,  ..., -0.1666,  0.0745,  0.1948]]],\n",
      "\n",
      "\n",
      "         [[[-0.2369, -0.0593, -0.3665,  ..., -0.4502, -0.5245, -0.4052],\n",
      "           [ 0.1284, -0.1299, -0.0852,  ...,  0.0085, -0.2789, -0.2180],\n",
      "           [ 0.2406, -0.1431,  0.0146,  ..., -0.0565, -0.1398, -0.1678],\n",
      "           ...,\n",
      "           [ 0.5234,  0.0105, -0.0645,  ...,  0.6941,  0.7305,  0.7090],\n",
      "           [ 0.6145,  0.5550,  0.1991,  ...,  0.6310,  0.7530,  0.6315],\n",
      "           [ 0.3450,  0.7088,  0.3303,  ...,  0.4143,  0.2427,  0.6282]]]]]), tensor([]), tensor([115.3661]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.7323, -1.2153, -2.5902,  ..., -0.7122, -1.2564, -0.7358],\n",
      "           [-1.2802,  0.4401, -2.3427,  ..., -0.7062,  1.6818,  1.1165],\n",
      "           [-2.1240,  0.7161,  2.1162,  ...,  0.2912, -2.1322, -1.0986],\n",
      "           ...,\n",
      "           [ 0.4846, -0.3680, -1.4377,  ...,  0.7251,  0.0176,  2.9512],\n",
      "           [ 0.9522, -0.3809, -0.8579,  ..., -0.6846,  0.6693,  0.0888],\n",
      "           [ 0.1714, -1.8778,  1.5578,  ...,  1.7208,  0.2672,  0.2472]]],\n",
      "\n",
      "\n",
      "         [[[-2.5141,  0.4168, -0.8857,  ..., -2.0862, -0.1853, -0.9628],\n",
      "           [ 0.1675, -1.5916, -0.0309,  ...,  0.6932, -1.9135,  0.0790],\n",
      "           [-2.4122, -1.1265,  0.2382,  ..., -0.2721, -0.8538,  0.1181],\n",
      "           ...,\n",
      "           [ 0.4253, -0.6444, -1.5254,  ...,  0.1500, -0.1786, -0.7162],\n",
      "           [ 0.5191, -0.0560, -1.3789,  ..., -0.8322, -0.0372, -0.9706],\n",
      "           [ 0.6881,  0.6020, -1.6745,  ...,  0.9903, -0.7944,  0.5532]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6733,  1.9763,  0.8421,  ...,  1.8060,  0.5140,  2.9660],\n",
      "           [ 1.2986,  0.8168, -0.7770,  ...,  1.8380,  0.0137,  1.9058],\n",
      "           [ 2.1000,  0.5042,  0.8361,  ...,  1.2955,  0.7886,  0.2141],\n",
      "           ...,\n",
      "           [-1.4556,  0.1905, -0.4662,  ...,  0.5309, -0.9542, -0.1333],\n",
      "           [ 0.6475,  0.5679, -0.1589,  ..., -0.6989,  0.8365, -1.7778],\n",
      "           [ 1.0813,  1.9974, -1.1202,  ..., -0.0621,  1.0828,  0.3469]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1351, -0.1819,  1.8593,  ...,  0.8219,  0.2925, -0.5472],\n",
      "           [-0.5876, -0.2649,  0.1764,  ...,  0.5817, -1.9515, -1.0686],\n",
      "           [ 0.8947, -0.7494,  0.2921,  ...,  0.3844,  0.1337, -1.9234],\n",
      "           ...,\n",
      "           [-0.0260,  0.3968,  1.0566,  ..., -0.1344,  0.0210,  0.3239],\n",
      "           [-0.5749,  0.3927,  0.1375,  ...,  0.5888, -1.0153,  0.2825],\n",
      "           [-0.0904,  2.9602, -1.2769,  ..., -0.5508, -0.1990,  0.8824]]],\n",
      "\n",
      "\n",
      "         [[[-1.3448,  0.9188, -0.2986,  ..., -0.2432, -1.9202, -1.1926],\n",
      "           [ 0.6361, -1.9954,  0.5559,  ...,  0.5562,  0.7546,  0.3373],\n",
      "           [ 0.7403, -0.5860, -0.7564,  ..., -1.0198, -0.3945,  0.5362],\n",
      "           ...,\n",
      "           [-0.2870,  1.6400,  1.3965,  ...,  0.3717, -0.2229,  1.1686],\n",
      "           [ 1.1589,  0.8080, -0.4397,  ..., -0.5800,  1.1103,  0.5884],\n",
      "           [-0.6627, -2.0331, -0.0754,  ..., -0.0286,  0.8779,  0.9022]]],\n",
      "\n",
      "\n",
      "         [[[-0.9291, -0.1804, -0.0326,  ..., -1.3263, -1.1051,  0.3160],\n",
      "           [ 0.7754,  1.3501,  0.9445,  ...,  1.0543,  0.1000,  0.8955],\n",
      "           [ 0.9181, -2.1648,  0.3421,  ...,  0.2640, -0.8571, -0.9217],\n",
      "           ...,\n",
      "           [-0.7073, -0.2546, -2.2181,  ..., -1.1476, -2.0961, -0.6224],\n",
      "           [-1.1137, -0.9593, -1.5725,  ..., -1.0880, -1.2032,  0.0496],\n",
      "           [-1.0966,  0.4575, -0.8832,  ..., -0.1247, -1.5094, -1.3378]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.0770, -0.0989,  1.0313,  ..., -0.2463, -0.7757,  0.4272],\n",
      "           [ 0.6881,  0.3277, -0.0424,  ...,  0.1932, -0.1008,  0.1645],\n",
      "           [ 0.4464,  0.7184,  0.4356,  ...,  0.1086,  0.2876,  0.3751],\n",
      "           ...,\n",
      "           [-0.4870,  0.5909, -0.9125,  ..., -0.2425, -0.3391, -0.0792],\n",
      "           [-0.4270, -0.3024, -0.2123,  ..., -0.0974,  0.3061, -0.7060],\n",
      "           [-0.6477, -0.4274, -0.6345,  ..., -0.1601, -0.0963, -0.0342]]],\n",
      "\n",
      "\n",
      "         [[[-0.0980,  0.0179,  0.3771,  ..., -0.0917,  0.6847,  0.0034],\n",
      "           [-0.1974, -0.1626, -0.2393,  ...,  0.8480,  0.2651,  0.8514],\n",
      "           [ 0.7667,  0.6968,  1.4129,  ...,  0.0546,  0.3598,  0.4639],\n",
      "           ...,\n",
      "           [-0.3536, -0.5088,  0.6384,  ..., -0.8729, -0.4771, -0.1023],\n",
      "           [ 0.2350, -0.1125, -0.4422,  ...,  0.5947, -0.0387,  0.0734],\n",
      "           [-1.0585, -0.1542,  0.4294,  ...,  0.3704, -0.2066, -0.3800]]],\n",
      "\n",
      "\n",
      "         [[[-0.5979, -0.1078, -0.3451,  ..., -1.1487, -0.8559, -0.2512],\n",
      "           [-0.1265, -0.5013, -1.2627,  ..., -0.0982, -0.9975, -0.8237],\n",
      "           [-1.2321, -0.8998, -0.5621,  ..., -0.9220, -0.4210, -0.8337],\n",
      "           ...,\n",
      "           [ 0.9062,  0.4071,  0.3191,  ...,  0.3831, -0.2235,  0.3686],\n",
      "           [ 0.9442, -0.0446,  0.3299,  ..., -0.1424,  0.1889, -0.4152],\n",
      "           [-0.0295, -0.0727, -0.6388,  ...,  1.0899, -0.3433,  0.0255]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.5779, -0.3517,  0.0078,  ...,  0.1747,  0.0681, -0.2367],\n",
      "           [-0.2348,  0.8680,  1.2024,  ...,  0.7982,  0.7612, -0.1088],\n",
      "           [ 0.0683, -0.8435, -0.3642,  ...,  0.2122, -0.1310,  0.4698],\n",
      "           ...,\n",
      "           [-0.0768, -0.6666, -1.0262,  ..., -0.0299,  0.0076, -0.3572],\n",
      "           [-0.6692, -0.9149,  0.9087,  ..., -0.3896, -0.7504, -0.2905],\n",
      "           [-0.6685, -0.6462, -0.0356,  ...,  0.1091,  0.1146, -0.2683]]],\n",
      "\n",
      "\n",
      "         [[[-0.4077,  0.0564,  0.0987,  ..., -0.1433,  0.1289,  0.5044],\n",
      "           [-0.4789, -0.3530, -0.4062,  ...,  0.0961, -0.0860, -0.1158],\n",
      "           [-0.0889,  0.3181, -0.3189,  ...,  0.1936,  0.3330, -0.0061],\n",
      "           ...,\n",
      "           [-0.1603,  0.1444,  0.1844,  ...,  0.2110,  0.3293, -0.4921],\n",
      "           [ 0.0022, -0.3470, -0.4077,  ..., -0.7256,  0.3031, -0.1476],\n",
      "           [ 0.2032,  0.0255, -0.6395,  ..., -0.3675,  0.1418,  0.1811]]],\n",
      "\n",
      "\n",
      "         [[[-0.2565,  0.4106, -0.5747,  ..., -0.3622, -0.3445,  0.0868],\n",
      "           [-0.1363, -0.4493, -0.2248,  ...,  0.1132,  0.1093, -0.2166],\n",
      "           [ 0.2230,  0.0080, -0.1983,  ..., -0.2080,  0.3093, -0.6015],\n",
      "           ...,\n",
      "           [ 0.6655, -0.3516,  0.4887,  ...,  0.0683,  1.4108,  0.4408],\n",
      "           [ 0.2558,  0.5351, -0.6222,  ...,  0.2644,  0.5816,  0.8463],\n",
      "           [ 0.0804,  1.0944, -0.0252,  ...,  0.6965,  1.0465,  0.0797]]]]]), tensor([]), tensor([393.3347]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.1680, -1.5110,  1.0689,  ..., -2.0565, -3.4077, -0.2319],\n",
      "           [ 0.6082,  0.0527, -0.8119,  ...,  0.0263, -0.6004, -0.2528],\n",
      "           [ 0.1932,  0.7894,  0.0712,  ..., -0.3737,  0.0481,  0.1759],\n",
      "           ...,\n",
      "           [-0.5427,  0.8837, -2.3902,  ...,  0.2438,  0.7732,  1.0708],\n",
      "           [-0.8252, -0.3264, -0.5321,  ...,  0.9999,  2.0758, -0.4956],\n",
      "           [-1.7205, -0.8657, -1.3919,  ...,  0.8810,  0.9469,  0.7581]]],\n",
      "\n",
      "\n",
      "         [[[-1.9049, -1.6286, -0.3821,  ..., -2.0876,  0.3386, -1.3022],\n",
      "           [-1.7087, -1.3073, -2.0720,  ...,  0.5459, -0.5397,  1.1306],\n",
      "           [-0.1743, -0.8115,  0.9382,  ..., -1.5376, -0.7094,  0.1158],\n",
      "           ...,\n",
      "           [-0.4605, -1.4193,  1.5239,  ..., -1.8789, -1.5664,  0.3153],\n",
      "           [-0.0560,  1.0979, -1.6403,  ...,  1.4351, -0.4316,  1.3111],\n",
      "           [-2.0119, -0.1025,  0.5931,  ...,  0.4118, -0.8686, -1.0228]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4396,  1.4078,  1.7833,  ..., -0.3548,  0.5979,  1.8369],\n",
      "           [ 1.3288,  1.5568, -0.4330,  ...,  2.5020,  0.1756,  0.5408],\n",
      "           [-1.1898,  0.0821,  1.2549,  ...,  0.0250,  1.1953, -0.1239],\n",
      "           ...,\n",
      "           [ 1.9769,  1.6320,  1.7358,  ...,  1.3607, -1.7342, -0.5335],\n",
      "           [ 1.7507,  0.1356,  0.7585,  ..., -0.3360, -1.2628, -2.2689],\n",
      "           [ 0.4623,  0.4934, -1.0282,  ...,  0.9193, -1.5230,  0.0730]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.8298, -0.7583,  0.7326,  ...,  1.0538,  0.2575, -0.6398],\n",
      "           [-2.9581,  1.0832,  1.3493,  ..., -0.2535, -0.2027, -1.8070],\n",
      "           [ 1.4042, -0.7725,  0.1637,  ...,  1.1799,  0.3845,  1.7384],\n",
      "           ...,\n",
      "           [ 1.0214, -0.3665, -1.5548,  ..., -0.1348,  0.9192,  1.2271],\n",
      "           [-0.7488, -0.1840,  2.4605,  ..., -0.9338, -0.6243,  1.0198],\n",
      "           [ 0.7883,  0.5408,  0.1501,  ...,  0.8832,  1.6226,  0.6208]]],\n",
      "\n",
      "\n",
      "         [[[-0.6858, -0.0117,  0.9998,  ...,  0.0096,  1.1330,  1.9242],\n",
      "           [-0.8305, -0.2201, -0.4397,  ...,  0.3205,  0.2487,  0.0888],\n",
      "           [ 0.2788,  1.4634, -0.4252,  ...,  0.3613,  0.6787,  0.2602],\n",
      "           ...,\n",
      "           [ 0.4889,  0.8934,  1.0229,  ...,  1.6229,  0.7825, -0.5122],\n",
      "           [ 0.1629,  0.3723, -0.9327,  ..., -0.9543,  0.8335,  0.3291],\n",
      "           [ 1.6683,  0.3661, -0.9879,  ..., -0.6236,  0.5509,  0.8906]]],\n",
      "\n",
      "\n",
      "         [[[-0.2276,  1.3712, -0.4855,  ...,  0.0598,  0.2064,  1.3108],\n",
      "           [-0.5175, -0.4845,  0.2459,  ...,  0.7605,  1.1525,  0.1694],\n",
      "           [-0.0694, -0.0503, -0.4684,  ..., -0.5658,  0.7653, -1.3437],\n",
      "           ...,\n",
      "           [ 0.2953, -1.0824,  0.7106,  ...,  0.3294,  0.4279, -1.5269],\n",
      "           [-1.1461, -0.2754, -2.4603,  ..., -0.0071, -1.5666, -0.0803],\n",
      "           [-0.9771,  1.1997, -1.0661,  ..., -0.3155,  0.2419, -2.1134]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 5.8616e-01,  9.0489e-01,  5.4892e-01,  ..., -6.6457e-01,\n",
      "             7.7315e-01, -3.1551e-01],\n",
      "           [ 2.2364e-02,  1.1344e+00,  4.5239e-01,  ...,  1.2033e+00,\n",
      "             5.9649e-01, -8.7516e-02],\n",
      "           [ 2.6407e-01,  9.4477e-02, -5.2384e-01,  ...,  8.9395e-01,\n",
      "             3.5714e-01,  2.0321e-01],\n",
      "           ...,\n",
      "           [-4.0349e-01,  3.0905e-01, -5.0816e-01,  ..., -1.8300e-01,\n",
      "            -1.1518e-01,  9.9840e-01],\n",
      "           [-1.0827e+00, -9.2271e-01,  4.3974e-01,  ..., -4.2772e-01,\n",
      "            -6.1028e-01, -1.2335e-01],\n",
      "           [-1.8742e-04,  2.9604e-01, -4.9312e-02,  ..., -5.9359e-01,\n",
      "            -3.4949e-01,  9.3341e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 9.0047e-01,  2.8572e-01, -4.8917e-01,  ..., -3.4462e-01,\n",
      "             4.5253e-01,  9.7123e-01],\n",
      "           [ 1.0166e+00,  4.3991e-01,  6.5462e-01,  ...,  3.4067e-01,\n",
      "             9.6862e-02,  6.3910e-01],\n",
      "           [-2.1741e-01,  1.1243e+00,  4.0519e-01,  ...,  8.8448e-01,\n",
      "            -1.5277e-01,  2.6789e-01],\n",
      "           ...,\n",
      "           [ 6.5094e-01,  4.2310e-01, -3.9232e-01,  ..., -1.7364e+00,\n",
      "             4.0707e-01,  1.9376e-01],\n",
      "           [ 4.3313e-02,  4.1818e-01,  2.9251e-01,  ..., -9.0078e-02,\n",
      "             9.9706e-01, -8.7250e-01],\n",
      "           [ 3.2934e-01,  1.3578e-01,  5.1652e-02,  ...,  7.0202e-01,\n",
      "             1.0812e+00, -4.4486e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4643e-01, -8.2455e-02, -4.9771e-01,  ..., -5.2672e-01,\n",
      "            -5.4064e-01,  4.2964e-01],\n",
      "           [-6.9668e-01, -1.1723e+00, -4.5777e-02,  ..., -1.0596e+00,\n",
      "            -1.6370e-01, -6.2666e-01],\n",
      "           [-7.3344e-01,  4.0548e-01, -9.3793e-01,  ..., -7.1572e-01,\n",
      "            -1.3636e+00,  2.0953e-01],\n",
      "           ...,\n",
      "           [-4.5589e-01,  8.5623e-01, -9.3155e-02,  ...,  5.1616e-02,\n",
      "             5.6798e-02,  2.6400e-01],\n",
      "           [ 6.0149e-01,  2.6236e-01,  9.1799e-01,  ...,  1.1512e-01,\n",
      "            -6.4943e-01,  5.3367e-01],\n",
      "           [ 1.6293e-01,  3.3349e-01,  8.9564e-01,  ..., -1.8563e-01,\n",
      "            -2.6701e-01, -5.0190e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.3916e+00, -3.5499e-01,  2.5689e-01,  ..., -3.8501e-01,\n",
      "            -6.6472e-01,  1.4718e-01],\n",
      "           [ 9.3740e-04,  1.0221e+00,  4.4408e-01,  ...,  1.4877e+00,\n",
      "             7.4145e-01,  6.9051e-01],\n",
      "           [ 1.2823e-02, -6.5410e-01, -1.5269e-01,  ...,  3.7926e-02,\n",
      "            -3.0809e-01, -1.3844e-01],\n",
      "           ...,\n",
      "           [-1.2348e+00,  3.1074e-01,  9.9063e-02,  ..., -8.3154e-01,\n",
      "             5.5383e-01,  8.9262e-01],\n",
      "           [-4.0496e-01,  5.3216e-01,  3.5727e-01,  ..., -4.2454e-01,\n",
      "             2.0937e-01,  1.8797e-01],\n",
      "           [-9.1357e-01, -1.2112e+00, -8.2570e-02,  ..., -1.5687e-01,\n",
      "            -1.9588e-01,  6.9411e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4065e-02,  1.4025e-01, -4.4689e-01,  ...,  4.4538e-01,\n",
      "             2.0374e-01, -5.1365e-01],\n",
      "           [-7.7456e-02,  2.6135e-01, -1.1654e-01,  ..., -3.3496e-01,\n",
      "            -1.9618e-02, -6.1617e-02],\n",
      "           [-8.3658e-02,  2.2102e-01, -1.0675e+00,  ...,  1.1888e+00,\n",
      "            -4.5493e-01, -5.9798e-01],\n",
      "           ...,\n",
      "           [ 1.3812e+00, -9.7760e-01,  1.1625e+00,  ..., -1.5428e-01,\n",
      "             1.4871e-01,  5.9382e-01],\n",
      "           [ 4.4790e-01,  9.6248e-01, -6.2052e-01,  ..., -6.7999e-02,\n",
      "             4.0436e-01,  1.2387e+00],\n",
      "           [-2.7849e-01,  1.1212e+00, -2.8023e-01,  ...,  2.7476e-01,\n",
      "            -4.6294e-01,  2.9460e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.3628e-01,  3.5321e-01, -7.1037e-02,  ..., -3.9800e-01,\n",
      "             2.3581e-01,  6.8938e-02],\n",
      "           [ 3.4165e-02,  5.8447e-01,  1.6744e-02,  ..., -5.9087e-01,\n",
      "             7.2027e-01, -1.4044e-01],\n",
      "           [ 4.0312e-01, -3.6848e-02,  4.6547e-01,  ...,  5.0621e-01,\n",
      "            -6.7785e-01, -6.5106e-01],\n",
      "           ...,\n",
      "           [ 6.3398e-01, -2.7847e-02, -9.3890e-01,  ...,  7.9444e-01,\n",
      "             6.9117e-02, -2.2466e-01],\n",
      "           [ 7.5743e-02,  3.3250e-01,  5.4508e-01,  ...,  2.1931e-02,\n",
      "             1.5069e+00,  5.0407e-01],\n",
      "           [ 7.7507e-01,  1.2933e+00,  2.9162e-01,  ...,  6.4101e-02,\n",
      "             8.8096e-02, -8.9277e-01]]]]]), tensor([]), tensor([587.8849]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.1443,  0.7572, -0.0750,  ..., -2.1634,  0.2589, -1.5368],\n",
      "           [-0.6730,  1.4557,  0.3207,  ...,  1.7192,  0.6798, -0.6128],\n",
      "           [-0.1432, -0.4540, -1.5222,  ...,  1.0969,  0.2209, -0.1307],\n",
      "           ...,\n",
      "           [-0.2202,  0.0171, -0.8808,  ...,  0.5113,  0.7013,  2.6823],\n",
      "           [-1.6614, -1.3790,  0.7171,  ...,  0.2576, -0.4375,  0.3168],\n",
      "           [ 0.0159,  0.5964,  0.0120,  ..., -0.1185,  0.0564,  0.5868]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4723, -0.6382, -1.9690,  ..., -1.6646, -0.0715,  0.8285],\n",
      "           [ 0.7119, -0.1405,  0.2095,  ..., -0.7102, -0.9580,  0.1907],\n",
      "           [-1.6182,  0.2322, -1.1997,  ...,  0.3776, -0.9969, -0.3395],\n",
      "           ...,\n",
      "           [ 1.6862,  0.6839, -0.5363,  ..., -2.1614,  0.6402,  0.4799],\n",
      "           [-0.2642,  1.7860,  0.1325,  ..., -0.0849,  1.8091, -1.3415],\n",
      "           [ 1.0144,  0.3510, -0.2806,  ...,  0.9460,  1.2755, -1.3521]]],\n",
      "\n",
      "\n",
      "         [[[ 1.5413,  0.9251,  0.9999,  ...,  0.7337,  0.8211,  2.3176],\n",
      "           [-0.1664, -0.2242,  1.5069,  ..., -0.0818,  1.4388,  0.4961],\n",
      "           [ 0.2456,  2.2666,  0.0060,  ...,  0.3091, -0.8215,  1.6436],\n",
      "           ...,\n",
      "           [-0.9987,  1.8373,  0.4763,  ..., -0.3908, -1.0621, -0.6302],\n",
      "           [ 0.5432,  0.6150,  1.5537,  ...,  0.0910, -2.0215, -0.3069],\n",
      "           [ 0.5099,  0.9855,  1.8941,  ..., -0.9913, -0.8352, -1.1394]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.2019, -0.6771,  0.5753,  ..., -0.4858, -1.0486,  0.0781],\n",
      "           [-1.3376,  1.1612,  0.2794,  ...,  1.3150, -0.0528,  0.2048],\n",
      "           [ 0.7967,  0.0374,  0.6874,  ...,  0.4683, -0.0743,  0.0846],\n",
      "           ...,\n",
      "           [-1.2309,  1.4027,  0.8171,  ..., -1.0883,  1.1945,  2.0052],\n",
      "           [ 0.0241,  2.4437,  0.6926,  ..., -0.7656,  0.9558,  0.7837],\n",
      "           [ 0.3957, -0.5895,  0.1387,  ..., -0.2166, -0.4582,  1.4675]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2601,  0.0788, -0.5664,  ...,  1.1912,  0.8410, -0.3980],\n",
      "           [ 0.0082,  0.7197,  0.0925,  ..., -0.4338,  0.4479,  0.4419],\n",
      "           [ 0.1554,  0.4782, -1.6217,  ...,  1.7738, -0.9231, -1.0087],\n",
      "           ...,\n",
      "           [ 2.9913, -1.2451,  2.3575,  ...,  0.5313,  0.3861,  1.2713],\n",
      "           [ 0.8889,  2.5010, -0.9231,  ...,  0.3714,  0.6924,  2.3540],\n",
      "           [ 0.3303,  2.1544, -0.0424,  ...,  0.7934, -0.8132,  0.1999]]],\n",
      "\n",
      "\n",
      "         [[[-0.8502,  0.6921,  0.5040,  ..., -0.1925,  0.9864,  0.8145],\n",
      "           [-0.0808,  1.4507,  0.3754,  ..., -0.9200,  1.5518,  0.3099],\n",
      "           [ 0.5138, -0.3551,  0.6158,  ...,  1.0830, -1.0369, -1.0242],\n",
      "           ...,\n",
      "           [ 0.0734, -0.1183, -1.9208,  ...,  0.0156, -1.7390, -1.4997],\n",
      "           [-1.1381, -0.5719,  0.2861,  ..., -1.3627,  0.9484, -0.1649],\n",
      "           [ 0.5190,  1.0804, -0.2154,  ..., -0.8229, -0.9379, -2.5276]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.4330,  0.4342,  0.0455,  ...,  0.5864,  0.8213,  0.3684],\n",
      "           [ 0.4746,  0.4207,  0.4964,  ...,  0.1574,  0.2333,  0.2030],\n",
      "           [ 0.6220,  0.4376,  0.2369,  ...,  0.6710,  0.4163,  0.5875],\n",
      "           ...,\n",
      "           [-0.4886,  0.1871,  0.0280,  ..., -0.2074, -0.1416, -0.4267],\n",
      "           [-0.1280, -0.1993, -0.0434,  ..., -0.3917, -0.2751, -0.2232],\n",
      "           [ 0.2908, -0.1840,  0.0540,  ..., -0.2405, -0.4180, -0.2222]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6166,  0.5099,  0.1648,  ...,  0.3983,  0.4139,  0.6314],\n",
      "           [ 0.3089,  0.1242,  0.1079,  ...,  0.7621,  0.5710,  0.3933],\n",
      "           [ 0.8435,  0.9504,  1.0647,  ...,  0.4105,  0.6119,  0.1751],\n",
      "           ...,\n",
      "           [-0.4396,  0.2173, -0.3012,  ..., -0.1912,  0.1580, -0.6868],\n",
      "           [ 0.0971, -0.4768,  0.0459,  ...,  0.0561,  0.0029,  0.0136],\n",
      "           [-0.3986, -0.0397,  0.2556,  ...,  0.1821,  0.1155,  0.3702]]],\n",
      "\n",
      "\n",
      "         [[[-0.0867, -0.8886, -0.6274,  ..., -0.6360, -0.9758, -0.9127],\n",
      "           [-0.5230, -0.9677, -1.1219,  ..., -0.9006, -0.7894, -0.5623],\n",
      "           [-0.6527, -0.8172, -0.7611,  ..., -0.6772, -0.7906, -0.7556],\n",
      "           ...,\n",
      "           [ 0.2667, -0.1701, -0.2059,  ..., -0.0308,  0.3203,  0.7275],\n",
      "           [ 0.0844,  0.0408,  0.0962,  ..., -0.2228,  0.0127,  0.7619],\n",
      "           [-0.2491, -0.0830, -0.0621,  ...,  0.7133,  0.0077,  0.0357]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1375,  0.4319, -0.4796,  ..., -0.0458, -0.0153, -0.1296],\n",
      "           [ 0.5236,  0.3445,  0.7629,  ...,  0.5414,  0.7456,  0.6255],\n",
      "           [-0.2577, -0.5720, -0.0391,  ..., -0.2919, -0.2448, -0.2281],\n",
      "           ...,\n",
      "           [-0.4360, -0.2308, -0.1120,  ..., -0.2109,  0.2036, -0.1837],\n",
      "           [-0.2749, -0.7472, -0.0120,  ..., -0.0212, -0.2170, -0.2609],\n",
      "           [-0.9805, -0.6472, -0.5018,  ..., -0.3083,  0.0215, -0.2237]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0117, -0.1909,  0.1131,  ..., -0.3533, -0.2482, -0.0944],\n",
      "           [ 0.2114,  0.0939,  0.0311,  ...,  0.1727, -0.1521, -0.4962],\n",
      "           [-0.2830,  0.1820,  0.1112,  ...,  0.3247,  0.0835,  0.2904],\n",
      "           ...,\n",
      "           [-0.2560, -0.2315, -0.1005,  ..., -0.1105, -0.1495, -0.1658],\n",
      "           [ 0.0779, -0.1312,  0.2180,  ...,  0.1745, -0.1361, -0.2080],\n",
      "           [-0.6163, -0.2411, -0.3300,  ...,  0.0915,  0.0197, -0.5480]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0440, -0.3896, -0.2830,  ..., -0.3615, -0.7505, -0.4763],\n",
      "           [ 0.0423, -0.4788, -0.0154,  ..., -0.0129, -0.3776, -0.4423],\n",
      "           [ 0.0721, -0.0492, -0.2037,  ..., -0.2228,  0.0637, -0.0862],\n",
      "           ...,\n",
      "           [ 0.4129,  0.0908,  0.1231,  ...,  0.2291,  0.5459,  0.4985],\n",
      "           [ 0.8079,  0.7027,  0.4234,  ...,  0.2786,  0.5100,  0.4561],\n",
      "           [ 0.2281,  0.8534,  0.3575,  ...,  0.3892,  0.6561,  0.2442]]]]]), tensor([]), tensor([200.3222]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-3.6149e-01, -3.3760e-01, -2.7583e+00,  ...,  1.0691e-01,\n",
      "             1.0212e+00, -7.0055e-01],\n",
      "           [ 2.2523e-01,  5.8557e-01,  7.9503e-01,  ..., -2.6630e-01,\n",
      "             6.4226e-01, -2.8173e-01],\n",
      "           [ 9.8808e-01, -6.0375e-02, -7.1496e-01,  ...,  1.6919e+00,\n",
      "             7.9914e-01,  1.3247e+00],\n",
      "           ...,\n",
      "           [-1.0270e+00, -4.3258e-01,  4.3681e-02,  ...,  1.6843e+00,\n",
      "             1.6694e+00,  3.5749e-02],\n",
      "           [ 3.0383e-02, -3.5779e-02, -1.8833e-01,  ...,  8.6879e-01,\n",
      "             1.1961e+00,  1.4163e+00],\n",
      "           [ 1.3187e+00, -6.3029e-01,  5.8237e-01,  ...,  1.0641e+00,\n",
      "             9.1889e-02,  6.8899e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.8650e-01, -8.3445e-01, -2.0878e+00,  ..., -1.5616e+00,\n",
      "            -7.2453e-01,  3.3154e-01],\n",
      "           [-9.2273e-01, -1.8266e+00, -2.3921e+00,  ...,  4.5337e-01,\n",
      "            -6.2739e-03, -5.5197e-01],\n",
      "           [-6.6453e-02, -3.3700e-01, -9.3325e-01,  ..., -1.0612e+00,\n",
      "             5.7437e-01, -1.9227e+00],\n",
      "           ...,\n",
      "           [-6.4297e-01,  1.1922e+00, -1.0692e+00,  ...,  1.1834e+00,\n",
      "             2.6472e-01, -6.5575e-01],\n",
      "           [-6.1242e-01,  3.5482e-01, -1.0498e+00,  ..., -3.2873e-03,\n",
      "            -8.2434e-01, -5.2176e-01],\n",
      "           [-7.9053e-01,  5.5608e-01,  3.0931e-01,  ...,  1.8770e-01,\n",
      "             1.3628e+00,  4.9240e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.4135e+00, -1.2197e+00,  2.1689e+00,  ...,  1.0761e+00,\n",
      "             4.4343e-01, -2.1799e-01],\n",
      "           [ 3.2969e-01,  4.3381e-01,  8.9650e-03,  ...,  1.4070e-01,\n",
      "             1.1243e+00,  1.6101e+00],\n",
      "           [ 9.3291e-01,  7.1739e-01,  1.3334e+00,  ...,  8.4733e-01,\n",
      "             2.4875e-01, -1.8653e-01],\n",
      "           ...,\n",
      "           [ 6.7331e-01,  3.2625e-01,  7.7483e-01,  ..., -4.2667e-01,\n",
      "            -1.0118e+00, -6.8687e-01],\n",
      "           [-9.4235e-01,  6.9503e-01,  3.2297e-01,  ..., -1.3145e+00,\n",
      "            -8.4059e-01,  6.2707e-01],\n",
      "           [-3.0580e-01,  9.4611e-01,  9.9982e-01,  ...,  1.7800e+00,\n",
      "            -6.7042e-01,  5.4075e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-6.4953e-01,  1.9037e+00, -8.4793e-01,  ...,  4.3662e-01,\n",
      "            -5.8582e-01, -1.4038e+00],\n",
      "           [-1.5122e+00, -3.4760e-01,  1.0234e+00,  ..., -9.6743e-01,\n",
      "             3.7210e-01,  5.7427e-01],\n",
      "           [ 4.8405e-01, -6.4967e-01,  1.3138e+00,  ...,  9.0335e-02,\n",
      "             4.4615e-01, -1.3837e-01],\n",
      "           ...,\n",
      "           [-2.8294e-02,  1.0971e+00,  1.2758e+00,  ..., -5.3635e-01,\n",
      "             4.0800e-02,  1.0752e+00],\n",
      "           [ 5.0794e-01,  4.3013e-01,  4.3681e-01,  ...,  1.1280e+00,\n",
      "             4.1613e-03, -2.4855e-01],\n",
      "           [ 9.2831e-02,  1.2259e+00, -1.8661e+00,  ..., -8.9243e-01,\n",
      "             1.4395e+00, -1.5545e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.3823e-01, -1.3333e+00,  1.5007e+00,  ..., -9.7957e-01,\n",
      "             4.4971e-02,  6.0811e-01],\n",
      "           [ 1.7672e+00,  1.3709e+00,  9.1121e-01,  ...,  1.4117e+00,\n",
      "             6.0741e-01, -1.4962e+00],\n",
      "           [-6.8370e-01,  1.6013e+00,  1.0004e+00,  ...,  1.2500e+00,\n",
      "             4.1353e-01,  1.4278e+00],\n",
      "           ...,\n",
      "           [ 6.2963e-01, -1.3094e-01,  6.9666e-01,  ...,  1.7874e+00,\n",
      "            -3.4933e-01,  6.1477e-01],\n",
      "           [ 6.0228e-01,  1.8303e+00,  1.3540e+00,  ...,  1.8501e+00,\n",
      "            -5.2702e-02,  2.6891e-01],\n",
      "           [-1.0755e+00, -6.0980e-01, -4.7564e-01,  ...,  7.7642e-01,\n",
      "             9.1785e-01, -1.7166e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 9.8194e-01, -1.7858e+00,  4.7239e-01,  ..., -5.0667e-01,\n",
      "            -1.8284e+00, -5.9211e-01],\n",
      "           [-6.1926e-02, -6.1233e-01,  1.4853e+00,  ...,  6.1680e-01,\n",
      "            -9.0796e-01, -1.0212e+00],\n",
      "           [-5.1248e-02, -2.2764e-01, -9.4302e-01,  ..., -5.6667e-01,\n",
      "             6.0879e-01, -2.5692e-01],\n",
      "           ...,\n",
      "           [-8.6823e-01,  3.5860e-02, -3.3260e-01,  ..., -1.3231e+00,\n",
      "            -9.8551e-01, -1.0589e+00],\n",
      "           [ 1.3389e-01,  2.9604e-01,  1.9481e-01,  ..., -1.0607e+00,\n",
      "            -1.1466e+00, -1.6584e+00],\n",
      "           [-1.2853e+00,  8.9131e-01, -4.0500e-01,  ..., -6.5933e-01,\n",
      "             1.6465e-01,  1.4781e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.2057,  0.1552,  0.7208,  ...,  0.2604,  0.7395,  0.2392],\n",
      "           [ 0.1745, -0.0690,  0.1237,  ...,  0.1354,  0.2733,  0.2763],\n",
      "           [ 0.6237,  0.4086,  0.1001,  ..., -0.0705,  0.3627,  0.5101],\n",
      "           ...,\n",
      "           [-0.4609, -0.0503,  0.0566,  ..., -0.0424, -0.3653, -0.4700],\n",
      "           [ 0.0503, -0.1078, -0.0413,  ..., -0.4747, -0.5674, -0.5282],\n",
      "           [-0.0451,  0.0264,  0.0240,  ...,  0.0980, -0.7761, -0.6059]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5082,  0.7212,  0.2025,  ...,  0.7437,  0.3992,  0.2829],\n",
      "           [ 0.3813,  0.2029,  0.4988,  ...,  0.5122,  0.6623,  0.2594],\n",
      "           [ 0.9259,  0.7619,  0.7717,  ...,  0.3502,  0.3981,  0.3096],\n",
      "           ...,\n",
      "           [-0.6452,  0.0271, -0.4144,  ..., -0.2572, -0.3713, -0.0680],\n",
      "           [ 0.3601, -0.5090,  0.0472,  ...,  0.1652,  0.2561,  0.0285],\n",
      "           [-0.0290,  0.1949,  0.0498,  ..., -0.0065,  0.0281,  0.2260]]],\n",
      "\n",
      "\n",
      "         [[[-0.4285, -0.5867, -0.8551,  ..., -0.6838, -0.8058, -0.8943],\n",
      "           [-0.4953, -0.9374, -1.0764,  ..., -0.5692, -0.7857, -1.0560],\n",
      "           [-0.6970, -0.8550, -0.6347,  ..., -0.7965, -0.7596, -0.6914],\n",
      "           ...,\n",
      "           [ 0.3595, -0.2022, -0.4971,  ..., -0.0735,  0.2589,  0.4615],\n",
      "           [ 0.2744,  0.0217,  0.3252,  ...,  0.3834, -0.1695, -0.0445],\n",
      "           [-0.1600, -0.2547,  0.0397,  ...,  0.4403,  0.7769,  0.0807]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.2801,  0.2885, -0.1358,  ..., -0.4147, -0.1087,  0.0127],\n",
      "           [ 0.8655,  0.0376,  0.6300,  ...,  0.6595,  0.6826,  0.2551],\n",
      "           [-0.2703, -0.8836, -0.2900,  ..., -0.5059, -0.5384,  0.0168],\n",
      "           ...,\n",
      "           [-0.2609, -0.1316, -0.2381,  ..., -0.1010, -0.4830, -0.1264],\n",
      "           [-0.7097, -0.7856, -0.1204,  ..., -0.5278, -0.2306, -0.5682],\n",
      "           [-0.7088, -0.6046, -0.2712,  ..., -0.6484, -0.3723, -0.4797]]],\n",
      "\n",
      "\n",
      "         [[[-0.0037,  0.0811,  0.0278,  ..., -0.2290, -0.2600,  0.2118],\n",
      "           [-0.2702, -0.0338, -0.3038,  ..., -0.0113, -0.2504,  0.0404],\n",
      "           [-0.3096, -0.0853,  0.0387,  ..., -0.1511, -0.3221, -0.0419],\n",
      "           ...,\n",
      "           [-0.1701,  0.0630, -0.1698,  ..., -0.3211, -0.5793,  0.2863],\n",
      "           [-0.0698, -0.4370, -0.1500,  ..., -0.1614,  0.1129, -0.2269],\n",
      "           [-0.4272, -0.0976, -0.5717,  ..., -0.3545,  0.4117, -0.2463]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2841,  0.3543, -0.1706,  ..., -0.3537, -0.4397, -0.0547],\n",
      "           [ 0.2074, -0.0549, -0.0681,  ..., -0.2161,  0.1692,  0.0074],\n",
      "           [ 0.0779,  0.2168,  0.1482,  ...,  0.0668, -0.4218, -0.1374],\n",
      "           ...,\n",
      "           [ 0.3114,  0.0039,  0.1449,  ...,  0.1469,  0.6851,  0.2824],\n",
      "           [ 0.4084,  0.0687,  0.4185,  ...,  0.7173,  0.2738,  0.3819],\n",
      "           [ 0.3349,  0.3050,  0.1274,  ...,  0.6806,  0.5430, -0.0608]]]]]), tensor([]), tensor([197.9304]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.4796, -1.6992,  0.7717,  ..., -1.7218,  1.2271, -1.0750],\n",
      "           [-1.4054, -1.7650, -0.8938,  ..., -0.0578,  0.4974,  0.1206],\n",
      "           [ 1.1575,  0.1932, -1.9402,  ..., -2.1741,  0.0250,  0.6041],\n",
      "           ...,\n",
      "           [-0.9785, -1.5392,  0.4536,  ...,  1.4513,  1.0664, -0.3399],\n",
      "           [ 0.7677,  0.1202, -0.1083,  ...,  0.1347, -0.8998, -0.1159],\n",
      "           [-0.2942,  0.4711,  0.6094,  ...,  2.3010, -1.2444, -1.1862]]],\n",
      "\n",
      "\n",
      "         [[[-0.5899, -0.1750, -1.5313,  ...,  0.2168, -1.0488, -0.7854],\n",
      "           [-1.2915, -1.8805, -0.7239,  ..., -0.5549,  0.7607, -0.9501],\n",
      "           [-0.0518, -1.5403, -1.6844,  ..., -1.0596, -0.2877, -0.4247],\n",
      "           ...,\n",
      "           [-2.1048,  0.0185, -1.5218,  ..., -1.3512,  0.3861, -0.5129],\n",
      "           [ 0.4962,  0.3600, -0.7432,  ...,  1.6223, -0.0781, -0.1502],\n",
      "           [ 0.6728,  1.4209, -0.6560,  ...,  1.9062,  0.0738,  0.7114]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8272,  0.4821,  0.7299,  ...,  1.6973,  1.1244,  0.2190],\n",
      "           [ 0.5544,  0.4650,  0.0818,  ...,  2.1111,  1.1644, -0.4653],\n",
      "           [ 0.6532,  0.2614,  2.0555,  ..., -0.0933,  0.3580, -0.1170],\n",
      "           ...,\n",
      "           [ 1.0500,  0.0272, -0.7994,  ..., -0.2702,  0.2090, -1.2545],\n",
      "           [ 0.0273,  0.5061,  1.6744,  ..., -0.1504, -1.5587, -1.8941],\n",
      "           [ 0.2158, -0.0085,  1.5072,  ..., -0.4415,  0.9054,  0.1654]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.2097,  0.1914,  0.7610,  ..., -0.5452, -0.7106,  0.1726],\n",
      "           [ 0.2332, -2.3312,  0.0372,  ...,  0.7561, -0.9605, -1.6887],\n",
      "           [ 1.3291, -0.7860,  0.8383,  ..., -1.2977, -0.3904,  0.3243],\n",
      "           ...,\n",
      "           [ 0.5687,  1.3263,  1.1071,  ...,  0.5539,  0.1151,  1.2592],\n",
      "           [-1.9149,  0.3605, -0.0242,  ...,  1.3407,  1.9244, -0.9104],\n",
      "           [ 1.6901,  1.9358, -0.1147,  ...,  0.4122, -0.3193,  0.7814]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4723, -0.2066,  1.4176,  ..., -0.4251, -0.0547,  2.4306],\n",
      "           [-0.8925,  0.9667, -0.7863,  ...,  0.0925, -0.3195,  1.3920],\n",
      "           [-0.8963,  0.0326,  0.8437,  ..., -0.7178, -1.0509,  0.3221],\n",
      "           ...,\n",
      "           [ 1.0765,  1.3686,  0.3732,  ...,  0.7621, -0.9477,  1.8762],\n",
      "           [-0.0792,  0.5320, -0.6081,  ...,  1.6576,  2.0945, -0.0980],\n",
      "           [-0.0993,  0.1649, -1.7460,  ..., -0.0782,  2.0877,  0.0097]]],\n",
      "\n",
      "\n",
      "         [[[ 2.2259,  1.8063,  0.7209,  ..., -0.0318, -0.1094,  1.9637],\n",
      "           [ 0.5069,  1.1546,  1.0316,  ..., -0.6125,  1.6935,  1.2778],\n",
      "           [-0.4680,  0.2057,  0.4324,  ...,  0.4074, -2.0778, -0.2364],\n",
      "           ...,\n",
      "           [-1.5017, -0.7423, -0.4670,  ...,  0.6245, -0.9476, -0.9854],\n",
      "           [-1.8416, -3.0803,  0.3697,  ..., -0.8062, -1.6910, -1.2784],\n",
      "           [-0.7427, -1.7315, -1.6876,  ..., -0.7405, -1.4692, -1.6665]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.2779,  1.2173,  0.6055,  ...,  0.3133,  0.6368,  0.1761],\n",
      "           [-0.3818,  0.2210,  0.0637,  ..., -0.1503,  0.3476, -0.5671],\n",
      "           [ 1.4248,  0.3964,  0.5382,  ..., -0.3860, -0.8828,  0.0610],\n",
      "           ...,\n",
      "           [-0.3376, -0.4030, -0.7667,  ...,  0.1837, -0.4400,  0.4433],\n",
      "           [-0.5257, -0.9786,  0.4747,  ..., -0.5547, -0.0298, -0.5337],\n",
      "           [ 0.2124,  0.4212,  0.6077,  ..., -0.0661,  0.8794, -0.1639]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0434, -0.0252,  1.0337,  ..., -0.4537,  0.8791, -0.6058],\n",
      "           [ 0.3427, -0.3325,  1.2598,  ...,  0.8783, -0.2751,  0.6053],\n",
      "           [ 0.2175, -0.2273,  0.6578,  ..., -0.5519,  0.0699, -0.2047],\n",
      "           ...,\n",
      "           [-0.7697, -0.7561,  0.3420,  ..., -0.3031,  0.3077,  0.5463],\n",
      "           [ 0.3252, -1.2602, -0.7636,  ..., -0.6859,  0.7142,  0.9244],\n",
      "           [ 0.9983,  0.5174, -0.0910,  ..., -0.1956, -0.4046, -0.9356]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3790,  0.3169,  0.1464,  ..., -0.9166, -0.7948, -0.9242],\n",
      "           [ 0.2819, -0.8569, -0.1517,  ..., -0.2168, -1.2165, -0.6481],\n",
      "           [-0.7738, -0.7829, -0.4649,  ..., -0.5845, -0.4510,  0.2144],\n",
      "           ...,\n",
      "           [-0.3841, -0.0130, -0.6112,  ...,  0.7117,  0.1585,  1.0272],\n",
      "           [ 0.1772,  0.0067,  0.4035,  ..., -0.9013, -0.4180,  0.0863],\n",
      "           [-0.3412,  1.7422,  0.5684,  ...,  0.9404,  0.4866,  0.1983]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.0712, -1.0285, -0.3075,  ..., -1.1945, -0.5816, -0.2774],\n",
      "           [ 1.0403,  0.4086,  0.0507,  ...,  1.1298, -0.6129,  0.5789],\n",
      "           [ 0.1016,  0.1438,  0.6551,  ..., -0.0469, -0.0656, -0.2495],\n",
      "           ...,\n",
      "           [ 0.8710, -0.1596,  0.2732,  ..., -0.5658,  0.2338, -0.3712],\n",
      "           [-0.1743, -1.4585, -0.3717,  ...,  0.3350, -0.2069, -0.2743],\n",
      "           [ 0.4863,  0.1846, -0.2781,  ...,  0.0684, -0.7814, -0.6023]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3776, -0.1241,  0.9832,  ...,  0.2359, -1.0753, -0.8497],\n",
      "           [ 0.9559, -0.3600, -1.2659,  ..., -0.5920,  0.4060,  0.1803],\n",
      "           [ 0.1092, -0.5303, -0.7317,  ...,  0.8708, -1.1482,  0.3423],\n",
      "           ...,\n",
      "           [ 0.0839, -0.7449, -0.2092,  ..., -0.1602,  0.9026, -0.0779],\n",
      "           [ 0.4000, -0.6000,  0.4179,  ..., -0.1750,  0.3699, -0.8777],\n",
      "           [-0.0609,  0.1692,  0.0562,  ...,  0.4377, -0.8525,  0.5004]]],\n",
      "\n",
      "\n",
      "         [[[-1.3634,  0.0230,  0.1333,  ...,  0.3258, -0.0462, -0.0257],\n",
      "           [ 0.3324, -0.6390,  0.4491,  ...,  0.5588,  0.1739, -0.4469],\n",
      "           [ 0.3772, -0.9287,  0.0606,  ..., -0.4467, -0.0666,  0.1452],\n",
      "           ...,\n",
      "           [-0.8551,  0.6839, -0.7904,  ...,  0.6462,  0.6211,  0.2117],\n",
      "           [-0.0797, -0.0381, -0.3799,  ...,  0.6129,  0.3012,  1.0991],\n",
      "           [ 0.8647, -0.9595,  1.2372,  ...,  1.0626, -0.7999, -0.4937]]]]]), tensor([]), tensor([564.2418]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.3449,  1.2963,  0.1361,  ..., -0.5202,  0.1420, -0.6636],\n",
      "           [-1.4500, -0.2331, -0.4475,  ..., -0.6195,  0.3362, -1.5450],\n",
      "           [ 1.8002, -0.0303,  0.2331,  ..., -1.1387, -2.1131, -0.5556],\n",
      "           ...,\n",
      "           [-0.1383, -1.1873, -1.3232,  ...,  1.4021,  0.1222,  1.7532],\n",
      "           [-0.7390, -1.4811,  0.8382,  ..., -0.1215,  0.9258, -0.0246],\n",
      "           [ 0.2978,  0.8541,  1.1510,  ...,  0.8707,  2.2409,  0.1460]]],\n",
      "\n",
      "\n",
      "         [[[-1.0574, -1.3337,  0.8768,  ..., -2.0704,  0.4654, -2.1226],\n",
      "           [-0.3756, -1.6325,  0.9759,  ...,  0.3760, -1.4745,  0.1684],\n",
      "           [-1.1353, -2.2386, -0.4982,  ..., -1.9383, -0.9040, -1.2441],\n",
      "           ...,\n",
      "           [-0.9704, -1.3823,  0.7524,  ..., -0.5433,  0.4209,  1.0089],\n",
      "           [ 0.1523, -1.1720, -1.7197,  ..., -1.5715,  0.9377,  1.2322],\n",
      "           [ 2.1744,  1.2016, -0.5493,  ..., -0.0511, -0.1886, -1.5059]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0324,  1.7876,  2.1139,  ...,  0.1892,  0.4172, -0.1004],\n",
      "           [ 1.5598,  0.3682,  1.5974,  ...,  1.4512, -0.1820,  0.5753],\n",
      "           [ 0.0626,  0.2905,  0.7343,  ...,  0.6073,  0.7961,  1.6500],\n",
      "           ...,\n",
      "           [-0.9505,  0.3545, -0.4743,  ...,  1.3133, -0.9283,  0.4980],\n",
      "           [-0.1387,  0.1638,  0.7075,  ..., -1.8010, -1.2225, -0.3130],\n",
      "           [-0.2955,  3.6041,  1.4353,  ...,  0.7274,  0.8303,  0.9191]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.1623, -2.2610,  0.0159,  ..., -1.5572, -1.0506, -0.6248],\n",
      "           [ 0.2834, -0.1345, -0.8592,  ...,  0.9102, -2.4945, -0.1482],\n",
      "           [ 0.8597,  1.1054,  2.1326,  ...,  0.5437,  0.7749,  0.1467],\n",
      "           ...,\n",
      "           [ 2.2002,  0.4016,  1.2376,  ..., -0.0828,  1.5329,  0.1688],\n",
      "           [ 0.2789, -1.1355, -0.5138,  ...,  0.9697, -0.0988, -0.5262],\n",
      "           [ 2.6875,  2.0078, -0.1157,  ...,  0.6423, -0.7933, -0.5276]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8450, -0.4342,  2.1041,  ...,  0.6739, -1.4720, -1.1165],\n",
      "           [ 1.8810, -0.3487, -1.9520,  ..., -0.9126,  1.0879,  0.6126],\n",
      "           [ 0.5337, -0.6716, -0.9228,  ...,  1.6818, -1.9566,  0.7265],\n",
      "           ...,\n",
      "           [ 0.8201, -0.9608,  0.0254,  ...,  0.3160,  1.7469,  0.3369],\n",
      "           [ 0.7960, -0.1078,  0.8113,  ...,  0.2281,  0.6533, -0.9499],\n",
      "           [ 0.6511,  0.5179,  0.5198,  ...,  1.2278, -0.8612,  1.8296]]],\n",
      "\n",
      "\n",
      "         [[[-2.1071,  0.0703,  0.7317,  ...,  1.1989,  0.6529,  0.6373],\n",
      "           [ 0.4616, -0.6683,  1.2006,  ...,  1.3233,  0.6765, -0.3613],\n",
      "           [ 0.4482, -1.7059,  0.0184,  ..., -0.6815, -0.1100,  0.4301],\n",
      "           ...,\n",
      "           [-2.5421,  0.9502, -1.7834,  ..., -0.1470, -0.7123, -1.2005],\n",
      "           [-1.4970, -1.2985, -1.2800,  ..., -0.2230, -0.9604,  0.8324],\n",
      "           [ 0.6599, -2.8432,  1.3890,  ...,  0.2582, -1.8833, -0.9044]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 1.0607e+00,  8.0115e-01, -5.6083e-02,  ...,  3.7052e-01,\n",
      "             6.2893e-01,  1.1801e-01],\n",
      "           [ 1.7836e-01,  5.6198e-01,  2.0228e-01,  ...,  1.6163e-01,\n",
      "            -1.6330e-01,  3.3819e-01],\n",
      "           [ 4.9891e-01,  3.7759e-01,  8.8008e-02,  ...,  4.8483e-01,\n",
      "             4.9103e-01,  5.2356e-01],\n",
      "           ...,\n",
      "           [-1.5092e-01, -8.1789e-02,  1.1459e-01,  ...,  5.4603e-02,\n",
      "            -3.0517e-01, -2.1351e-01],\n",
      "           [-1.0276e-01,  1.2887e-01, -2.8670e-01,  ..., -6.5196e-02,\n",
      "            -4.3466e-01, -5.9429e-01],\n",
      "           [-4.7575e-01,  3.0131e-01, -1.0528e-01,  ..., -4.8608e-01,\n",
      "            -2.2924e-01, -1.0246e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 7.5410e-01,  7.4858e-01,  6.5820e-02,  ...,  4.2257e-01,\n",
      "             3.2254e-01,  2.8210e-02],\n",
      "           [ 6.2487e-01,  1.4632e-01,  9.8856e-02,  ...,  4.7961e-01,\n",
      "             1.6113e-01,  5.2535e-01],\n",
      "           [ 4.2608e-01,  8.2774e-01,  9.2709e-01,  ...,  4.5632e-02,\n",
      "             1.0690e-01,  5.8343e-01],\n",
      "           ...,\n",
      "           [-6.4918e-01, -2.2267e-01,  4.8729e-01,  ...,  1.2302e-01,\n",
      "            -1.7103e-01,  3.5085e-01],\n",
      "           [ 3.2581e-01, -4.3276e-01, -4.5831e-01,  ..., -2.7850e-01,\n",
      "            -3.0699e-01,  4.4042e-01],\n",
      "           [-4.5732e-03, -6.2570e-01, -2.6889e-02,  ...,  5.4716e-03,\n",
      "            -2.4919e-01, -6.9602e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.5366e-01, -6.6257e-01,  6.7775e-02,  ..., -2.1761e-01,\n",
      "            -1.2837e+00, -5.2299e-01],\n",
      "           [-5.8886e-01, -7.4330e-01, -7.7381e-01,  ..., -1.0233e+00,\n",
      "            -8.4043e-01, -4.5894e-01],\n",
      "           [-5.9413e-01, -7.6168e-01, -4.8435e-01,  ..., -7.8187e-01,\n",
      "            -5.1540e-01, -6.0958e-01],\n",
      "           ...,\n",
      "           [ 1.4501e-01,  2.7146e-02, -2.0266e-01,  ...,  2.0774e-01,\n",
      "             2.1954e-01,  2.1733e-01],\n",
      "           [ 2.2902e-01, -4.9686e-01,  4.3611e-01,  ...,  7.2467e-01,\n",
      "             1.3849e-01,  3.0488e-01],\n",
      "           [ 9.2984e-02, -7.2282e-01, -4.1885e-01,  ...,  4.8919e-01,\n",
      "             3.9759e-02, -6.1139e-04]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-3.1553e-01, -6.9915e-02, -3.0915e-01,  ..., -1.4785e-01,\n",
      "             5.9962e-02,  9.4753e-02],\n",
      "           [ 9.3170e-01,  5.7921e-01,  8.7925e-02,  ...,  5.5828e-02,\n",
      "             6.1164e-01, -1.4112e-02],\n",
      "           [ 1.5486e-01, -7.6367e-01, -4.5693e-02,  ..., -9.0969e-01,\n",
      "            -7.8187e-01,  5.2729e-02],\n",
      "           ...,\n",
      "           [-5.7247e-01, -2.6425e-01, -1.5888e-01,  ...,  2.6668e-01,\n",
      "             2.3468e-01, -4.4238e-01],\n",
      "           [ 5.5343e-02, -7.9785e-01,  2.2198e-01,  ..., -4.6400e-01,\n",
      "            -2.5070e-01, -6.6500e-01],\n",
      "           [-7.6624e-01, -2.1998e-01, -2.0873e-01,  ..., -5.2653e-02,\n",
      "             2.5700e-01, -3.1817e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.6057e-01, -2.5965e-02, -2.4474e-01,  ..., -1.4843e-01,\n",
      "            -3.5060e-01, -2.8660e-02],\n",
      "           [ 1.3938e-01, -1.8060e-01, -7.2368e-02,  ..., -3.7053e-01,\n",
      "            -3.3259e-01, -4.1805e-02],\n",
      "           [ 3.0875e-01, -4.8583e-02, -2.5672e-01,  ...,  8.8588e-02,\n",
      "             5.2018e-01,  1.0569e-02],\n",
      "           ...,\n",
      "           [-1.8791e-01,  1.3641e-01, -3.6076e-02,  ..., -3.0897e-01,\n",
      "            -4.2325e-01,  1.2627e-01],\n",
      "           [-4.5658e-01, -7.5553e-02, -7.9172e-02,  ..., -7.9618e-01,\n",
      "             4.7807e-01, -2.5559e-01],\n",
      "           [-9.8503e-01, -2.7905e-01, -6.4449e-02,  ..., -2.9521e-01,\n",
      "             3.1429e-01, -5.3205e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.5453e-01,  3.3484e-01, -4.4404e-01,  ..., -8.0969e-02,\n",
      "            -4.8091e-01, -2.2472e-01],\n",
      "           [ 9.1501e-02, -5.5534e-02,  2.2613e-01,  ..., -3.5845e-01,\n",
      "            -1.6861e-01, -1.3707e-01],\n",
      "           [ 5.1257e-02, -2.5483e-01, -1.2718e-01,  ...,  2.9262e-01,\n",
      "             5.1737e-03, -2.9850e-01],\n",
      "           ...,\n",
      "           [ 2.9257e-01,  4.2222e-01,  3.5817e-01,  ...,  2.5016e-02,\n",
      "             8.7060e-01,  5.6383e-01],\n",
      "           [ 1.1690e+00,  4.7788e-01, -1.0591e-01,  ...,  1.1075e+00,\n",
      "             7.1107e-01,  4.6367e-01],\n",
      "           [ 8.1809e-02,  5.6410e-02,  2.2734e-01,  ...,  7.6368e-01,\n",
      "             8.1309e-01,  8.9827e-01]]]]]), tensor([]), tensor([287.8298]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.9613,  1.0408, -2.1818,  ..., -0.6191,  0.0416, -1.4237],\n",
      "           [-0.9299,  1.0292, -0.4629,  ..., -0.1495, -1.1576,  0.1321],\n",
      "           [ 0.3149, -0.1193, -1.2418,  ...,  0.3684,  0.5629,  0.5108],\n",
      "           ...,\n",
      "           [ 0.4493, -1.2747,  0.3804,  ...,  1.3142,  0.5492,  0.3232],\n",
      "           [ 0.1101,  1.0289, -0.8517,  ...,  1.4062,  0.1277, -0.5511],\n",
      "           [-1.7378,  1.2483, -0.1267,  ..., -0.1110,  1.2152, -2.7690]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4363,  0.0576, -1.5193,  ..., -1.0057, -0.8332, -1.7628],\n",
      "           [ 0.3017, -1.2490, -1.7334,  ..., -0.4819, -1.1749,  0.1831],\n",
      "           [-1.8321, -0.9636, -0.9329,  ..., -2.0662, -1.4703,  0.7857],\n",
      "           ...,\n",
      "           [-1.2218, -0.7655,  2.0593,  ...,  0.0267,  0.6723,  2.0739],\n",
      "           [ 0.4313,  0.5001, -2.4246,  ..., -0.3092, -0.6546,  0.9451],\n",
      "           [ 0.7004, -1.7720, -0.7631,  ...,  1.6890, -0.9770, -0.7126]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9705, -0.0945,  3.7852,  ...,  2.3480, -0.7412,  1.2984],\n",
      "           [ 0.1952,  0.9470,  0.8449,  ..., -0.0064,  0.7124,  1.4113],\n",
      "           [ 0.7481,  0.3772,  1.7510,  ...,  0.2323,  1.1451,  0.5624],\n",
      "           ...,\n",
      "           [ 0.0189,  0.8575,  0.5525,  ...,  1.0877,  0.0799, -1.1504],\n",
      "           [-0.0906, -1.4088,  1.4002,  ...,  1.3034, -0.1321, -0.3559],\n",
      "           [ 0.8727, -1.6353, -0.6358,  ...,  0.1683, -0.6823,  0.7024]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.5540, -0.2553,  0.0917,  ...,  0.1360, -0.0681, -0.2891],\n",
      "           [ 0.2649,  0.6194, -1.7045,  ..., -2.4254, -0.8974, -1.9866],\n",
      "           [ 1.1744, -0.8072,  1.2143,  ..., -2.6577, -2.1452,  0.0500],\n",
      "           ...,\n",
      "           [-0.5747,  0.6641,  0.4733,  ...,  1.2306,  1.7277,  0.6598],\n",
      "           [ 1.2970,  0.0942,  1.1293,  ..., -0.0295, -0.5535, -0.0724],\n",
      "           [ 0.9162,  2.2957, -0.3532,  ...,  1.2411,  1.6040,  1.4121]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0972, -0.3470, -0.1852,  ...,  0.0597, -0.2854,  0.6385],\n",
      "           [ 0.9586, -0.0049,  0.2485,  ..., -0.9597, -0.1328,  0.6078],\n",
      "           [ 1.4691,  0.2475, -0.5857,  ...,  0.2018,  1.9364,  0.2520],\n",
      "           ...,\n",
      "           [ 0.6721,  1.2351,  0.7926,  ...,  0.2434, -0.0761,  0.8512],\n",
      "           [-1.4193,  1.4281, -0.0219,  ..., -1.4377,  2.4357, -0.3234],\n",
      "           [-2.0048, -0.5439,  0.6242,  ...,  0.7760,  1.1428, -0.5590]]],\n",
      "\n",
      "\n",
      "         [[[-1.1303,  1.0995, -0.4813,  ...,  0.8426, -0.2909,  0.5622],\n",
      "           [-0.0842,  0.7332,  1.6919,  ..., -0.8889,  0.1442,  0.4387],\n",
      "           [-0.5023, -1.4266, -0.8192,  ...,  1.4148,  0.5695, -0.4275],\n",
      "           ...,\n",
      "           [-1.1625,  1.1288,  0.6008,  ..., -0.6726, -1.0049, -0.3080],\n",
      "           [ 1.3824, -0.6833, -1.6405,  ...,  0.2178, -0.6611, -1.8248],\n",
      "           [-1.4734, -2.0764, -0.7348,  ..., -0.2679,  0.5656,  2.3136]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.3127, -0.0503,  0.4592,  ...,  0.4099,  0.2226,  0.4195],\n",
      "           [ 0.4534, -0.0151, -0.3345,  ..., -0.2773, -1.1243,  0.3395],\n",
      "           [ 0.4388,  0.8006,  0.2100,  ...,  0.3932, -0.1140,  0.9597],\n",
      "           ...,\n",
      "           [ 0.0190,  0.1134, -0.0310,  ..., -0.4224,  0.0920,  0.3904],\n",
      "           [ 0.6044, -0.4133,  0.1118,  ...,  0.0416, -0.2064, -0.3863],\n",
      "           [ 0.8234,  0.3161,  0.3714,  ..., -0.6829, -0.2896, -0.0128]]],\n",
      "\n",
      "\n",
      "         [[[-0.5493,  0.4384,  0.6329,  ...,  0.7803, -0.1309,  0.7000],\n",
      "           [ 0.2798,  0.6453,  0.1012,  ...,  0.1505,  0.1641, -0.4594],\n",
      "           [ 0.5903,  0.3550,  1.1685,  ...,  0.5179, -0.3032,  0.8895],\n",
      "           ...,\n",
      "           [ 0.2452, -0.2053,  0.1989,  ...,  0.4596, -0.2088, -0.0133],\n",
      "           [ 0.2179, -0.2478, -0.2461,  ...,  0.2528, -0.2036,  0.0414],\n",
      "           [ 0.0662, -0.2108, -0.0915,  ..., -0.2572,  0.6447, -0.3528]]],\n",
      "\n",
      "\n",
      "         [[[-0.4386,  0.2937, -0.4272,  ..., -0.6707, -0.5787,  0.0613],\n",
      "           [-0.8401, -0.5837, -1.0811,  ..., -0.6107, -0.1753, -0.3646],\n",
      "           [-0.4723, -0.6442, -0.3489,  ..., -0.3273,  0.3430, -0.9121],\n",
      "           ...,\n",
      "           [-0.0315, -0.2002, -0.4119,  ...,  0.0136,  0.6903,  0.7387],\n",
      "           [-0.0517,  0.6506,  0.1290,  ...,  0.6681, -0.4900,  0.6302],\n",
      "           [ 0.3567,  0.4515, -0.5536,  ...,  0.2884,  0.2176,  0.0932]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.1906,  0.3515, -0.1242,  ..., -0.4990,  0.5925, -0.2399],\n",
      "           [ 0.6003,  0.4999,  0.1677,  ...,  0.4868,  0.3924, -0.3970],\n",
      "           [-0.1848, -0.6456, -0.3486,  ..., -0.3361,  0.2001,  0.3184],\n",
      "           ...,\n",
      "           [-0.5265, -0.0763, -1.1245,  ..., -0.3988, -0.3162, -0.3344],\n",
      "           [-0.4142, -0.7361, -0.1946,  ...,  0.2895,  0.2096, -0.3505],\n",
      "           [-0.7880, -1.0388,  0.3820,  ..., -0.7065,  0.8328,  0.1090]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4032,  0.0521, -0.4215,  ..., -0.4242,  0.3647, -0.5353],\n",
      "           [-0.3738, -0.6580, -0.4117,  ..., -0.1571, -0.5792, -0.0282],\n",
      "           [-0.0567, -0.4639, -0.2843,  ..., -0.1854,  0.1365,  0.1014],\n",
      "           ...,\n",
      "           [ 0.0598, -0.1214, -0.6573,  ..., -0.3701,  0.2216, -0.1847],\n",
      "           [-0.2558, -0.1494,  0.1525,  ...,  0.1148, -0.1900, -0.2074],\n",
      "           [-0.7778, -0.0241,  0.4409,  ..., -0.4034,  0.0534, -0.0380]]],\n",
      "\n",
      "\n",
      "         [[[-0.4151,  0.3806, -0.1200,  ...,  0.1174, -0.7905, -1.1966],\n",
      "           [-0.3661,  0.0189, -0.3372,  ...,  0.3187, -0.1264, -0.8212],\n",
      "           [-0.2031,  0.0016, -0.5944,  ..., -0.3045, -0.4435, -0.1025],\n",
      "           ...,\n",
      "           [-0.0706,  0.2864,  0.2868,  ...,  0.0699,  1.0968,  0.0102],\n",
      "           [ 0.0921,  0.0512,  0.3583,  ...,  0.6197,  1.0065,  0.0569],\n",
      "           [ 0.4941,  0.1460,  0.4111,  ...,  0.2030,  0.5008, -0.3350]]]]]), tensor([]), tensor([403.7738]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-5.0880e-01, -1.4655e+00, -4.4831e-01,  ..., -4.9256e-01,\n",
      "            -9.6691e-01, -4.1469e-01],\n",
      "           [ 7.2086e-02, -8.6273e-01, -1.7146e+00,  ..., -1.1065e+00,\n",
      "            -3.2860e+00,  2.4675e-01],\n",
      "           [-6.1019e-02,  9.4690e-01, -6.2856e-01,  ...,  3.2227e-01,\n",
      "            -9.6414e-01,  1.6367e+00],\n",
      "           ...,\n",
      "           [ 7.7868e-01, -3.8244e-01, -1.1248e-01,  ...,  3.8033e-01,\n",
      "             1.5340e+00,  2.1571e+00],\n",
      "           [ 1.7887e+00, -6.4436e-01,  3.2441e-01,  ...,  1.3146e+00,\n",
      "             1.2289e+00,  5.8442e-01],\n",
      "           [ 2.0587e+00,  9.1160e-01,  1.1097e+00,  ..., -2.7159e-01,\n",
      "             6.6799e-01,  9.6492e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.9166e+00, -6.9031e-01,  6.3647e-02,  ...,  3.7596e-01,\n",
      "            -1.8970e+00,  2.6856e-01],\n",
      "           [-6.6532e-01,  3.2542e-01, -1.4535e+00,  ..., -1.2069e+00,\n",
      "            -1.1600e+00, -2.2170e+00],\n",
      "           [-5.3226e-01, -1.2841e+00,  7.6052e-01,  ..., -4.7611e-01,\n",
      "            -2.2467e+00,  1.0147e+00],\n",
      "           ...,\n",
      "           [ 1.3332e+00, -5.0293e-01,  4.7438e-01,  ...,  1.0171e+00,\n",
      "            -1.3592e+00,  5.8298e-01],\n",
      "           [-3.4256e-03,  8.7955e-01, -1.3008e+00,  ...,  2.9610e-02,\n",
      "            -6.3336e-01, -2.8339e-03],\n",
      "           [ 9.2494e-01, -3.5173e-01, -7.7998e-01,  ..., -6.1730e-01,\n",
      "             1.3074e+00, -1.0602e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 8.1724e-01,  2.2607e+00,  1.4173e+00,  ...,  8.3093e-01,\n",
      "             1.1040e+00,  2.4273e+00],\n",
      "           [-5.5941e-01,  1.2240e+00, -2.1106e-01,  ...,  1.0885e+00,\n",
      "             2.3293e+00,  1.4392e+00],\n",
      "           [ 9.3472e-01,  9.2240e-01,  1.9083e+00,  ...,  1.3694e+00,\n",
      "             3.1052e+00, -3.8972e-01],\n",
      "           ...,\n",
      "           [-4.7551e-01,  1.5267e-01, -3.4776e-02,  ...,  3.7291e-01,\n",
      "             4.9322e-01,  2.4755e-01],\n",
      "           [-7.4404e-01,  1.8686e+00,  2.7120e-01,  ...,  1.5645e+00,\n",
      "            -1.4448e+00,  9.4904e-01],\n",
      "           [ 1.4295e+00,  1.7224e+00, -7.9268e-01,  ..., -9.3011e-01,\n",
      "            -7.3929e-02,  5.0558e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-6.5015e-01,  8.9029e-01,  1.7648e-01,  ..., -7.9562e-01,\n",
      "             1.3494e+00, -1.1773e+00],\n",
      "           [-3.2063e-01,  3.5183e-01, -6.5188e-01,  ..., -7.2027e-01,\n",
      "            -1.5369e+00, -2.6388e+00],\n",
      "           [ 1.0446e-01, -6.7796e-02,  1.5007e-01,  ..., -5.3510e-01,\n",
      "             1.2608e+00,  1.0197e+00],\n",
      "           ...,\n",
      "           [-1.0103e-01,  9.0848e-01, -1.8263e+00,  ..., -1.5113e-01,\n",
      "             5.3859e-01, -3.3151e-02],\n",
      "           [-1.7249e-01,  4.5863e-01, -2.8445e-01,  ...,  8.4125e-01,\n",
      "             1.3551e+00, -6.9624e-01],\n",
      "           [ 7.2107e-01, -2.4527e-01,  1.2286e+00,  ..., -9.6840e-01,\n",
      "             2.1388e+00,  1.2898e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.2610e+00, -1.3389e-01, -4.1593e-01,  ..., -5.8707e-01,\n",
      "             1.6375e+00, -7.6875e-01],\n",
      "           [-5.7542e-01, -1.1730e+00, -5.8694e-01,  ..., -2.6342e-01,\n",
      "            -1.0205e+00,  7.0683e-01],\n",
      "           [ 1.7263e-01, -8.1673e-01, -4.7232e-01,  ..., -8.2489e-01,\n",
      "             8.4381e-02,  2.3089e-01],\n",
      "           ...,\n",
      "           [ 1.1707e+00,  2.2543e-01, -1.1100e+00,  ..., -3.7459e-02,\n",
      "             7.6397e-01,  1.5830e-01],\n",
      "           [-5.1457e-01,  9.3043e-01,  6.1544e-01,  ...,  7.3138e-01,\n",
      "            -3.2217e-01, -6.8517e-02],\n",
      "           [-8.9675e-01,  9.1773e-02,  1.6222e+00,  ..., -6.1786e-01,\n",
      "             1.3746e-01,  2.9236e-01]]],\n",
      "\n",
      "\n",
      "         [[[-5.9382e-01,  1.0480e+00,  5.6740e-01,  ...,  1.2350e+00,\n",
      "            -5.7053e-01, -1.9588e+00],\n",
      "           [-9.1337e-01,  8.2263e-01, -1.3404e-01,  ...,  1.1953e+00,\n",
      "             3.7762e-01, -1.2595e+00],\n",
      "           [-7.7077e-01,  2.3994e-01, -1.3537e+00,  ..., -5.7680e-01,\n",
      "            -9.5499e-01, -7.0547e-02],\n",
      "           ...,\n",
      "           [-1.5621e+00,  5.7044e-01,  2.3825e-01,  ..., -1.0051e+00,\n",
      "             6.4511e-01, -1.8454e+00],\n",
      "           [-1.6792e+00, -1.4480e+00,  6.2943e-04,  ..., -8.7180e-02,\n",
      "             2.7754e-01, -2.2626e+00],\n",
      "           [ 6.0874e-02, -1.3835e+00, -4.0263e-02,  ..., -1.7926e+00,\n",
      "            -2.7138e-01, -1.0811e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 8.6708e-03,  3.6824e-01,  4.8802e-01,  ...,  6.7245e-02,\n",
      "            -4.2307e-01, -6.1790e-01],\n",
      "           [-1.0663e-01,  5.9448e-01,  8.7438e-01,  ..., -4.1415e-01,\n",
      "             3.5552e-01,  3.2223e-01],\n",
      "           [ 1.7731e-01,  1.6651e-01, -6.2672e-02,  ..., -1.6340e-01,\n",
      "             2.5426e-01,  1.1078e-01],\n",
      "           ...,\n",
      "           [-2.0218e-01, -1.0188e-03,  2.0018e-01,  ..., -7.5219e-01,\n",
      "            -1.6787e-01, -4.6119e-01],\n",
      "           [-2.5112e-01,  4.4221e-01,  1.1070e-01,  ..., -2.3020e-01,\n",
      "            -1.5975e-02,  5.1682e-01],\n",
      "           [-7.3343e-01,  2.1983e-01,  3.7879e-01,  ..., -9.9750e-03,\n",
      "             6.1175e-01, -9.4432e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0136e-01,  8.3059e-01,  2.5812e-01,  ...,  9.9352e-01,\n",
      "             1.3856e-01,  1.8081e-01],\n",
      "           [ 5.0173e-01,  9.6636e-02, -2.6450e-03,  ...,  1.0428e-02,\n",
      "             8.0109e-01, -2.5856e-01],\n",
      "           [ 1.0793e+00,  1.0541e+00, -4.4253e-01,  ...,  1.4989e-01,\n",
      "             8.0704e-01,  3.3455e-01],\n",
      "           ...,\n",
      "           [-6.2826e-01, -5.2046e-01,  7.1506e-01,  ..., -2.7508e-01,\n",
      "             3.6970e-01, -5.7018e-01],\n",
      "           [-1.8642e-01, -8.6015e-01,  2.9092e-01,  ...,  4.8691e-01,\n",
      "            -2.1388e-01, -8.9856e-01],\n",
      "           [ 4.3476e-01,  1.3698e-01, -6.9848e-02,  ..., -1.8124e-01,\n",
      "             6.6560e-01, -2.8772e-01]]],\n",
      "\n",
      "\n",
      "         [[[-9.3742e-02, -1.1905e-01, -1.7447e-02,  ...,  4.6989e-01,\n",
      "            -2.3424e-01, -3.6837e-01],\n",
      "           [-3.3673e-01, -1.2521e+00, -8.6307e-01,  ..., -3.8208e-02,\n",
      "            -9.9703e-01, -2.0999e-02],\n",
      "           [-7.0474e-01,  7.2171e-02, -7.3910e-01,  ..., -1.2710e+00,\n",
      "            -5.9479e-01, -5.9755e-01],\n",
      "           ...,\n",
      "           [ 1.4917e+00,  7.4427e-02, -3.8945e-01,  ...,  4.3485e-01,\n",
      "             4.0667e-01, -3.6656e-01],\n",
      "           [ 2.6680e-02,  1.5302e-02, -1.6900e-01,  ...,  3.2689e-01,\n",
      "             1.0523e+00, -7.6142e-02],\n",
      "           [ 5.4516e-01, -9.5755e-01,  2.2014e-01,  ...,  9.7278e-02,\n",
      "             7.1487e-01, -4.3494e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-3.0828e-01,  2.1158e-01,  9.8847e-02,  ...,  3.5102e-02,\n",
      "             5.4871e-01,  2.0444e-01],\n",
      "           [-7.9457e-03,  1.1540e+00,  2.8099e-01,  ...,  7.4027e-01,\n",
      "             2.5120e-01,  3.5570e-01],\n",
      "           [ 4.3127e-01, -6.1550e-01, -6.0806e-02,  ..., -3.3871e-01,\n",
      "             8.9023e-01,  2.3264e-01],\n",
      "           ...,\n",
      "           [-1.1085e-01,  3.1167e-01, -9.6880e-01,  ...,  1.2518e-01,\n",
      "            -3.1489e-01, -4.9959e-01],\n",
      "           [-7.7009e-01, -7.8997e-01,  7.4379e-02,  ..., -1.8463e-01,\n",
      "            -1.1141e+00, -1.0493e+00],\n",
      "           [-6.7187e-01, -3.2772e-02,  1.7672e-01,  ..., -3.3806e-01,\n",
      "             3.9175e-02, -4.0536e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.5748e-01,  1.6010e-01,  4.9655e-03,  ...,  3.8162e-02,\n",
      "             3.9919e-02, -1.2821e-01],\n",
      "           [ 4.5907e-01,  8.8304e-02, -4.0648e-01,  ..., -3.5852e-01,\n",
      "            -3.2804e-01, -2.8109e-01],\n",
      "           [-2.5479e-02, -4.1556e-01, -5.7529e-01,  ...,  1.0388e-01,\n",
      "             5.1017e-01, -5.3288e-02],\n",
      "           ...,\n",
      "           [-1.1941e-01, -5.6250e-01,  3.8440e-01,  ..., -8.6788e-01,\n",
      "             3.5186e-02, -7.8937e-01],\n",
      "           [ 5.2413e-01, -6.5026e-01, -8.1369e-02,  ..., -5.1895e-01,\n",
      "            -2.5452e-01,  3.2746e-01],\n",
      "           [-4.9022e-01, -1.1988e+00,  4.0614e-01,  ...,  1.9785e-01,\n",
      "             5.4673e-02, -2.0550e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.0857e-01, -7.6265e-02,  3.3417e-01,  ...,  1.5378e-01,\n",
      "             1.1987e-01,  1.4183e-01],\n",
      "           [-2.4386e-01, -3.8479e-01,  7.4788e-01,  ..., -4.6458e-01,\n",
      "            -6.6825e-02, -2.6536e-01],\n",
      "           [-1.6807e-01, -6.8415e-01,  3.6129e-01,  ...,  8.8708e-02,\n",
      "            -3.8594e-02,  1.2210e-01],\n",
      "           ...,\n",
      "           [ 3.7436e-01, -7.2039e-01, -8.4302e-03,  ...,  8.2017e-02,\n",
      "             1.7382e-01,  6.2588e-01],\n",
      "           [-2.4965e-01,  7.3391e-01, -4.7519e-01,  ...,  5.1845e-01,\n",
      "             4.2169e-01, -1.8473e-01],\n",
      "           [ 4.5921e-01,  1.4185e+00,  7.3761e-01,  ...,  4.4068e-01,\n",
      "             1.6832e-01,  6.6524e-01]]]]]), tensor([]), tensor([462.3073]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.1974, -0.3251, -0.1923,  ..., -1.1805, -2.1449, -2.4526],\n",
      "           [-1.2510,  0.5201,  1.1215,  ..., -1.2972,  0.4417,  0.1288],\n",
      "           [-0.4884, -0.4748, -1.0974,  ..., -1.0329, -0.0876, -0.5023],\n",
      "           ...,\n",
      "           [ 0.1846, -0.5073,  0.3571,  ..., -0.6247,  0.9348,  0.2208],\n",
      "           [-0.3382,  1.3087,  0.2301,  ...,  0.5736,  1.1221,  2.1121],\n",
      "           [-1.6517,  0.6870,  0.9766,  ...,  1.0523,  2.2846, -1.1864]]],\n",
      "\n",
      "\n",
      "         [[[-1.2551,  0.4329, -0.5376,  ...,  0.6687, -1.0268, -0.7664],\n",
      "           [ 0.0263, -0.8215, -1.5710,  ..., -1.2895,  0.7104, -1.4075],\n",
      "           [ 0.5301,  0.1728, -3.0281,  ..., -1.3955,  0.0723, -0.2878],\n",
      "           ...,\n",
      "           [-0.9997, -1.2486,  1.4201,  ...,  0.1524,  0.5507, -0.6714],\n",
      "           [-1.0022, -0.6951,  0.1873,  ...,  1.7152, -0.6781, -1.4719],\n",
      "           [ 1.5456,  0.5307, -0.6062,  ..., -0.6154,  1.0665, -0.8086]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4798,  1.1465,  2.1234,  ...,  3.2586,  1.7664,  1.2391],\n",
      "           [ 0.6368, -0.3830,  0.5353,  ...,  2.2024,  0.1333,  2.1925],\n",
      "           [ 0.2458,  2.1688,  0.7650,  ..., -0.9104,  0.6274,  0.3362],\n",
      "           ...,\n",
      "           [ 2.9506,  0.6421, -0.0575,  ...,  1.1573, -0.2024, -2.2234],\n",
      "           [-0.4554,  0.2514, -0.4390,  ...,  0.5277,  1.0056, -1.6403],\n",
      "           [ 1.6099, -1.5121,  0.9660,  ..., -0.4834,  0.8474, -1.1197]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.0399,  0.7172,  1.1659,  ...,  0.7246,  1.1723,  0.4305],\n",
      "           [-1.8571,  1.5979, -1.0526,  ..., -0.0395, -1.1640, -0.4755],\n",
      "           [ 1.9984, -0.0076,  0.8083,  ..., -0.3192,  2.4931,  0.7563],\n",
      "           ...,\n",
      "           [ 0.7110,  1.8450, -1.2077,  ...,  0.7878,  0.8071,  0.3093],\n",
      "           [-0.9491,  0.1149,  0.2795,  ..., -0.1252, -1.2337, -0.6685],\n",
      "           [ 0.7140,  1.8174,  0.5952,  ..., -0.0572,  1.1104, -0.0290]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8027,  0.2579,  0.8018,  ...,  0.5893,  0.7767,  0.2369],\n",
      "           [ 1.1949,  0.5619, -0.6134,  ..., -0.6168, -0.3220, -0.2940],\n",
      "           [ 0.3978, -0.5551, -0.9999,  ...,  0.0868,  0.9250,  0.1216],\n",
      "           ...,\n",
      "           [ 0.5105, -0.7566,  1.2620,  ..., -0.6364,  0.3922, -1.3064],\n",
      "           [ 1.2135, -0.3572, -0.1006,  ..., -0.2128, -0.5548,  1.3094],\n",
      "           [-0.0740, -2.3247,  1.4161,  ...,  0.7499,  0.2613, -0.0355]]],\n",
      "\n",
      "\n",
      "         [[[-0.1214,  0.0047,  1.5005,  ...,  1.1524,  1.3644,  1.2372],\n",
      "           [-0.6136, -0.0963,  2.3620,  ..., -0.7387,  0.4404,  0.0201],\n",
      "           [-0.6304, -1.4606,  0.7221,  ...,  0.1792, -0.2121,  0.4276],\n",
      "           ...,\n",
      "           [-0.3337, -1.7545, -0.5263,  ..., -0.4591, -1.4235, -0.2138],\n",
      "           [-2.0399,  0.1991, -1.7782,  ..., -0.6828, -0.7523, -2.4113],\n",
      "           [-0.0070,  1.7333,  0.7424,  ..., -0.6950, -1.3322,  0.7553]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-0.1065,  0.5434, -0.1546,  ...,  0.4289,  0.4498, -0.6874],\n",
      "           [ 0.1158,  0.2104,  0.3647,  ...,  0.7772,  1.3889, -0.9995],\n",
      "           [-0.5855,  0.4208,  0.9667,  ...,  0.0973,  0.3487, -0.0777],\n",
      "           ...,\n",
      "           [-0.4936, -0.0919, -0.1225,  ..., -0.1075, -0.8390, -1.8595],\n",
      "           [ 0.0297,  0.0896, -0.5077,  ...,  0.0111, -1.0873, -0.5173],\n",
      "           [-0.5750,  0.4751, -0.6670,  ..., -1.4556,  0.2173,  0.2563]]],\n",
      "\n",
      "\n",
      "         [[[-0.2292, -0.8264,  0.0182,  ...,  0.1748,  0.6961, -0.1266],\n",
      "           [ 1.9939,  0.9556,  1.1564,  ...,  0.3624, -0.4978, -0.3227],\n",
      "           [ 0.7948,  0.6015,  1.4753,  ...,  1.1082,  1.0705,  1.0590],\n",
      "           ...,\n",
      "           [-0.7706, -0.3157, -0.4758,  ...,  0.6451,  0.4896,  0.3104],\n",
      "           [ 0.1480, -0.1214,  1.1851,  ...,  1.0274, -0.8453,  0.8568],\n",
      "           [ 0.1861,  0.8620,  1.4601,  ..., -0.1639,  1.3629, -0.0810]]],\n",
      "\n",
      "\n",
      "         [[[-0.1791,  0.8491, -0.2147,  ..., -0.1995,  0.1289,  0.0202],\n",
      "           [ 0.3440, -0.7835, -0.1603,  ...,  0.3911,  0.1752, -1.4611],\n",
      "           [-0.1463,  0.1966,  0.4699,  ..., -0.5641, -0.8121, -0.6215],\n",
      "           ...,\n",
      "           [-0.7520, -0.3206, -0.2675,  ..., -0.1650, -0.8154,  0.0345],\n",
      "           [ 1.0579,  0.3900, -0.1704,  ...,  0.8699, -1.3002,  0.1653],\n",
      "           [ 0.8510, -0.6647, -1.2510,  ..., -0.1955, -0.2088, -0.1824]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0145, -0.3020, -0.3485,  ..., -1.1385, -0.7101,  0.8978],\n",
      "           [-0.3021,  0.6558,  0.2151,  ...,  1.4437,  1.1724, -0.2296],\n",
      "           [ 0.0764, -0.4637,  1.2951,  ..., -0.8525, -0.1567, -0.7270],\n",
      "           ...,\n",
      "           [-0.7370,  0.4565,  0.5499,  ..., -0.0163, -0.4585, -0.8131],\n",
      "           [-1.2477, -0.5445, -0.2429,  ...,  0.1515, -0.0976,  0.7741],\n",
      "           [-0.7651,  0.2175,  0.2480,  ..., -1.3131,  0.3600, -0.0355]]],\n",
      "\n",
      "\n",
      "         [[[-0.1934,  0.6930,  0.7699,  ..., -0.2413, -1.1958, -0.5201],\n",
      "           [-1.2041,  0.2989,  0.1417,  ..., -0.0216,  0.8266, -0.0744],\n",
      "           [-0.2757, -0.3154, -0.3086,  ..., -0.3021, -1.2650,  1.1282],\n",
      "           ...,\n",
      "           [-0.2694, -0.8180, -0.1500,  ..., -0.7282, -0.5390, -1.6207],\n",
      "           [ 0.1587,  0.9813,  0.6254,  ...,  0.7565,  0.1853,  0.0876],\n",
      "           [ 0.7329,  0.0915,  0.2922,  ...,  0.3274,  0.8707, -0.5326]]],\n",
      "\n",
      "\n",
      "         [[[-0.1367,  0.6364, -0.7131,  ..., -0.3031, -0.2969,  1.0437],\n",
      "           [-0.7105, -0.9597,  0.2094,  ..., -0.3513, -1.0433, -0.7501],\n",
      "           [ 0.5731, -0.4905,  0.2303,  ...,  0.1407, -1.4731,  0.3341],\n",
      "           ...,\n",
      "           [ 0.7372, -0.0893, -0.5015,  ..., -0.1304, -0.4984, -0.5752],\n",
      "           [ 0.5467,  0.8678, -0.5788,  ...,  0.1575, -0.1103, -0.5035],\n",
      "           [-0.5658,  0.3952, -1.1354,  ...,  0.4202,  0.9405, -0.9595]]]]]), tensor([]), tensor([614.7997]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.0595e+00,  1.3957e-02, -1.2647e+00,  ..., -2.1891e-01,\n",
      "            -2.5239e-01, -1.9680e+00],\n",
      "           [-5.4918e-01, -1.7733e-01,  6.5789e-03,  ...,  9.8082e-01,\n",
      "             2.0844e+00, -1.9885e+00],\n",
      "           [-1.6266e+00,  1.7409e-02,  9.0224e-01,  ..., -4.1467e-01,\n",
      "             9.5535e-02, -7.4180e-01],\n",
      "           ...,\n",
      "           [-3.3952e-01, -5.7312e-01, -2.1637e-01,  ...,  4.9141e-01,\n",
      "            -4.9194e-01, -2.2457e+00],\n",
      "           [ 2.9390e-01,  4.7005e-01, -8.0345e-01,  ...,  9.5265e-01,\n",
      "            -7.8029e-01, -1.5512e-01],\n",
      "           [-9.8862e-01,  8.8944e-01, -9.8597e-01,  ..., -1.6033e+00,\n",
      "             1.1921e+00,  9.6705e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.4633e+00, -2.4501e+00, -8.5266e-01,  ..., -8.5386e-01,\n",
      "             3.1391e-01, -1.0545e+00],\n",
      "           [ 2.3618e+00,  6.8773e-01,  8.1164e-01,  ..., -4.4237e-01,\n",
      "            -1.7608e+00, -1.3115e+00],\n",
      "           [-3.0366e-01, -6.5004e-01,  7.4442e-01,  ...,  6.4093e-01,\n",
      "             8.9437e-01,  1.1365e+00],\n",
      "           ...,\n",
      "           [-7.5661e-01, -4.5902e-01, -6.5634e-01,  ...,  9.0450e-01,\n",
      "             6.4894e-01,  6.7789e-01],\n",
      "           [-9.2844e-02,  7.2485e-01,  1.4963e+00,  ...,  1.3661e+00,\n",
      "            -1.5240e+00,  4.2268e-01],\n",
      "           [ 6.5035e-01,  1.6003e+00,  2.0261e+00,  ...,  5.5483e-02,\n",
      "             1.8183e+00, -2.1438e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 9.0848e-01,  2.4095e+00,  1.2883e+00,  ...,  1.0650e+00,\n",
      "             1.9508e+00,  1.4747e+00],\n",
      "           [ 1.4847e+00,  3.9213e-01,  1.3563e+00,  ...,  2.2146e+00,\n",
      "             1.9599e+00, -9.8597e-01],\n",
      "           [ 1.0176e+00,  1.7616e+00,  2.4102e+00,  ...,  3.4766e-01,\n",
      "             4.8384e-02,  2.3781e-01],\n",
      "           ...,\n",
      "           [-1.4470e+00, -1.5775e-01,  1.3396e-01,  ...,  4.3857e-02,\n",
      "            -1.9049e+00, -7.7771e-01],\n",
      "           [ 1.2786e+00,  7.8969e-01, -3.1570e-01,  ...,  9.3965e-01,\n",
      "            -2.5668e+00, -1.6028e-01],\n",
      "           [ 1.6363e+00, -6.8865e-01, -1.6376e+00,  ..., -1.4308e+00,\n",
      "            -5.5774e-01,  8.6868e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-4.0323e-01, -5.9606e-01, -7.1077e-02,  ..., -1.5604e+00,\n",
      "            -1.2963e+00,  1.2444e+00],\n",
      "           [-1.9177e+00,  4.0578e-01, -4.8010e-01,  ...,  1.1820e+00,\n",
      "             1.0242e+00, -1.0184e+00],\n",
      "           [ 4.9715e-01, -2.4506e-02,  2.5096e+00,  ..., -9.4222e-01,\n",
      "             4.9790e-01, -1.0806e+00],\n",
      "           ...,\n",
      "           [-4.7930e-01,  1.4562e+00,  1.5217e+00,  ...,  1.0446e-01,\n",
      "            -4.9516e-01, -7.3506e-01],\n",
      "           [-1.4501e+00,  4.4401e-01, -2.7468e-01,  ...,  1.9928e-01,\n",
      "             4.3031e-01,  1.4048e+00],\n",
      "           [ 4.2852e-01,  1.8356e+00,  6.1480e-01,  ..., -9.3814e-01,\n",
      "             1.2787e+00,  5.8480e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.0595e-01,  1.0230e+00,  1.5525e+00,  ..., -1.7479e-01,\n",
      "            -1.5390e+00, -4.6952e-01],\n",
      "           [-1.6273e+00,  8.8195e-01,  6.1383e-01,  ...,  9.6538e-02,\n",
      "             1.7656e+00,  1.8300e-01],\n",
      "           [-2.5284e-01, -1.4813e-01, -2.2861e-01,  ..., -6.0544e-01,\n",
      "            -1.9895e+00,  1.9399e+00],\n",
      "           ...,\n",
      "           [ 1.8251e-01, -9.8368e-01,  1.7020e-01,  ..., -3.2018e-01,\n",
      "            -6.8701e-01, -2.0080e+00],\n",
      "           [ 3.6304e-01,  2.3881e+00,  1.0901e+00,  ...,  1.3682e+00,\n",
      "             6.8868e-01,  3.8972e-01],\n",
      "           [ 1.8404e+00,  3.3681e-01,  8.5426e-01,  ...,  1.1806e+00,\n",
      "             1.7941e+00, -2.1407e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.9767e-04,  1.0192e+00, -6.4242e-01,  ..., -6.7899e-02,\n",
      "             1.4761e-01,  2.3300e+00],\n",
      "           [-1.3394e+00, -1.1951e+00,  6.6185e-01,  ..., -4.3277e-01,\n",
      "            -1.4119e+00, -8.9516e-01],\n",
      "           [ 5.8253e-01, -9.9381e-01,  2.6182e-01,  ...,  4.1458e-01,\n",
      "            -2.3305e+00,  6.2679e-01],\n",
      "           ...,\n",
      "           [ 2.3388e-01, -2.7484e-01, -1.1075e+00,  ..., -3.6186e-01,\n",
      "            -2.5907e+00, -2.2886e+00],\n",
      "           [-3.7910e-01,  3.5416e-01, -1.5660e+00,  ..., -1.2976e+00,\n",
      "            -1.9338e+00, -2.1796e+00],\n",
      "           [-1.7362e+00, -4.4095e-01, -2.5541e+00,  ..., -7.9889e-01,\n",
      "             6.5497e-01, -1.6574e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-9.9729e-01,  1.7870e-01,  3.2960e-01,  ..., -4.5079e-01,\n",
      "            -5.2968e-01, -1.3576e-01],\n",
      "           [ 1.4260e-01, -3.5909e-01,  5.0885e-01,  ...,  2.2855e-01,\n",
      "             9.3074e-01,  1.1826e+00],\n",
      "           [ 2.8304e-01, -5.4192e-01, -3.2939e-01,  ..., -5.0997e-01,\n",
      "             1.3072e+00, -9.0906e-01],\n",
      "           ...,\n",
      "           [-1.2249e+00, -4.7317e-01,  2.2496e-01,  ...,  6.8650e-02,\n",
      "            -2.0301e-01, -1.2028e+00],\n",
      "           [-1.2774e-01,  1.3857e+00,  1.9072e+00,  ..., -6.1778e-01,\n",
      "             7.5383e-01,  1.1085e-01],\n",
      "           [ 1.0934e-01, -7.8228e-01, -2.4867e+00,  ..., -6.9309e-01,\n",
      "            -1.2809e+00,  1.6034e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3958e+00,  4.5235e-01, -2.8553e-01,  ...,  9.8854e-02,\n",
      "             3.5496e-01, -8.9456e-02],\n",
      "           [-7.1768e-01, -1.1578e+00, -9.4558e-01,  ...,  3.2210e-02,\n",
      "             1.4094e+00, -3.4006e-01],\n",
      "           [-8.2179e-01, -3.6425e-01,  3.7657e-01,  ...,  1.2128e+00,\n",
      "            -4.4294e-01, -3.8374e-01],\n",
      "           ...,\n",
      "           [-1.8919e-01, -2.5010e-01, -7.4886e-01,  ..., -1.3302e-01,\n",
      "             9.0802e-01, -3.0039e-01],\n",
      "           [-1.1819e-02, -1.5773e-01,  4.8402e-01,  ..., -5.7473e-01,\n",
      "             1.3173e-01,  1.1819e+00],\n",
      "           [-3.1811e-01,  1.3980e+00, -4.6175e-01,  ...,  3.4393e-01,\n",
      "            -5.2893e-02,  5.2309e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 9.6847e-01, -9.5061e-01,  9.6875e-02,  ..., -4.3785e-01,\n",
      "             6.3448e-01,  1.3453e+00],\n",
      "           [-1.0104e+00, -1.0495e+00,  5.9933e-01,  ...,  4.7385e-01,\n",
      "            -6.7302e-01, -8.3921e-01],\n",
      "           [-5.5335e-01,  2.4632e-04, -9.7185e-01,  ...,  1.1178e-01,\n",
      "            -4.6578e-01, -2.1349e-01],\n",
      "           ...,\n",
      "           [ 1.0432e+00,  5.8386e-02,  4.3682e-01,  ..., -7.6388e-01,\n",
      "             4.7997e-01,  5.1162e-01],\n",
      "           [ 4.9093e-01, -5.0239e-01, -6.2750e-02,  ...,  3.1480e-01,\n",
      "            -1.6383e-01, -3.9919e-01],\n",
      "           [-1.0463e+00,  9.4483e-01,  5.0596e-01,  ...,  8.8352e-01,\n",
      "             2.5783e-01, -1.4150e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 9.6163e-01, -5.8472e-02,  1.3554e-02,  ...,  6.6519e-01,\n",
      "             7.4643e-01,  5.6355e-01],\n",
      "           [-1.6085e-01,  1.3714e+00, -1.0776e-01,  ...,  1.4425e+00,\n",
      "             1.3744e-01, -4.3656e-01],\n",
      "           [-1.0327e+00,  1.0297e+00,  1.3622e+00,  ..., -1.1188e+00,\n",
      "            -5.6700e-01,  8.7504e-02],\n",
      "           ...,\n",
      "           [-2.7587e-02, -3.6896e-01,  3.1066e-01,  ...,  7.1930e-01,\n",
      "             3.7791e-01, -6.6710e-01],\n",
      "           [ 2.9348e-01, -9.2892e-01, -1.1508e+00,  ..., -6.8815e-01,\n",
      "            -4.0224e-03, -6.1940e-01],\n",
      "           [-1.1843e+00,  4.0090e-02,  5.1957e-01,  ..., -1.0645e+00,\n",
      "            -9.7299e-01, -1.5601e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1468e+00, -6.8914e-01,  4.6063e-01,  ..., -1.1890e+00,\n",
      "            -1.8725e+00, -6.0769e-01],\n",
      "           [ 2.1244e-01,  1.3256e+00, -2.4102e-01,  ..., -1.0267e+00,\n",
      "            -2.4595e-01, -1.6470e-03],\n",
      "           [-1.4390e-01, -1.4426e-01, -8.5828e-01,  ..., -2.9876e-02,\n",
      "             2.5361e-01,  6.3404e-01],\n",
      "           ...,\n",
      "           [-1.4089e+00, -2.7861e-01,  2.5370e-01,  ..., -4.5369e-01,\n",
      "            -4.3275e-01, -9.0289e-02],\n",
      "           [ 1.2866e-01,  8.3818e-01, -5.8661e-01,  ..., -1.7875e-01,\n",
      "            -1.0739e-01, -3.2924e-01],\n",
      "           [-1.2696e-01,  1.7054e-02,  3.4624e-01,  ..., -3.3369e-01,\n",
      "             4.6563e-01,  1.3885e+00]]],\n",
      "\n",
      "\n",
      "         [[[-6.9915e-01, -3.2325e-01, -4.4007e-01,  ...,  1.0276e+00,\n",
      "             6.1238e-01, -1.3094e+00],\n",
      "           [-5.6857e-02, -3.3390e-01,  9.2318e-01,  ...,  9.8954e-01,\n",
      "             5.4245e-02, -1.3502e+00],\n",
      "           [-2.7390e-02,  4.8673e-01,  2.1048e-02,  ..., -4.9393e-01,\n",
      "            -4.2575e-01,  7.2863e-01],\n",
      "           ...,\n",
      "           [ 5.9779e-01, -2.4483e-01, -7.2492e-01,  ...,  1.2172e+00,\n",
      "             1.5682e-01, -2.6042e-01],\n",
      "           [ 9.9721e-01, -1.1683e+00, -6.6578e-01,  ...,  4.9553e-01,\n",
      "             4.5267e-01,  1.1088e+00],\n",
      "           [-4.8880e-01,  1.5981e+00,  3.0242e-01,  ...,  1.0948e+00,\n",
      "            -9.8000e-01, -7.5056e-01]]]]]), tensor([]), tensor([745.1884]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-2.0576e+00, -4.5198e-01, -4.1071e-01,  ..., -1.3796e+00,\n",
      "            -1.5767e+00, -1.0161e+00],\n",
      "           [-3.4628e-01, -9.3517e-01,  2.3539e-01,  ...,  1.8142e-01,\n",
      "             1.0065e+00,  1.2234e+00],\n",
      "           [-1.7398e-01, -1.2691e+00, -1.0343e+00,  ..., -1.0888e+00,\n",
      "             1.3841e+00, -1.6339e+00],\n",
      "           ...,\n",
      "           [-1.3210e+00, -1.0039e+00,  2.6422e-01,  ...,  9.3680e-01,\n",
      "             5.9983e-01, -1.0535e+00],\n",
      "           [-4.2874e-02,  2.0643e+00,  2.5859e+00,  ..., -1.1309e-01,\n",
      "             1.8140e+00,  7.2822e-01],\n",
      "           [ 1.4224e-01, -1.0137e+00, -3.2461e+00,  ..., -2.7513e-01,\n",
      "            -1.1526e+00,  2.7081e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 9.7816e-01, -3.6616e-01, -1.1795e+00,  ..., -5.9614e-01,\n",
      "            -4.8823e-01, -1.0574e+00],\n",
      "           [-1.6443e+00, -2.2545e+00, -2.2343e+00,  ..., -8.4458e-01,\n",
      "             1.1343e+00, -1.0123e+00],\n",
      "           [-2.3526e+00, -1.8272e+00, -8.5926e-01,  ...,  6.7737e-01,\n",
      "            -1.4574e+00, -1.1107e+00],\n",
      "           ...,\n",
      "           [ 7.4105e-02, -3.2214e-01, -9.2034e-01,  ..., -7.5763e-02,\n",
      "             8.2983e-01, -2.6208e-03],\n",
      "           [-3.3118e-01,  6.0639e-01,  2.9791e-01,  ..., -1.0965e+00,\n",
      "            -6.6010e-01,  1.5333e+00],\n",
      "           [-4.8067e-02,  1.9389e+00, -8.8454e-01,  ...,  8.9094e-02,\n",
      "            -3.9290e-01, -3.7189e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.3249e+00, -4.6072e-01,  1.4580e+00,  ...,  8.5184e-01,\n",
      "             2.3078e+00,  2.9950e+00],\n",
      "           [-5.5099e-01,  2.7637e-02,  2.1814e+00,  ...,  1.9721e+00,\n",
      "             5.6478e-01,  1.7690e-01],\n",
      "           [ 3.8159e-01,  1.3235e+00,  1.7156e-01,  ...,  1.4209e+00,\n",
      "             6.4232e-01,  7.8263e-01],\n",
      "           ...,\n",
      "           [ 1.1859e+00,  4.1817e-01,  1.0964e+00,  ..., -1.2908e+00,\n",
      "             2.7312e-01, -3.0277e-01],\n",
      "           [ 2.9112e-01, -5.6039e-01, -1.0598e-01,  ...,  4.0617e-01,\n",
      "            -4.3352e-01, -1.5586e+00],\n",
      "           [-1.1847e+00,  1.5878e+00,  9.8414e-01,  ...,  7.7670e-01,\n",
      "             1.5262e-01, -2.3086e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.1308e+00, -6.6262e-02,  2.9978e-01,  ...,  1.2654e+00,\n",
      "             9.5452e-01,  3.6832e-01],\n",
      "           [-1.2395e+00,  1.2478e+00, -9.1898e-01,  ...,  9.6885e-01,\n",
      "            -1.0387e+00, -1.6003e+00],\n",
      "           [-8.5124e-01,  2.2215e+00,  2.4844e+00,  ..., -1.2206e+00,\n",
      "            -4.1939e-01,  2.5179e-01],\n",
      "           ...,\n",
      "           [ 5.3927e-01,  1.1793e-01,  9.4864e-01,  ...,  1.2558e+00,\n",
      "             3.8719e-01, -2.2941e-01],\n",
      "           [ 8.6715e-01, -7.0642e-02, -1.4374e+00,  ..., -1.1368e+00,\n",
      "             2.8164e-01, -1.5057e-01],\n",
      "           [-8.0259e-02,  1.2539e+00,  8.4307e-01,  ..., -1.5176e+00,\n",
      "            -1.0324e+00,  1.7724e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.4074e+00, -1.0530e+00,  9.4295e-01,  ..., -1.2618e+00,\n",
      "            -2.1599e+00, -5.5502e-01],\n",
      "           [ 4.5929e-01,  2.0308e+00, -1.0469e-01,  ..., -1.2836e+00,\n",
      "            -5.8943e-02,  3.8239e-01],\n",
      "           [ 2.5988e-02, -1.0752e-02, -1.0464e+00,  ..., -1.5969e-01,\n",
      "             2.0061e-01,  8.3551e-01],\n",
      "           ...,\n",
      "           [-1.3639e+00, -1.0776e-01,  5.7092e-01,  ..., -3.0799e-02,\n",
      "            -4.0907e-01,  1.4881e-01],\n",
      "           [ 2.4191e-01,  1.8224e+00, -6.6921e-01,  ..., -1.2139e-02,\n",
      "             9.1318e-02, -2.4087e-02],\n",
      "           [ 3.6239e-01,  1.3861e-01,  7.7584e-01,  ..., -2.4934e-01,\n",
      "             7.9208e-01,  2.0920e+00]]],\n",
      "\n",
      "\n",
      "         [[[-7.0358e-01, -3.4665e-01, -1.3814e-01,  ...,  1.8740e+00,\n",
      "             1.5405e+00, -1.1687e+00],\n",
      "           [-7.7468e-02, -1.7662e-02,  1.5721e+00,  ...,  1.5617e+00,\n",
      "             3.8483e-01, -1.4367e+00],\n",
      "           [-2.0225e-01,  6.0276e-01,  1.1148e-01,  ..., -5.0760e-01,\n",
      "            -4.3537e-01,  1.1989e+00],\n",
      "           ...,\n",
      "           [-1.3116e-02, -4.7541e-01, -1.2648e+00,  ...,  2.6486e-01,\n",
      "            -6.8528e-01, -1.3631e+00],\n",
      "           [ 2.6561e-01, -2.4838e+00, -1.3980e+00,  ..., -4.1302e-01,\n",
      "            -6.0948e-01,  4.5201e-01],\n",
      "           [-1.3396e+00,  1.2168e+00, -1.7770e-01,  ...,  7.7464e-01,\n",
      "            -2.0702e+00, -1.5252e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 1.1820, -1.1274, -0.0851,  ..., -0.3619, -0.2228, -0.8249],\n",
      "           [ 0.6165, -0.5783, -0.2030,  ...,  0.6552,  2.3236,  0.9291],\n",
      "           [-0.1318,  0.1495, -0.4340,  ..., -0.1559,  0.6137, -0.4295],\n",
      "           ...,\n",
      "           [-0.5386, -0.1953, -0.3501,  ...,  0.7467, -1.5352,  2.0640],\n",
      "           [-0.0692, -0.7159, -0.6012,  ..., -0.0725, -1.1250, -0.0078],\n",
      "           [ 1.0307,  0.2347, -0.1462,  ..., -0.8404, -0.7306, -0.8279]]],\n",
      "\n",
      "\n",
      "         [[[-0.0602,  0.4788,  1.1366,  ...,  1.1180,  1.3938,  0.9760],\n",
      "           [ 0.4674, -0.5078,  2.1861,  ...,  0.2678,  1.2249, -0.0719],\n",
      "           [-0.8949,  2.6963,  0.9220,  ...,  0.2711,  1.3179,  1.3700],\n",
      "           ...,\n",
      "           [-1.0219,  0.4927, -1.0638,  ..., -0.0034, -0.9797,  0.5322],\n",
      "           [-1.6147, -0.0290,  0.6804,  ..., -0.9009, -0.3500,  0.2565],\n",
      "           [ 0.3214, -1.8003,  0.4540,  ..., -0.9478,  1.2784,  0.5475]]],\n",
      "\n",
      "\n",
      "         [[[-0.8801,  0.1896,  0.8547,  ..., -0.9378,  0.9783, -0.7698],\n",
      "           [-0.4306,  2.0641, -0.1633,  ...,  0.0243,  0.4253, -0.1768],\n",
      "           [ 0.2090,  0.8636,  1.2927,  ..., -0.2761, -0.6670,  0.5833],\n",
      "           ...,\n",
      "           [-2.3976,  1.6114,  1.0396,  ...,  1.7980,  0.4746, -1.6237],\n",
      "           [ 1.8413,  0.5386, -0.8829,  ..., -0.1091, -0.6870, -1.0293],\n",
      "           [-0.7104, -0.0422, -2.8314,  ...,  0.3326, -0.3273,  0.5155]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.2711, -0.3394, -1.3344,  ..., -0.4023, -0.2743,  0.8723],\n",
      "           [ 0.2748,  1.5907, -0.1202,  ..., -0.9868, -0.9187, -0.4229],\n",
      "           [ 0.7687,  0.1944, -0.8116,  ...,  0.2776,  1.7881,  1.0292],\n",
      "           ...,\n",
      "           [-0.8734,  0.2117, -1.3401,  ..., -0.6196, -0.3397,  0.3453],\n",
      "           [ 0.1710, -0.2659, -0.1618,  ...,  0.0204,  0.2970,  1.1771],\n",
      "           [-0.6774, -0.1532, -1.4662,  ..., -0.1012,  1.3837, -1.0525]]],\n",
      "\n",
      "\n",
      "         [[[-1.2399, -0.6721, -1.8147,  ..., -0.4550,  0.5572, -0.6345],\n",
      "           [-0.8311, -0.3386, -0.6732,  ..., -0.5593, -1.4132, -0.8986],\n",
      "           [ 0.9335, -0.4537,  0.1315,  ..., -0.3097,  1.7016, -0.9541],\n",
      "           ...,\n",
      "           [ 0.3171,  0.7068, -0.1254,  ..., -1.0070,  0.7016,  0.4055],\n",
      "           [-1.3023,  0.5715,  0.3822,  ...,  0.8179,  0.0109, -0.7171],\n",
      "           [-1.1599,  1.2879, -0.4691,  ..., -0.2040,  1.0337,  0.2109]]],\n",
      "\n",
      "\n",
      "         [[[-1.2102,  0.4771,  0.5823,  ...,  1.6221, -0.1519, -0.1749],\n",
      "           [ 0.1030,  0.9016,  0.8588,  ...,  0.0759,  0.6375,  0.6433],\n",
      "           [-0.9699, -0.7195,  0.6121,  ..., -0.5872, -0.4967,  1.0104],\n",
      "           ...,\n",
      "           [ 0.9431, -1.4432,  0.9164,  ..., -0.2132,  0.5045, -0.6705],\n",
      "           [-0.6287, -1.7007, -1.5914,  ...,  0.2499,  0.1866,  0.5232],\n",
      "           [ 0.3054,  0.0081,  0.0253,  ...,  0.4395,  1.1538,  1.1512]]]]]), tensor([]), tensor([866.6936]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.6920, -1.8966, -0.7245,  ..., -1.1518, -0.9452, -1.6075],\n",
      "           [ 0.1609, -1.0181, -0.6464,  ...,  0.5594,  2.5428,  0.7614],\n",
      "           [-0.6998, -0.3175, -0.9502,  ..., -0.4529,  0.3653, -0.8696],\n",
      "           ...,\n",
      "           [-0.3371, -0.5451, -0.4533,  ...,  1.4949, -1.3520,  3.0726],\n",
      "           [ 0.0411, -0.6540, -0.6769,  ...,  0.5177, -0.6079,  0.5317],\n",
      "           [ 1.1707,  0.3310, -0.1164,  ..., -0.4922, -0.3861, -0.4729]]],\n",
      "\n",
      "\n",
      "         [[[-0.8724, -0.2480,  0.8264,  ...,  0.5828,  1.0386,  0.5405],\n",
      "           [-0.0684, -1.2410,  1.7179,  ..., -0.3136,  0.7346, -0.5845],\n",
      "           [-2.0953,  1.8246, -0.2415,  ..., -0.4987,  0.6099,  0.9069],\n",
      "           ...,\n",
      "           [-0.9092,  0.6009, -1.1953,  ..., -0.2048, -1.2027,  0.5344],\n",
      "           [-2.0985,  0.7172,  0.5166,  ..., -1.8114, -0.0820,  0.3960],\n",
      "           [ 0.5791, -1.9708,  0.2682,  ..., -1.3514,  0.8332,  0.6280]]],\n",
      "\n",
      "\n",
      "         [[[-0.0722,  0.9464,  2.1684,  ...,  0.1016,  2.3312,  0.1814],\n",
      "           [ 0.2178,  3.4462,  0.9576,  ...,  1.3284,  1.7136,  0.9909],\n",
      "           [ 1.0751,  1.9389,  2.5095,  ...,  0.6560,  0.1769,  1.5810],\n",
      "           ...,\n",
      "           [-2.9466,  2.1457,  1.5917,  ...,  2.0575, -0.1994, -2.5691],\n",
      "           [ 1.7937,  0.7613, -1.0305,  ..., -0.4513, -1.4818, -1.7541],\n",
      "           [-0.6243,  0.1791, -2.9763,  ...,  0.1999, -0.4768,  0.0438]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.5513, -0.4584, -1.3103,  ..., -0.1184,  0.1313,  1.2101],\n",
      "           [-0.5392,  1.4502, -0.8308,  ..., -1.9328, -2.0756, -1.1163],\n",
      "           [ 1.6017,  0.7810, -0.3694,  ...,  0.7938,  2.4721,  1.5049],\n",
      "           ...,\n",
      "           [-0.5440,  0.6363, -1.1801,  ..., -0.1288,  0.6777,  0.4457],\n",
      "           [ 0.5779,  0.6065, -0.1493,  ...,  0.0415,  1.3854,  1.3619],\n",
      "           [ 0.3272,  0.7054, -1.5200,  ...,  0.1815,  1.6781,  0.0893]]],\n",
      "\n",
      "\n",
      "         [[[-1.3465, -0.8535, -1.7656,  ..., -0.2509,  0.9147, -0.4656],\n",
      "           [-0.8682, -0.1101, -0.5309,  ..., -0.4943, -1.3215, -0.7999],\n",
      "           [ 1.3225, -0.2992,  0.3934,  ..., -0.3365,  1.9070, -1.0899],\n",
      "           ...,\n",
      "           [ 0.7874,  1.0488,  0.0952,  ..., -1.0982,  1.0184,  0.4658],\n",
      "           [-1.4115,  1.2383,  0.5138,  ...,  0.7527,  0.1056, -0.8164],\n",
      "           [-0.8191,  1.7164, -0.3133,  ..., -0.0378,  1.1308,  0.5077]]],\n",
      "\n",
      "\n",
      "         [[[-1.2068,  0.6091,  1.1725,  ...,  2.2997,  0.2350,  0.1917],\n",
      "           [ 0.1487,  1.3736,  1.3833,  ...,  0.2916,  1.0336,  1.0274],\n",
      "           [-1.1482, -0.9077,  0.7491,  ..., -0.6446, -0.5158,  1.2276],\n",
      "           ...,\n",
      "           [ 0.4255, -1.7198,  0.8185,  ..., -1.1650, -0.6276, -1.4877],\n",
      "           [-1.6101, -2.7203, -2.2895,  ..., -0.6754, -1.1248, -0.6503],\n",
      "           [-0.2265, -0.7167, -0.4739,  ..., -0.2060,  0.8654,  0.0188]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 4.9395e-01,  1.4685e-01, -2.9236e-01,  ..., -9.7111e-01,\n",
      "            -2.3225e-01,  4.5046e-01],\n",
      "           [-5.6458e-01,  6.4366e-01,  1.3562e+00,  ...,  1.1048e+00,\n",
      "             1.3832e+00,  4.1357e-01],\n",
      "           [-2.3378e-01,  1.5555e-01, -1.2140e+00,  ..., -2.9743e+00,\n",
      "             9.8164e-01, -1.6666e+00],\n",
      "           ...,\n",
      "           [ 3.7348e-01, -8.6138e-01, -5.3343e-01,  ...,  2.3555e-01,\n",
      "            -3.4016e-01, -1.6197e-01],\n",
      "           [ 2.2735e-01, -6.4306e-01, -7.8067e-01,  ...,  9.3677e-01,\n",
      "            -1.9710e-01, -1.6771e-01],\n",
      "           [ 5.8101e-01, -1.1612e+00,  1.2852e+00,  ..., -3.9924e-01,\n",
      "            -1.3946e+00, -3.3778e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.8456e-02,  2.8044e-01, -1.1294e+00,  ..., -7.4175e-01,\n",
      "            -5.0112e-01,  1.0508e+00],\n",
      "           [ 2.2363e-01, -1.0428e+00,  7.0169e-01,  ..., -6.4919e-01,\n",
      "             1.6333e+00, -5.5936e-01],\n",
      "           [-7.8639e-03, -2.1792e-01, -9.0976e-01,  ...,  1.3719e+00,\n",
      "            -7.4031e-01, -1.1908e+00],\n",
      "           ...,\n",
      "           [ 9.9846e-01,  9.2229e-02, -6.7498e-01,  ..., -3.6515e-01,\n",
      "            -2.2728e-02,  1.7770e+00],\n",
      "           [ 1.8344e+00,  6.5534e-01,  1.3904e+00,  ..., -8.6358e-01,\n",
      "            -3.3398e-01, -1.3049e+00],\n",
      "           [ 5.3500e-01, -6.3317e-01, -5.2125e-01,  ...,  3.0989e-01,\n",
      "             4.2993e-01, -1.4114e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.7369e+00,  2.4053e-02, -4.0253e-01,  ...,  1.7418e+00,\n",
      "             5.1626e-01, -1.0378e-01],\n",
      "           [ 9.1588e-01,  2.3232e-01,  4.3972e-01,  ..., -6.4035e-01,\n",
      "            -2.5037e-02,  1.0684e+00],\n",
      "           [ 3.5801e-01, -1.7583e+00,  1.0713e+00,  ...,  2.4811e-02,\n",
      "            -3.1206e-01, -4.2003e-01],\n",
      "           ...,\n",
      "           [-1.9029e+00, -1.4383e+00,  1.6734e+00,  ..., -1.8478e-01,\n",
      "             1.3695e+00, -6.2499e-01],\n",
      "           [-5.5975e-01, -2.7407e+00, -5.9804e-01,  ..., -2.0785e-01,\n",
      "             9.3495e-01,  3.3393e-01],\n",
      "           [ 1.1459e-01, -8.4124e-01, -1.7396e-01,  ...,  1.7677e-01,\n",
      "            -1.2257e-01,  2.9072e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.8553e-01, -4.3417e-01, -6.4659e-01,  ...,  1.8708e-01,\n",
      "             1.6470e-02, -8.6206e-01],\n",
      "           [-8.4985e-01, -7.8263e-01,  8.6079e-02,  ...,  2.4090e+00,\n",
      "             7.1116e-02,  4.8957e-01],\n",
      "           [ 3.3916e-01,  3.9692e-01,  1.5543e-01,  ...,  9.0393e-01,\n",
      "            -9.9750e-02, -4.0728e-02],\n",
      "           ...,\n",
      "           [-1.0905e+00,  1.6000e-01, -2.1404e-01,  ..., -4.0297e-01,\n",
      "            -3.7853e-01, -1.4298e-02],\n",
      "           [ 4.1529e-01,  1.0490e-01,  1.0681e+00,  ..., -9.2913e-01,\n",
      "            -2.4042e-01, -4.1876e-01],\n",
      "           [-4.1073e-01,  3.0662e-01,  2.5172e-01,  ..., -1.0872e+00,\n",
      "            -8.9590e-01, -1.7485e+00]]],\n",
      "\n",
      "\n",
      "         [[[-8.6943e-01, -1.1191e-01,  4.2578e-01,  ...,  1.5679e-01,\n",
      "            -1.3292e+00, -1.4000e+00],\n",
      "           [-1.8427e-01, -1.9860e+00,  6.7106e-01,  ...,  1.0269e+00,\n",
      "             1.5609e-03, -3.5698e-01],\n",
      "           [ 2.1899e-01, -9.0246e-01,  1.4745e+00,  ..., -1.9347e-01,\n",
      "            -1.5963e+00, -9.1034e-01],\n",
      "           ...,\n",
      "           [ 2.8591e+00,  1.3252e+00, -5.7519e-01,  ...,  5.7852e-01,\n",
      "             8.8494e-02,  5.2812e-01],\n",
      "           [ 5.5377e-01, -2.2204e-01, -6.2369e-01,  ..., -9.4091e-01,\n",
      "            -3.4159e-01,  6.9014e-01],\n",
      "           [-1.5160e+00,  1.1318e+00,  5.5970e-01,  ...,  5.7111e-01,\n",
      "             7.3224e-01,  1.4782e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 3.0290e-01, -3.6436e-01,  2.0151e-01,  ...,  4.6398e-02,\n",
      "            -1.4107e-01, -2.5521e-01],\n",
      "           [ 9.0762e-01, -1.3003e+00, -1.5381e+00,  ...,  1.3001e+00,\n",
      "             2.0733e-01,  4.1096e-01],\n",
      "           [ 3.8075e-01, -8.7343e-01,  1.9373e+00,  ...,  1.3248e+00,\n",
      "             8.5345e-01,  1.6539e-01],\n",
      "           ...,\n",
      "           [-1.4554e+00,  2.4082e-01,  1.7777e-01,  ..., -1.5593e+00,\n",
      "            -7.2349e-01, -6.1592e-01],\n",
      "           [ 8.2868e-01, -3.1510e-01, -1.5343e-01,  ...,  7.2106e-01,\n",
      "            -4.6416e-01,  9.0477e-01],\n",
      "           [ 1.6529e+00,  1.3125e+00,  7.7686e-01,  ..., -1.5119e-01,\n",
      "            -3.6832e-01, -8.0139e-01]]]]]), tensor([]), tensor([928.4009]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.0267, -0.4440, -0.9605,  ..., -1.7151, -0.8591, -0.0968],\n",
      "           [-1.1343,  0.3923,  1.1683,  ...,  1.0541,  1.3188,  0.1903],\n",
      "           [-0.7179, -0.3015, -1.8352,  ..., -3.5253,  0.7148, -2.1845],\n",
      "           ...,\n",
      "           [ 0.6625, -1.1933, -0.5930,  ...,  0.8483,  0.1669,  0.4873],\n",
      "           [ 0.3745, -0.5173, -0.8016,  ...,  1.5627,  0.3782,  0.3306],\n",
      "           [ 0.6068, -1.1759,  1.4233,  ...,  0.0680, -0.9455,  0.1153]]],\n",
      "\n",
      "\n",
      "         [[[-0.6587, -0.5574, -1.8146,  ..., -1.4581, -1.1668,  0.5685],\n",
      "           [-0.2652, -1.7223,  0.1172,  ..., -1.3493,  1.1260, -1.1368],\n",
      "           [-0.8484, -1.4163, -2.0752,  ...,  0.6361, -1.4609, -1.6472],\n",
      "           ...,\n",
      "           [ 1.3686,  0.1295, -0.6755,  ..., -0.5531, -0.1014,  2.0346],\n",
      "           [ 1.7322,  1.4021,  1.2770,  ..., -1.5067, -0.2107, -1.3252],\n",
      "           [ 0.7896, -0.5191, -0.8090,  ..., -0.0461, -0.1634, -0.9957]]],\n",
      "\n",
      "\n",
      "         [[[-1.0276,  0.7199,  0.6348,  ...,  2.9772,  1.6704,  0.8348],\n",
      "           [ 1.6887,  1.3627,  1.5893,  ...,  0.4924,  1.1606,  2.2617],\n",
      "           [ 1.3247, -0.8698,  2.4310,  ...,  0.9554,  0.5994,  0.3904],\n",
      "           ...,\n",
      "           [-2.2220, -1.2843,  2.1650,  ..., -0.0688,  0.8270, -1.3285],\n",
      "           [-0.9016, -2.8179, -0.6667,  ..., -0.4003,  0.6058, -0.3420],\n",
      "           [ 0.3244, -0.6608,  0.0853,  ...,  0.1394, -0.2702, -0.1271]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.3633, -0.5463, -0.3642,  ...,  0.4396,  0.2359, -0.8947],\n",
      "           [-1.7605, -1.1489, -0.6272,  ...,  1.7808, -0.9274, -0.2030],\n",
      "           [ 0.7979,  0.8143,  0.2919,  ...,  1.2830,  0.1733,  0.0369],\n",
      "           ...,\n",
      "           [-0.7166,  0.5872,  0.1667,  ...,  0.1159,  0.3420, -0.1309],\n",
      "           [ 0.7997,  1.0072,  1.2565,  ..., -0.5928,  0.6041, -0.0637],\n",
      "           [ 0.5696,  1.1754,  0.4365,  ..., -0.6092, -0.5811, -0.5725]]],\n",
      "\n",
      "\n",
      "         [[[-0.7635, -0.1740,  0.7703,  ...,  0.4205, -1.1653, -1.2992],\n",
      "           [-0.0908, -1.8575,  0.9606,  ...,  1.2498,  0.2261, -0.2078],\n",
      "           [ 0.4722, -0.8343,  1.5795,  ..., -0.2750, -1.6532, -0.8677],\n",
      "           ...,\n",
      "           [ 3.4878,  1.6424, -0.3703,  ...,  0.9158,  0.4274,  0.5741],\n",
      "           [ 0.6563,  0.2814, -0.5904,  ..., -0.9486, -0.0965,  1.0115],\n",
      "           [-1.1530,  1.4380,  0.8011,  ...,  0.8369,  0.7342,  2.0541]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5294, -0.3132,  0.6159,  ...,  0.3734,  0.3363,  0.1150],\n",
      "           [ 1.0237, -0.9797, -1.2527,  ...,  1.5861,  0.4822,  0.7129],\n",
      "           [ 0.3719, -0.9638,  2.1375,  ...,  1.3508,  0.9032,  0.3934],\n",
      "           ...,\n",
      "           [-2.1893,  0.1823, -0.0436,  ..., -2.8194, -1.9695, -1.2035],\n",
      "           [ 0.0799, -1.0387, -0.5894,  ..., -0.4374, -1.5511, -0.0818],\n",
      "           [ 1.2320,  0.7327,  0.3653,  ..., -0.9949, -1.1991, -1.7528]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.4667,  0.4362,  0.4647,  ...,  0.5707,  0.5740,  0.2735],\n",
      "           [ 0.3850,  0.2807,  0.1701,  ...,  0.0457,  0.1017,  0.0982],\n",
      "           [ 0.3800,  0.4925,  0.4940,  ...,  0.1380,  0.3346,  0.2366],\n",
      "           ...,\n",
      "           [-0.2586,  0.1676,  0.0346,  ..., -0.3158, -0.6665, -0.4718],\n",
      "           [-0.1134, -0.2166, -0.1849,  ..., -0.5322, -0.8088, -0.4497],\n",
      "           [ 0.0372, -0.0903, -0.0229,  ..., -0.3310, -0.4687, -0.3723]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5716,  0.6143,  0.5093,  ...,  0.5621,  0.5061,  0.4104],\n",
      "           [ 0.5612,  0.6720,  0.5416,  ...,  0.6733,  0.5138,  0.3631],\n",
      "           [ 0.5083,  0.9143,  0.8426,  ...,  0.5755,  0.3981,  0.2370],\n",
      "           ...,\n",
      "           [-0.2638,  0.0722, -0.2209,  ...,  0.1387, -0.0666,  0.0270],\n",
      "           [ 0.0883, -0.5351,  0.2033,  ..., -0.1481,  0.0728, -0.1399],\n",
      "           [-0.2412, -0.0764,  0.1484,  ...,  0.0135, -0.0021,  0.0506]]],\n",
      "\n",
      "\n",
      "         [[[-0.7863, -0.6065, -1.0625,  ..., -1.0129, -1.1032, -0.8782],\n",
      "           [-0.6326, -0.9975, -0.8201,  ..., -0.9587, -0.8505, -0.8321],\n",
      "           [-0.7635, -0.9082, -0.8557,  ..., -0.8687, -0.9466, -0.6208],\n",
      "           ...,\n",
      "           [ 0.0112, -0.2760, -0.2181,  ..., -0.1127, -0.0396,  0.5830],\n",
      "           [ 0.3339, -0.2687,  0.0409,  ...,  0.2366,  0.4108,  0.1592],\n",
      "           [-0.1318, -0.3888, -0.3121,  ...,  0.4025, -0.0237, -0.2977]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.0523,  0.1473, -0.4000,  ..., -0.2596,  0.0889,  0.0190],\n",
      "           [ 0.7179,  0.6354,  0.3382,  ...,  0.6734,  0.6624,  0.6196],\n",
      "           [-0.2040, -0.5598, -0.1885,  ..., -0.0975, -0.2100, -0.2206],\n",
      "           ...,\n",
      "           [-0.4530, -0.4293, -0.3681,  ..., -0.1542, -0.1335, -0.4849],\n",
      "           [-0.2235, -0.8776, -0.1102,  ..., -0.0650,  0.0089,  0.1265],\n",
      "           [-0.9588, -1.0223, -0.2950,  ..., -0.1232, -0.1602, -0.2184]]],\n",
      "\n",
      "\n",
      "         [[[-0.1565,  0.1784, -0.4367,  ..., -0.1578, -0.1483, -0.3444],\n",
      "           [-0.1356, -0.1940, -0.2003,  ..., -0.0657, -0.1021, -0.1057],\n",
      "           [-0.0650, -0.1361, -0.2229,  ...,  0.0794, -0.1556, -0.0845],\n",
      "           ...,\n",
      "           [-0.2929, -0.1475, -0.2181,  ..., -0.3378, -0.3212,  0.0321],\n",
      "           [ 0.0589, -0.5033, -0.0561,  ..., -0.1360,  0.0636, -0.4403],\n",
      "           [-0.4697, -0.0785, -0.1674,  ..., -0.2354, -0.1006, -0.5876]]],\n",
      "\n",
      "\n",
      "         [[[-0.3321, -0.1793, -0.2632,  ..., -0.4355, -0.3635, -0.4564],\n",
      "           [ 0.1353, -0.1235, -0.2221,  ..., -0.1657, -0.3011, -0.2378],\n",
      "           [ 0.1429,  0.1148, -0.0013,  ..., -0.1375,  0.0585, -0.1597],\n",
      "           ...,\n",
      "           [ 0.5453,  0.2360,  0.3690,  ...,  0.2818,  0.5250,  0.7635],\n",
      "           [ 0.6763,  0.5381,  0.2582,  ...,  0.7710,  0.6737,  0.4854],\n",
      "           [ 0.3976,  0.7886,  0.3891,  ...,  0.7768,  0.5002,  0.1467]]]]]), tensor([]), tensor([85.8417]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-0.3237, -0.6183, -1.0701,  ..., -0.3635,  0.9915, -2.0471],\n",
      "           [-0.9280, -0.8408, -1.8581,  ..., -1.0682, -0.5759, -1.6738],\n",
      "           [-1.2671,  0.8851,  0.9308,  ..., -1.4676,  0.0378, -1.1752],\n",
      "           ...,\n",
      "           [ 0.1272, -1.0195,  0.8037,  ...,  0.4094, -0.6354,  0.2574],\n",
      "           [ 0.0322, -0.9560, -2.0233,  ...,  0.3381, -1.4493,  0.6063],\n",
      "           [-0.0591, -0.3011,  0.4059,  ...,  1.4784,  0.5069,  0.2325]]],\n",
      "\n",
      "\n",
      "         [[[-0.7162, -1.1763, -0.3089,  ..., -1.4782, -1.2445, -0.5561],\n",
      "           [-0.4335,  0.6510, -1.2922,  ...,  0.6489,  0.3876, -0.2474],\n",
      "           [-3.4260,  0.5379, -3.1022,  ...,  0.1902, -1.3077, -2.9380],\n",
      "           ...,\n",
      "           [-0.4249,  0.6063, -1.3374,  ...,  0.2725, -0.3554, -0.9749],\n",
      "           [-1.5432,  0.6978, -0.0491,  ..., -2.2122, -0.1027, -1.4042],\n",
      "           [-0.1482,  0.9812, -0.6788,  ..., -1.1705, -1.1642,  1.4313]]],\n",
      "\n",
      "\n",
      "         [[[-0.2073,  1.1463, -0.4528,  ...,  0.2999, -0.7387,  0.8437],\n",
      "           [-0.2413,  0.7552,  2.8795,  ...,  0.7376,  2.1991,  1.7788],\n",
      "           [ 1.6204,  0.6894,  2.2593,  ...,  0.4024, -0.3273,  1.2913],\n",
      "           ...,\n",
      "           [-1.5927, -0.7584,  1.3146,  ...,  1.3145, -0.9755, -1.9742],\n",
      "           [ 0.8692, -2.1212,  0.3999,  ..., -0.9096,  0.2947,  1.1836],\n",
      "           [ 0.4958, -1.2183, -0.9366,  ...,  0.8463,  0.7618,  0.5523]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.5277, -0.9723,  0.2731,  ...,  0.5563,  0.8190,  0.5499],\n",
      "           [-1.3099,  1.8333, -1.0157,  ...,  1.3425, -2.1251,  0.8161],\n",
      "           [ 0.3434,  1.5177,  1.3965,  ..., -0.3761,  1.2921, -0.9231],\n",
      "           ...,\n",
      "           [-0.7411, -0.4811,  1.1014,  ..., -0.6235,  0.2192, -0.7365],\n",
      "           [ 1.2811, -0.2412, -0.3398,  ...,  2.0071,  2.9103,  0.7873],\n",
      "           [ 1.0429, -0.3424, -0.8287,  ...,  1.7908,  1.2947, -1.0868]]],\n",
      "\n",
      "\n",
      "         [[[-0.5748,  0.6500, -2.4590,  ..., -0.2131,  1.1737, -0.8224],\n",
      "           [-0.1185, -0.1241, -0.5194,  ..., -0.6493,  0.8300,  1.2765],\n",
      "           [ 0.9262,  0.3981, -1.2157,  ...,  1.6586, -1.4187,  0.0401],\n",
      "           ...,\n",
      "           [ 0.9689,  0.6430,  0.3126,  ..., -0.7535, -0.5371,  1.4780],\n",
      "           [ 1.2807,  0.4098, -0.2693,  ...,  0.2204,  0.1378, -0.3813],\n",
      "           [-0.5621,  0.5198,  0.9505,  ..., -0.9798,  2.3208, -1.5778]]],\n",
      "\n",
      "\n",
      "         [[[-1.9478, -1.7715,  0.5721,  ..., -1.0842,  0.7662, -0.0693],\n",
      "           [ 0.8479,  1.2977,  0.2766,  ..., -0.1823, -1.2769,  0.3064],\n",
      "           [-0.0176,  1.0339, -0.2989,  ..., -0.9652,  0.9954, -1.1526],\n",
      "           ...,\n",
      "           [-0.6231,  1.0681,  1.5366,  ..., -1.0323, -0.9332,  0.6000],\n",
      "           [-1.0754, -1.7749, -0.8769,  ..., -1.9967, -0.9785, -1.5747],\n",
      "           [-1.0878,  1.6433, -0.7395,  ..., -1.9596, -0.8152,  1.1111]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-0.7245, -0.4606,  0.8109,  ..., -1.7462,  0.8412, -0.3468],\n",
      "           [-0.0494,  1.6072,  0.5924,  ..., -0.8167, -0.5108,  1.0860],\n",
      "           [ 0.5518, -0.8382,  0.3529,  ..., -0.1348,  1.4205,  0.3405],\n",
      "           ...,\n",
      "           [ 0.4006,  0.0575,  1.0115,  ...,  0.8531,  0.8747,  1.2079],\n",
      "           [ 0.2565,  0.1225, -0.4419,  ..., -1.6421, -1.0113, -0.6056],\n",
      "           [ 0.3488, -0.5361, -0.2750,  ...,  2.1379, -0.1606, -0.5097]]],\n",
      "\n",
      "\n",
      "         [[[-0.1140,  0.9108, -0.1709,  ...,  0.2957,  1.0925,  0.9403],\n",
      "           [-0.5772,  1.3422,  0.0368,  ..., -2.5655,  2.2861,  0.7792],\n",
      "           [ 0.8377, -0.1176, -0.4222,  ..., -0.5871, -0.5717,  0.3171],\n",
      "           ...,\n",
      "           [ 0.5611, -0.8594, -1.0988,  ...,  0.9792, -0.7885, -0.3865],\n",
      "           [ 1.6219,  0.6878, -1.3091,  ..., -0.4879,  1.3240,  1.7355],\n",
      "           [-0.1496, -1.2394, -0.3292,  ...,  0.9774,  0.7548,  0.4735]]],\n",
      "\n",
      "\n",
      "         [[[-1.3024,  0.4527, -1.2415,  ...,  0.4910,  0.0029,  0.5861],\n",
      "           [ 1.0137, -0.7852, -1.5562,  ...,  1.0756, -0.8359, -0.9394],\n",
      "           [-0.1735,  0.7451,  0.2272,  ..., -0.7656,  0.5648,  0.4412],\n",
      "           ...,\n",
      "           [ 0.8532,  0.0043, -0.1356,  ..., -1.6404, -1.4623,  1.2594],\n",
      "           [ 0.6985, -0.4117, -0.9905,  ...,  0.7192, -0.0674,  0.4097],\n",
      "           [-0.4735,  0.1602,  1.0320,  ...,  0.4954, -1.9365,  0.0360]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.8141, -0.0519, -0.6274,  ..., -0.5455, -0.0503,  0.7808],\n",
      "           [ 0.2499,  1.5523,  0.3835,  ...,  0.9084, -0.1208,  1.2141],\n",
      "           [ 1.0040,  0.8290, -0.4444,  ...,  0.0642, -1.0308,  0.6349],\n",
      "           ...,\n",
      "           [ 0.6175,  0.5827, -0.7508,  ...,  0.8415,  0.0041, -0.1541],\n",
      "           [ 0.0872,  0.2260, -0.2068,  ..., -0.4811, -0.9259,  1.4322],\n",
      "           [-0.4296,  0.7814,  0.2651,  ...,  0.2879,  1.4678,  0.5866]]],\n",
      "\n",
      "\n",
      "         [[[-0.0230, -0.3449,  0.1882,  ..., -0.4221, -0.0644, -1.3617],\n",
      "           [ 1.2300, -0.7840, -2.0926,  ...,  0.6592, -0.9808,  0.8951],\n",
      "           [ 0.3068, -0.1646, -0.1612,  ...,  1.5699,  0.2269, -0.8039],\n",
      "           ...,\n",
      "           [-0.2375, -0.4556, -1.3054,  ...,  1.5491,  0.4596, -0.9183],\n",
      "           [-0.8993,  0.3004,  0.6211,  ...,  1.2923, -0.2662, -0.6936],\n",
      "           [ 0.5100,  0.6714, -0.1070,  ...,  0.5945, -0.4751, -0.4065]]],\n",
      "\n",
      "\n",
      "         [[[-0.0421,  0.1279, -0.6864,  ...,  0.5812, -0.0639, -0.5262],\n",
      "           [ 0.1999, -0.6831, -0.0834,  ..., -1.1500,  0.3131, -0.2622],\n",
      "           [ 0.9011,  0.8643, -0.0224,  ...,  0.9094, -1.3710,  0.6163],\n",
      "           ...,\n",
      "           [ 0.0290, -0.0443, -0.4933,  ..., -0.4527,  1.3987,  0.8353],\n",
      "           [ 0.4362, -0.0762,  0.9313,  ...,  0.6507, -0.8294, -0.7004],\n",
      "           [-0.7453,  0.2690,  0.7421,  ...,  1.4216, -1.2757,  0.0387]]]]]), tensor([]), tensor([846.2551]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.4533, -1.1679,  0.2375,  ..., -2.8064,  0.2654, -1.1257],\n",
      "           [-0.5463,  1.5314,  0.2989,  ..., -1.1660, -0.8414,  1.0158],\n",
      "           [ 0.2363, -1.4648, -0.0355,  ..., -0.4828,  1.3594,  0.0672],\n",
      "           ...,\n",
      "           [ 0.8212, -0.2497,  1.1756,  ...,  1.6415,  1.6907,  2.1596],\n",
      "           [ 0.4442,  0.2995, -0.5170,  ..., -1.4038, -0.6184, -0.0599],\n",
      "           [ 0.4417, -0.5801, -0.2549,  ...,  3.0612,  0.3418, -0.2418]]],\n",
      "\n",
      "\n",
      "         [[[-0.8667,  0.2425, -1.0113,  ..., -0.4111,  0.5297,  0.4003],\n",
      "           [-1.3446,  0.9419, -0.7921,  ..., -3.6793,  1.9240,  0.4118],\n",
      "           [-0.0917, -1.2457, -1.5571,  ..., -1.3397, -1.3675, -0.1821],\n",
      "           ...,\n",
      "           [ 1.0238, -1.0149, -1.2867,  ...,  0.9410, -1.1096, -0.1636],\n",
      "           [ 1.6785,  1.4942, -1.9192,  ..., -0.9053,  1.5088,  2.0001],\n",
      "           [ 0.1925, -1.3536, -0.6041,  ...,  1.3695,  0.7193,  0.2369]]],\n",
      "\n",
      "\n",
      "         [[[-0.6264,  1.2864, -0.2799,  ...,  1.7709,  1.2023,  1.7485],\n",
      "           [ 1.9205,  0.3264, -0.6228,  ...,  2.4856,  0.2452, -0.0299],\n",
      "           [ 0.7100,  2.0752,  1.4523,  ...,  0.2093,  1.6789,  1.4257],\n",
      "           ...,\n",
      "           [ 0.8092,  0.2956,  0.3046,  ..., -1.6422, -2.2852,  0.7682],\n",
      "           [ 0.5159, -0.3783, -1.2094,  ...,  0.7142, -0.3953,  0.1580],\n",
      "           [-0.3116,  0.4913,  1.5002,  ..., -0.3197, -2.7369, -0.0958]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.0349, -0.0248, -0.3706,  ..., -0.3009, -0.0699,  0.7113],\n",
      "           [-0.5222,  1.4691, -0.2060,  ...,  0.2145, -1.2761,  0.7087],\n",
      "           [ 2.0321,  1.9175,  0.2476,  ...,  0.2604, -0.9454,  0.8553],\n",
      "           ...,\n",
      "           [ 1.3083,  1.2693, -0.4739,  ...,  1.2069,  0.7049,  0.3163],\n",
      "           [ 0.5675,  1.4007, -0.1508,  ..., -0.2512, -0.6135,  1.7035],\n",
      "           [ 0.8063,  1.9404,  0.4050,  ...,  0.7826,  1.5100,  0.9821]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0909, -0.5549,  0.4973,  ..., -0.2581,  0.2958, -1.3237],\n",
      "           [ 1.5986, -0.7155, -2.3296,  ...,  0.8347, -0.9665,  1.4329],\n",
      "           [ 0.5642, -0.0870, -0.0895,  ...,  1.7533,  0.1096, -0.9830],\n",
      "           ...,\n",
      "           [ 0.2094, -0.2761, -1.3246,  ...,  2.2040,  0.7682, -0.7581],\n",
      "           [-0.9934,  0.9609,  0.8514,  ...,  1.7246, -0.1461, -0.4959],\n",
      "           [ 1.1001,  0.8836,  0.1373,  ...,  0.9699, -0.5636, -0.3265]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1503,  0.2100, -0.3756,  ...,  1.0937,  0.5026, -0.1637],\n",
      "           [ 0.1469, -0.4273,  0.1995,  ..., -1.1437,  0.6957,  0.0530],\n",
      "           [ 0.8867,  1.0367, -0.1231,  ...,  1.1585, -1.5348,  0.8627],\n",
      "           ...,\n",
      "           [-0.6534, -0.1390, -0.8015,  ..., -0.9775,  0.4674, -0.0442],\n",
      "           [-0.4043, -0.9032,  0.6731,  ..., -0.0988, -2.0834, -1.8022],\n",
      "           [-1.4283, -0.4940,  0.3739,  ...,  0.7170, -2.2218, -0.2654]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 8.0195e-01,  3.5787e-01, -8.2427e-02,  ...,  1.2480e-01,\n",
      "             1.5654e-01,  8.6045e-02],\n",
      "           [ 1.7502e-01,  5.7209e-02, -2.9536e-01,  ..., -3.5606e-02,\n",
      "            -5.8860e-01, -1.1514e-01],\n",
      "           [ 2.3186e-01, -1.5708e-03, -7.2928e-01,  ...,  7.6231e-01,\n",
      "             8.0323e-01, -5.3793e-01],\n",
      "           ...,\n",
      "           [-3.0854e-01, -5.0320e-01,  7.0982e-01,  ..., -1.2074e+00,\n",
      "            -7.9804e-01, -4.9181e-01],\n",
      "           [ 6.1149e-01,  5.6718e-01, -1.6866e-01,  ...,  5.1713e-01,\n",
      "            -6.0302e-01, -8.6475e-01],\n",
      "           [ 1.9083e-01, -7.1666e-01,  3.0275e-01,  ...,  5.9589e-01,\n",
      "            -1.2949e+00,  4.2121e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.0998e-01, -1.6255e-01,  5.2343e-01,  ...,  1.7769e-01,\n",
      "             4.1885e-01, -4.2885e-01],\n",
      "           [ 1.5041e+00, -1.1134e-02,  4.5541e-01,  ...,  3.2981e-01,\n",
      "             8.7871e-01, -4.9751e-01],\n",
      "           [-5.1869e-01,  2.3618e-01,  1.0053e+00,  ...,  1.1194e+00,\n",
      "            -2.6604e-01,  8.1106e-01],\n",
      "           ...,\n",
      "           [-6.0340e-01, -9.0989e-01,  6.8437e-01,  ...,  5.2512e-01,\n",
      "            -5.5628e-02, -3.8885e-01],\n",
      "           [-1.4035e-01, -1.3296e-02, -2.3048e-01,  ..., -5.0932e-01,\n",
      "            -5.4571e-01,  7.5984e-01],\n",
      "           [ 6.8876e-02,  3.2067e-01,  2.0734e-02,  ..., -5.7223e-01,\n",
      "             5.0092e-01,  2.8637e-01]]],\n",
      "\n",
      "\n",
      "         [[[-5.8228e-01,  1.0425e+00,  3.5189e-02,  ..., -9.2385e-01,\n",
      "            -1.1802e+00, -1.1707e+00],\n",
      "           [-8.6181e-01, -1.0039e+00, -8.6392e-01,  ..., -1.8793e+00,\n",
      "            -6.7235e-01, -4.1408e-01],\n",
      "           [-3.4169e-01, -4.1185e-01,  4.6069e-01,  ..., -6.9009e-01,\n",
      "            -8.8737e-01, -4.1746e-01],\n",
      "           ...,\n",
      "           [ 1.0037e+00, -9.6838e-01,  9.2144e-01,  ...,  3.4606e-01,\n",
      "             4.3089e-01,  2.0392e-01],\n",
      "           [ 2.6777e-01, -7.8598e-01, -1.1015e+00,  ..., -4.1417e-03,\n",
      "            -9.6355e-01, -7.3472e-01],\n",
      "           [ 6.9202e-01, -3.9151e-02,  4.3621e-01,  ..., -1.8624e-01,\n",
      "            -2.1628e-01, -1.6520e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.8231e-01,  5.5044e-02,  5.6927e-01,  ..., -1.0538e+00,\n",
      "             6.2277e-01, -3.2482e-01],\n",
      "           [ 5.3281e-01,  4.9840e-01,  5.5683e-01,  ..., -6.5935e-01,\n",
      "             1.1929e+00, -8.2182e-01],\n",
      "           [-5.0390e-01, -1.0218e+00, -6.2906e-01,  ..., -7.1711e-01,\n",
      "             8.0704e-01,  3.1684e-01],\n",
      "           ...,\n",
      "           [ 2.0732e-01, -2.5131e-01,  4.7657e-02,  ...,  3.2179e-01,\n",
      "            -4.7339e-01,  9.8585e-02],\n",
      "           [-4.3473e-01, -5.3536e-01,  1.5207e-01,  ...,  1.6457e-01,\n",
      "             2.9376e-01,  6.6848e-02],\n",
      "           [-1.2996e+00, -1.1395e+00,  2.8390e-01,  ...,  1.5899e-03,\n",
      "             2.6923e-01, -7.6662e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.9164e-02,  5.7549e-01, -2.7207e-01,  ...,  1.6440e-02,\n",
      "            -3.3007e-01, -6.2005e-01],\n",
      "           [-3.5940e-01, -7.1979e-01, -9.2769e-01,  ..., -5.7698e-02,\n",
      "            -2.6822e-06, -1.0544e+00],\n",
      "           [-2.2956e-01,  2.3242e-01,  6.4381e-02,  ..., -6.5510e-01,\n",
      "             2.0653e-01, -7.4961e-01],\n",
      "           ...,\n",
      "           [-6.8889e-01,  6.7511e-01,  8.4366e-01,  ...,  7.4910e-02,\n",
      "            -8.6299e-01,  9.6635e-01],\n",
      "           [ 8.5081e-01, -1.8162e-01,  9.2609e-01,  ..., -1.0143e+00,\n",
      "            -1.4603e-01, -6.4633e-01],\n",
      "           [-4.9886e-01, -4.8980e-01,  1.1361e-01,  ..., -2.1840e-01,\n",
      "             6.5423e-01, -6.2736e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.6045e-01, -1.4537e-01,  2.6491e-01,  ...,  2.4551e-01,\n",
      "            -4.0100e-01, -7.6542e-01],\n",
      "           [ 2.4203e-01,  3.1894e-01,  4.0931e-01,  ...,  2.2349e-01,\n",
      "            -8.4361e-01,  1.5874e-01],\n",
      "           [ 1.6063e+00, -4.2274e-02,  5.0043e-01,  ...,  1.5021e-01,\n",
      "            -3.0712e-01,  8.4603e-02],\n",
      "           ...,\n",
      "           [ 7.3680e-02, -4.2999e-02, -1.0480e-01,  ...,  5.7943e-01,\n",
      "             8.4614e-01,  2.9384e-01],\n",
      "           [ 3.9433e-01,  3.5230e-01,  8.7045e-01,  ...,  8.3946e-01,\n",
      "            -1.2766e-01, -1.9188e-01],\n",
      "           [ 1.9072e-01,  9.0931e-01,  2.3263e-01,  ...,  7.1534e-01,\n",
      "             1.0692e+00, -2.3947e-01]]]]]), tensor([]), tensor([591.9661]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.4874, -0.2555, -1.1036,  ..., -0.7167, -0.7746, -0.7660],\n",
      "           [-0.4063, -0.3726, -1.0460,  ..., -0.3773, -1.2501, -0.6583],\n",
      "           [-0.2159, -0.6289, -1.9915,  ...,  0.6777,  0.9103, -1.5490],\n",
      "           ...,\n",
      "           [-0.0431, -1.2971,  1.1876,  ..., -1.4408, -0.3104, -0.1935],\n",
      "           [ 1.2762,  1.2466, -0.2193,  ...,  1.7466, -0.3529, -0.6339],\n",
      "           [ 0.2944, -1.0996,  0.6296,  ...,  1.6526, -1.3215,  1.2585]]],\n",
      "\n",
      "\n",
      "         [[[-1.6303, -1.5522,  0.0383,  ..., -0.9011, -0.2371, -1.6332],\n",
      "           [ 1.6140, -0.8798, -0.1415,  ..., -0.4994,  0.5326, -1.6063],\n",
      "           [-2.6098, -1.3596, -0.0750,  ...,  0.8556, -1.3913,  0.6737],\n",
      "           ...,\n",
      "           [-0.4973, -1.5509,  1.3009,  ...,  0.8575,  0.7474, -0.5842],\n",
      "           [-0.5893,  0.9440, -0.7858,  ..., -0.6843, -1.1554,  1.1216],\n",
      "           [ 0.4724,  0.7314, -0.2761,  ..., -0.4001,  0.9221,  0.3842]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2864,  2.8534,  1.8146,  ..., -0.0504, -0.1865, -0.4743],\n",
      "           [-0.4302,  0.0562,  0.3219,  ..., -1.4362,  0.6593,  0.8216],\n",
      "           [ 0.6617,  0.7966,  2.4863,  ...,  0.2633, -0.0690,  0.5243],\n",
      "           ...,\n",
      "           [ 1.4518, -1.2528,  2.1591,  ...,  0.5700,  0.5325, -0.8927],\n",
      "           [ 0.0089, -1.1739, -1.9190,  ..., -0.5362, -1.6627, -1.8257],\n",
      "           [ 1.4393,  0.3506,  1.1293,  ..., -1.2001, -1.2542, -0.2882]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.3858,  0.0142,  1.4848,  ..., -1.4756,  0.8593, -0.8443],\n",
      "           [-0.7050,  0.1483, -0.0205,  ..., -2.4173,  0.4767, -2.4824],\n",
      "           [-0.1757, -0.6363, -0.5125,  ..., -1.0897,  1.3298,  0.3737],\n",
      "           ...,\n",
      "           [ 1.0547,  0.3605,  0.5882,  ...,  0.8244, -0.1103,  0.9447],\n",
      "           [-0.1480,  0.5209,  0.4278,  ...,  1.2529,  1.3134,  0.9111],\n",
      "           [-0.3897, -0.3985,  0.6873,  ...,  1.1551,  1.0885, -0.1572]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1703,  0.8617, -0.1282,  ...,  0.3082, -0.1010, -0.6556],\n",
      "           [-0.3695, -0.8918, -1.3692,  ...,  0.0292,  0.5028, -1.4178],\n",
      "           [-0.1722,  0.6575,  0.2743,  ..., -1.2393,  0.4109, -1.2583],\n",
      "           ...,\n",
      "           [-0.5080,  1.5112,  1.8798,  ...,  0.9001, -0.6885,  1.8525],\n",
      "           [ 1.5163,  0.5060,  1.6744,  ..., -0.9103,  0.3366, -0.7626],\n",
      "           [-0.1544, -0.6309,  0.6084,  ...,  0.2001,  1.0893, -0.6568]]],\n",
      "\n",
      "\n",
      "         [[[-0.8785, -0.2380,  0.9927,  ...,  0.9535, -0.0184, -0.6424],\n",
      "           [ 0.2027,  1.0322,  1.1284,  ...,  0.5133, -1.0209,  0.6858],\n",
      "           [ 2.2549, -0.3714,  0.7006,  ...,  0.3654, -0.2717,  0.3524],\n",
      "           ...,\n",
      "           [-0.9254, -0.2455, -0.4822,  ...,  0.8316,  0.0319, -0.4211],\n",
      "           [-0.6255, -0.5434,  0.8414,  ..., -0.1037, -1.1459, -1.3413],\n",
      "           [-0.5374,  0.4351, -0.3431,  ..., -0.2930,  0.4574, -0.8831]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 5.5271e-01,  6.6271e-01,  4.1850e-01,  ...,  5.1572e-01,\n",
      "             6.8012e-01,  4.3714e-01],\n",
      "           [ 2.6288e-01,  1.2341e-01,  2.1201e-01,  ...,  9.4188e-02,\n",
      "             2.4862e-01,  1.5325e-01],\n",
      "           [ 3.4152e-01,  3.8855e-01,  4.1858e-01,  ...,  2.3235e-01,\n",
      "             2.0597e-01,  5.4688e-03],\n",
      "           ...,\n",
      "           [-4.0608e-01,  3.4666e-01, -1.3458e-01,  ..., -2.7762e-01,\n",
      "            -1.6870e-01, -4.7063e-01],\n",
      "           [-1.3880e-01, -2.4049e-01, -1.1053e-01,  ..., -5.8413e-01,\n",
      "            -5.6717e-01, -3.8982e-01],\n",
      "           [ 6.7856e-02,  1.2606e-02, -1.0061e-01,  ..., -3.6958e-01,\n",
      "            -4.7226e-01,  1.5175e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 5.1865e-01,  6.8143e-01,  3.2282e-01,  ...,  5.0845e-01,\n",
      "             3.8329e-01,  5.3685e-01],\n",
      "           [ 4.1130e-01,  2.6167e-01,  5.8639e-01,  ...,  6.0439e-01,\n",
      "             4.1668e-01,  4.8330e-01],\n",
      "           [ 7.5245e-01,  8.5203e-01,  1.0681e+00,  ...,  5.9173e-01,\n",
      "             4.5593e-01,  2.2363e-01],\n",
      "           ...,\n",
      "           [-1.7583e-01,  5.1244e-02, -1.4087e-01,  ...,  1.4971e-01,\n",
      "             2.0858e-02,  3.8173e-01],\n",
      "           [ 2.7089e-01, -4.0563e-01,  1.7155e-01,  ...,  2.4631e-01,\n",
      "            -3.1256e-01,  8.9235e-02],\n",
      "           [-2.5169e-01,  3.0045e-02,  2.3776e-01,  ...,  2.4483e-01,\n",
      "             6.5991e-01,  6.6389e-02]]],\n",
      "\n",
      "\n",
      "         [[[-9.6941e-01, -6.7128e-01, -1.0195e+00,  ..., -1.0080e+00,\n",
      "            -8.7079e-01, -7.5602e-01],\n",
      "           [-3.0682e-01, -7.5473e-01, -8.2027e-01,  ..., -9.4799e-01,\n",
      "            -8.8259e-01, -8.0546e-01],\n",
      "           [-7.4790e-01, -7.3040e-01, -7.1556e-01,  ..., -4.8156e-01,\n",
      "            -8.3330e-01, -8.1111e-01],\n",
      "           ...,\n",
      "           [ 5.8562e-02, -2.6110e-01, -3.0011e-01,  ...,  1.4127e-01,\n",
      "             2.2565e-01,  3.7713e-01],\n",
      "           [ 3.8382e-01, -1.4773e-01,  1.3137e-02,  ...,  5.8045e-02,\n",
      "             6.8315e-01,  3.4469e-01],\n",
      "           [-1.9334e-01, -1.8157e-01, -1.6501e-01,  ...,  1.9029e-01,\n",
      "             2.1049e-02,  4.4577e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.9572e-01,  8.4260e-02,  1.7007e-01,  ..., -2.8230e-01,\n",
      "            -3.7979e-01,  7.6844e-02],\n",
      "           [ 8.0614e-01,  3.2674e-01,  4.5052e-01,  ...,  7.0055e-01,\n",
      "             7.5074e-01,  4.2727e-01],\n",
      "           [-3.0793e-01, -2.4186e-01, -4.4000e-01,  ..., -3.0098e-01,\n",
      "            -6.4170e-01, -1.4277e-01],\n",
      "           ...,\n",
      "           [-2.9936e-01, -2.1598e-01, -1.1462e-01,  ..., -3.2794e-01,\n",
      "            -6.5823e-01,  6.7596e-03],\n",
      "           [-2.2278e-01, -7.5127e-01,  1.3763e-01,  ...,  1.9222e-02,\n",
      "            -6.4357e-01,  1.1548e-01],\n",
      "           [-8.6744e-01, -7.7650e-01, -2.9940e-01,  ...,  1.3148e-02,\n",
      "            -1.1078e-01, -1.1174e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0752e-01, -5.7557e-02, -2.2600e-01,  ..., -1.5073e-01,\n",
      "            -3.5238e-01, -1.0786e-01],\n",
      "           [-9.4832e-02, -3.3381e-01, -3.8455e-01,  ...,  8.6446e-02,\n",
      "            -4.5629e-01, -3.5170e-01],\n",
      "           [-3.0037e-01, -3.0033e-01, -1.6519e-01,  ...,  5.8379e-02,\n",
      "             9.7498e-03, -5.6870e-02],\n",
      "           ...,\n",
      "           [-2.7127e-01, -9.8485e-02, -2.8305e-01,  ..., -4.3481e-02,\n",
      "            -2.2188e-01,  5.3576e-02],\n",
      "           [-4.4588e-02, -5.5486e-01, -2.4433e-01,  ...,  2.5554e-01,\n",
      "            -1.4994e-01,  1.0569e-01],\n",
      "           [-4.5794e-01, -1.3012e-01, -6.1371e-02,  ..., -2.9500e-01,\n",
      "            -2.4889e-01, -2.3587e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.5402e-02,  9.0829e-04, -4.3711e-01,  ..., -3.5441e-01,\n",
      "            -3.3016e-01, -4.7383e-01],\n",
      "           [-1.1984e-01, -3.1039e-01, -1.4133e-01,  ..., -1.7144e-01,\n",
      "            -4.4170e-01, -6.0301e-02],\n",
      "           [ 2.9117e-01, -7.8363e-02, -1.0919e-01,  ..., -1.5912e-01,\n",
      "            -1.5165e-01,  8.4055e-03],\n",
      "           ...,\n",
      "           [ 4.0139e-01, -1.2318e-04,  1.3199e-01,  ...,  8.9832e-01,\n",
      "             1.0782e+00,  6.3936e-01],\n",
      "           [ 5.2533e-01,  5.3190e-01,  4.0173e-01,  ...,  7.8023e-01,\n",
      "             9.6138e-01,  1.1023e+00],\n",
      "           [ 3.7549e-01,  4.0097e-01,  3.6154e-01,  ...,  4.8685e-01,\n",
      "             2.7488e-01,  8.3758e-01]]]]]), tensor([]), tensor([123.7371]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-3.1084e-01,  1.2460e+00, -5.2082e-01,  ..., -1.0310e+00,\n",
      "             6.0331e-01, -7.8266e-01],\n",
      "           [-1.9539e+00, -1.4361e+00, -9.6781e-01,  ..., -9.5366e-01,\n",
      "             4.2786e-01, -9.3622e-01],\n",
      "           [-1.6794e+00, -3.1462e-01,  1.5218e-01,  ..., -1.0060e+00,\n",
      "            -1.5043e+00, -3.1692e+00],\n",
      "           ...,\n",
      "           [-1.2501e+00,  6.4762e-01, -1.5688e+00,  ...,  2.2528e+00,\n",
      "             1.4369e+00,  1.0517e+00],\n",
      "           [-2.0326e-01, -7.1887e-01, -6.2357e-01,  ..., -4.9591e-01,\n",
      "             3.1588e-01,  5.6183e-01],\n",
      "           [ 3.9406e-01,  6.0031e-01, -4.7582e-01,  ...,  2.7590e-01,\n",
      "            -7.1300e-01,  3.4746e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.4085e+00, -4.3550e-02, -7.8408e-01,  ..., -5.1426e-01,\n",
      "            -6.6211e-01,  3.9973e-01],\n",
      "           [-1.1238e+00, -1.3495e+00,  3.5995e-01,  ...,  8.1130e-02,\n",
      "            -1.1658e+00, -3.8526e-01],\n",
      "           [-1.7780e+00, -2.1176e+00, -3.4773e-01,  ..., -6.2081e-01,\n",
      "            -1.1610e+00, -1.3882e+00],\n",
      "           ...,\n",
      "           [ 5.7473e-01,  2.9297e-01, -1.0512e+00,  ..., -3.9600e-01,\n",
      "            -3.7591e-01,  2.6145e+00],\n",
      "           [ 3.3183e-01,  1.7038e+00, -4.4116e-01,  ..., -3.2482e+00,\n",
      "            -4.2802e-01,  1.4876e+00],\n",
      "           [-6.7691e-01,  8.9984e-01,  2.4775e-02,  ...,  1.4854e-01,\n",
      "             9.8474e-01,  2.4579e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1891e+00, -2.1975e-01, -8.2939e-02,  ...,  1.5937e-01,\n",
      "             1.3879e+00,  1.6254e+00],\n",
      "           [ 2.3966e+00,  1.8139e+00,  1.6696e+00,  ...,  6.2727e-01,\n",
      "             1.1019e+00,  1.0659e+00],\n",
      "           [-1.1727e-01,  1.5284e-01,  1.6552e+00,  ...,  3.1066e+00,\n",
      "             4.4435e-01, -2.8155e-01],\n",
      "           ...,\n",
      "           [-6.6794e-01, -1.1631e-01,  4.2807e-01,  ...,  9.0820e-01,\n",
      "            -3.3612e+00, -1.7154e+00],\n",
      "           [ 8.2143e-01, -2.3052e-01, -2.1827e-02,  ..., -1.8513e+00,\n",
      "             7.4118e-01, -1.2603e+00],\n",
      "           [-1.5965e-01,  2.7437e-01,  7.1051e-01,  ...,  3.0002e-01,\n",
      "            -5.2895e-01, -2.6299e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.7889e-01, -2.9920e-01,  2.8906e+00,  ...,  2.8004e-01,\n",
      "             1.2278e-01,  1.7796e+00],\n",
      "           [-1.5712e-01, -5.6324e-01, -1.0848e+00,  ..., -1.0425e+00,\n",
      "            -1.2030e+00, -1.9209e+00],\n",
      "           [ 1.0986e+00,  1.9523e+00, -6.4381e-01,  ...,  1.3188e-01,\n",
      "            -1.2287e+00,  1.8810e-02],\n",
      "           ...,\n",
      "           [ 1.0239e+00,  1.0270e+00,  1.4688e+00,  ...,  2.0383e+00,\n",
      "             1.6299e+00,  1.8077e-01],\n",
      "           [ 7.9809e-01,  5.4186e-01,  1.7032e+00,  ..., -1.0013e-01,\n",
      "             1.9658e+00,  7.8174e-01],\n",
      "           [ 8.0922e-01,  2.8478e-01, -1.1956e+00,  ...,  2.0488e+00,\n",
      "            -3.6904e-01,  2.7094e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.2093e+00, -1.1040e+00,  4.0392e-01,  ...,  5.7266e-01,\n",
      "            -8.3798e-01,  1.1285e+00],\n",
      "           [-1.4915e-01, -3.6412e-01, -1.3451e+00,  ...,  1.7246e+00,\n",
      "            -1.6756e+00, -1.3600e+00],\n",
      "           [-7.5783e-01, -5.7271e-01,  1.4978e-01,  ...,  8.5656e-01,\n",
      "            -2.3303e-02,  2.6044e-01],\n",
      "           ...,\n",
      "           [ 5.4277e-01,  7.3406e-01, -6.6100e-01,  ..., -5.7784e-02,\n",
      "            -1.2058e-01,  3.5182e-01],\n",
      "           [ 3.5453e-02, -4.7431e-01, -1.4546e+00,  ...,  7.7516e-01,\n",
      "            -6.6794e-01,  9.6913e-01],\n",
      "           [ 2.0231e-02,  6.6229e-01,  1.2129e+00,  ..., -1.0424e+00,\n",
      "            -2.2509e+00,  2.9550e-03]]],\n",
      "\n",
      "\n",
      "         [[[ 8.4437e-01,  4.5464e-01, -5.1261e-01,  ...,  4.1477e-01,\n",
      "             1.0620e+00, -3.1317e-01],\n",
      "           [-9.3340e-01,  1.3479e-01,  1.6627e+00,  ...,  4.1078e-01,\n",
      "            -6.7410e-01,  1.8468e+00],\n",
      "           [ 1.4779e+00, -1.0660e+00, -1.4462e+00,  ..., -1.0300e-01,\n",
      "             7.7385e-02,  1.6737e+00],\n",
      "           ...,\n",
      "           [-1.3167e+00, -4.5875e-01, -7.4776e-01,  ...,  7.3959e-01,\n",
      "             2.6641e-01,  1.8912e-01],\n",
      "           [-1.6662e+00, -6.3174e-01,  1.9188e-01,  ..., -3.3279e-01,\n",
      "            -1.7154e+00,  1.0057e-01],\n",
      "           [-5.9774e-01, -1.5316e+00, -1.7655e-01,  ..., -9.4900e-01,\n",
      "            -9.4077e-01, -2.4592e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 6.0234e-01,  4.0362e-01,  5.3693e-01,  ...,  6.1225e-01,\n",
      "             6.0341e-01,  5.0908e-01],\n",
      "           [ 4.3058e-01,  5.6888e-01,  2.3891e-01,  ...,  1.8074e-02,\n",
      "            -3.9111e-02,  1.6195e-01],\n",
      "           [ 2.9197e-01,  2.8612e-01,  2.8164e-01,  ...,  2.0118e-01,\n",
      "             4.0725e-01,  1.9300e-01],\n",
      "           ...,\n",
      "           [-2.6856e-01,  3.2910e-01, -3.6781e-02,  ..., -1.5777e-01,\n",
      "            -5.7412e-01, -4.1616e-01],\n",
      "           [-1.8789e-01, -2.2436e-01, -1.8424e-01,  ..., -3.5986e-01,\n",
      "            -2.3984e-01, -2.7003e-01],\n",
      "           [ 1.6569e-01, -1.1079e-01,  5.6528e-02,  ..., -3.0918e-01,\n",
      "            -5.3951e-01, -1.6466e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.8880e-01,  4.4808e-01,  4.0848e-01,  ...,  6.6246e-01,\n",
      "             4.8038e-01,  7.1591e-01],\n",
      "           [ 4.6138e-01,  5.5411e-01,  8.4962e-01,  ...,  4.5688e-01,\n",
      "             4.3009e-01,  3.9339e-01],\n",
      "           [ 7.7398e-01,  7.4800e-01,  6.1570e-01,  ...,  6.4674e-01,\n",
      "             7.0538e-01,  4.9382e-01],\n",
      "           ...,\n",
      "           [-5.1682e-01, -3.0948e-02, -2.8049e-01,  ..., -3.0529e-01,\n",
      "             9.1284e-02,  1.6679e-01],\n",
      "           [ 2.5576e-01, -2.9556e-01, -7.7236e-02,  ...,  2.3433e-01,\n",
      "            -9.4221e-02,  4.7431e-02],\n",
      "           [-3.2605e-01,  1.0276e-01,  2.6210e-01,  ...,  2.4331e-01,\n",
      "             2.1851e-01,  5.9455e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.6550e-01, -7.1558e-01, -8.2997e-01,  ..., -9.4723e-01,\n",
      "            -1.0124e+00, -9.9557e-01],\n",
      "           [-3.4857e-01, -7.9320e-01, -9.0399e-01,  ..., -6.7755e-01,\n",
      "            -1.1891e+00, -6.7655e-01],\n",
      "           [-6.5641e-01, -7.6248e-01, -5.2832e-01,  ..., -8.6045e-01,\n",
      "            -3.0958e-01, -6.1683e-01],\n",
      "           ...,\n",
      "           [ 3.8079e-01, -2.3177e-01, -1.9300e-01,  ...,  3.8017e-01,\n",
      "             6.1394e-01,  6.8775e-01],\n",
      "           [ 3.4305e-01, -8.4328e-02, -1.1203e-01,  ...,  7.8807e-02,\n",
      "             3.0432e-01,  5.6568e-01],\n",
      "           [-2.1337e-01, -2.9578e-01, -1.3370e-01,  ...,  6.9026e-01,\n",
      "             5.7379e-02,  1.6727e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.2713e-01, -1.0665e-01, -2.1862e-01,  ...,  1.8722e-01,\n",
      "             1.4298e-01,  2.6367e-01],\n",
      "           [ 5.5497e-01,  3.6709e-01,  4.1000e-01,  ...,  7.4531e-01,\n",
      "             6.3601e-01,  5.5209e-01],\n",
      "           [-5.3998e-01, -4.0654e-01, -5.1614e-01,  ..., -3.9134e-01,\n",
      "            -3.0109e-01, -3.1043e-01],\n",
      "           ...,\n",
      "           [-8.5494e-01, -4.6932e-01, -4.2058e-01,  ...,  3.1810e-02,\n",
      "            -4.1575e-01, -4.3320e-01],\n",
      "           [-4.6655e-01, -6.7885e-01, -4.2329e-03,  ..., -1.8346e-01,\n",
      "            -1.5728e-01, -3.7674e-01],\n",
      "           [-8.6075e-01, -8.4076e-01, -5.3307e-02,  ..., -2.2114e-01,\n",
      "            -3.5863e-01, -1.9720e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.6319e-01,  6.0782e-02,  1.1722e-01,  ..., -8.0808e-02,\n",
      "            -9.2378e-02, -3.3134e-02],\n",
      "           [-1.0688e-01,  1.6112e-01,  3.1936e-02,  ..., -1.6129e-01,\n",
      "            -1.2953e-01, -8.1608e-02],\n",
      "           [-7.4070e-02, -8.0627e-02, -7.4926e-02,  ...,  1.1497e-02,\n",
      "            -5.0661e-03, -8.8662e-02],\n",
      "           ...,\n",
      "           [-1.0370e-01,  4.6770e-02, -3.1164e-02,  ..., -4.8521e-01,\n",
      "             1.4063e-01,  3.3951e-02],\n",
      "           [-9.9732e-02, -5.4882e-01,  6.7310e-02,  ..., -4.5926e-01,\n",
      "            -2.4401e-01,  2.9412e-02],\n",
      "           [-3.4803e-01, -2.3099e-01, -1.0774e-01,  ..., -3.4749e-01,\n",
      "            -3.2560e-01,  3.5128e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.7286e-03,  9.1338e-02, -4.5557e-01,  ..., -1.7015e-01,\n",
      "            -3.4290e-01, -4.9608e-01],\n",
      "           [ 1.7807e-01, -2.1419e-01, -3.3272e-01,  ..., -2.3338e-01,\n",
      "            -2.2139e-01, -2.2885e-01],\n",
      "           [ 1.0643e-01, -1.8157e-04,  1.1969e-01,  ..., -1.1424e-01,\n",
      "             2.1733e-01, -3.4828e-01],\n",
      "           ...,\n",
      "           [ 7.2945e-01, -1.3103e-01,  8.0356e-02,  ...,  7.3279e-01,\n",
      "             8.7731e-01,  5.5684e-01],\n",
      "           [ 5.7924e-01,  4.0174e-01,  5.3373e-01,  ...,  6.7126e-01,\n",
      "             6.7433e-01,  6.2703e-01],\n",
      "           [ 4.7462e-01,  3.9555e-01,  2.9836e-01,  ...,  7.2354e-01,\n",
      "             7.2783e-01,  5.4719e-01]]]]]), tensor([]), tensor([142.9032]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.5914, -0.4037, -0.5905,  ...,  0.0543, -0.2568, -0.3684],\n",
      "           [-0.1771,  1.9381, -0.3610,  ..., -0.6938, -1.4634, -0.6428],\n",
      "           [-0.5014, -0.7831, -0.4131,  ..., -0.8222,  0.8960, -0.6648],\n",
      "           ...,\n",
      "           [ 0.0859,  0.2740, -0.2424,  ...,  1.9677,  0.3530,  1.6183],\n",
      "           [-0.5480, -0.7874, -1.2843,  ...,  0.9359,  2.0420,  2.0805],\n",
      "           [ 1.1250, -0.2771,  0.8731,  ...,  0.7536, -0.4137,  1.6241]]],\n",
      "\n",
      "\n",
      "         [[[-0.3062, -1.4287, -1.0497,  ..., -0.7867, -1.1953,  1.4954],\n",
      "           [-0.6396,  0.4856,  1.7932,  ..., -1.0170, -0.9692, -0.2031],\n",
      "           [-0.8188, -1.7540, -2.1805,  ..., -0.3698,  1.1534,  0.2492],\n",
      "           ...,\n",
      "           [-1.1787, -0.2787, -1.4528,  ...,  0.1331,  0.7744,  1.8459],\n",
      "           [ 0.3201,  2.1284, -2.0600,  ...,  2.0394, -1.5062,  0.3550],\n",
      "           [-0.4879,  1.2024,  0.5074,  ...,  1.5644,  1.6933, -2.7065]]],\n",
      "\n",
      "\n",
      "         [[[ 2.1782, -0.6009,  1.5988,  ...,  0.3450,  0.6596, -0.0642],\n",
      "           [ 1.8469,  1.9510,  0.3957,  ...,  2.6705, -0.2856,  1.9877],\n",
      "           [ 0.9082,  1.0579,  3.2147,  ..., -0.3162,  3.7995,  0.4589],\n",
      "           ...,\n",
      "           [ 1.7767, -0.0938,  1.1915,  ...,  0.9613, -0.3593, -0.8617],\n",
      "           [ 0.3525,  0.1503, -0.8428,  ..., -1.0603, -1.5097, -0.9508],\n",
      "           [-0.3318, -0.2833,  0.6880,  ..., -0.8940, -0.7549, -1.0002]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.7633, -0.5298,  0.5964,  ...,  0.9811,  0.2793, -0.0537],\n",
      "           [-1.6408,  0.3385,  0.5244,  ...,  0.5339, -0.6980,  0.0471],\n",
      "           [-0.2842,  1.4899, -1.2511,  ..., -0.5595,  0.5240,  0.7753],\n",
      "           ...,\n",
      "           [-2.3917,  0.0311, -0.2802,  ...,  1.3750, -2.6972,  1.3295],\n",
      "           [-0.1840,  1.2680,  0.3439,  ..., -0.2481,  0.3934,  1.3275],\n",
      "           [ 1.6391,  0.2715,  0.8058,  ...,  0.2231,  0.3403, -0.4333]]],\n",
      "\n",
      "\n",
      "         [[[-1.8727, -0.2394,  2.2121,  ...,  0.4451,  0.7390,  1.2354],\n",
      "           [-0.2215,  2.0140,  0.8633,  ..., -0.9912,  0.1867,  0.6677],\n",
      "           [ 0.6810, -0.0861, -0.2393,  ..., -0.3882, -0.5644, -0.7098],\n",
      "           ...,\n",
      "           [ 1.9012,  1.8998,  1.2360,  ..., -0.0347,  1.1582,  1.3381],\n",
      "           [-0.2064, -0.2298,  0.9946,  ..., -0.9472, -1.0651,  1.0710],\n",
      "           [ 0.6216, -0.7326,  0.8989,  ..., -0.3507,  0.0320,  1.3134]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1102,  1.0247, -0.9625,  ...,  1.6237,  0.3826, -0.9419],\n",
      "           [ 0.9218,  0.7250, -0.6252,  ..., -0.8117, -0.0527, -0.0424],\n",
      "           [-0.5522, -0.3898,  0.5830,  ..., -0.2144,  1.9402, -1.6922],\n",
      "           ...,\n",
      "           [ 0.8429, -1.2569, -0.7759,  ...,  1.2947,  0.1134, -1.0402],\n",
      "           [-1.1861, -1.7835,  1.0874,  ..., -0.7606, -2.3467, -1.8795],\n",
      "           [ 0.0242, -1.7815, -0.8596,  ...,  0.3903,  0.1654,  1.9484]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.7826,  0.9313, -0.0223,  ...,  0.2848, -0.1373,  0.2566],\n",
      "           [ 0.2507,  1.0993,  0.8111,  ...,  0.5342, -0.6859, -0.4650],\n",
      "           [-0.3620,  0.4358, -0.2171,  ..., -0.0343, -0.7080,  0.6967],\n",
      "           ...,\n",
      "           [ 0.2365, -0.9052, -0.6368,  ...,  0.2355, -0.0348, -0.9189],\n",
      "           [-0.3441,  0.4137,  0.9546,  ..., -0.4417, -0.2135, -0.7748],\n",
      "           [ 0.2077,  0.5853,  0.2432,  ..., -0.1224,  0.7830,  0.1096]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6004, -0.3079, -0.9454,  ...,  0.4827,  0.3659, -0.0161],\n",
      "           [-0.6066, -0.5080, -0.7133,  ...,  0.7590, -0.2979,  0.3213],\n",
      "           [-0.2449, -0.1472, -0.6258,  ...,  0.7944, -0.2080, -0.6905],\n",
      "           ...,\n",
      "           [-1.2058, -0.6597, -0.6783,  ..., -0.0385,  0.0044, -0.1684],\n",
      "           [-0.0719, -0.7276,  0.1518,  ..., -0.2553,  0.4526,  1.3157],\n",
      "           [-0.2761, -0.4386, -0.6373,  ...,  0.3749, -0.1477, -0.9007]]],\n",
      "\n",
      "\n",
      "         [[[-1.2083,  0.0710, -0.1616,  ..., -0.6204,  0.0513,  0.8041],\n",
      "           [-0.4607, -0.9235, -1.2813,  ..., -0.9107, -0.6388, -0.3379],\n",
      "           [-0.4130, -0.3260, -1.6669,  ...,  0.0678,  0.6715, -0.5228],\n",
      "           ...,\n",
      "           [-0.1738,  0.2330,  0.6298,  ..., -1.2273,  0.4136, -0.0627],\n",
      "           [-0.4810,  0.1242,  0.3742,  ...,  1.1147, -0.2221,  0.0213],\n",
      "           [ 1.1194, -0.2356, -0.4737,  ..., -0.7532,  0.7391,  0.1413]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.4957, -0.1492, -1.0139,  ..., -0.1209, -0.0336,  0.3734],\n",
      "           [ 0.2512, -0.3588, -0.3220,  ...,  0.4439,  0.0623,  0.7437],\n",
      "           [-0.3120, -0.6735, -0.6442,  ..., -0.4746,  0.3973,  0.6657],\n",
      "           ...,\n",
      "           [ 0.1168, -0.0905,  0.1328,  ..., -0.5480, -0.6393,  0.3311],\n",
      "           [ 0.0951,  0.7432, -0.5307,  ...,  0.3865, -0.8374, -0.8410],\n",
      "           [ 0.6992,  0.7482, -0.5101,  ..., -0.8835, -0.3269, -0.1648]]],\n",
      "\n",
      "\n",
      "         [[[-0.7581,  1.1248, -0.1851,  ...,  0.0858, -0.0138, -0.2557],\n",
      "           [ 0.0694,  0.1580, -0.2659,  ..., -0.1755, -0.8370,  0.2210],\n",
      "           [ 0.2207,  0.8565,  0.5222,  ...,  0.3615, -0.1781, -0.4781],\n",
      "           ...,\n",
      "           [-0.3408, -0.0686, -0.2142,  ..., -0.5037,  0.6126, -0.3458],\n",
      "           [-0.1233, -0.4183, -0.3253,  ...,  0.1585,  0.3114, -0.7822],\n",
      "           [ 0.2242,  0.3671, -0.6580,  ..., -0.5129,  0.2096, -0.7426]]],\n",
      "\n",
      "\n",
      "         [[[-0.9626,  0.4694,  0.4339,  ...,  0.0590,  0.1505, -0.5144],\n",
      "           [-0.3215,  0.3377,  0.5345,  ...,  0.4828,  1.0925, -0.6203],\n",
      "           [ 0.2829, -0.4955, -0.4051,  ..., -0.6355,  0.2343, -0.4730],\n",
      "           ...,\n",
      "           [ 0.3018, -0.6536,  0.5027,  ...,  0.3771, -0.6456,  0.3650],\n",
      "           [ 0.5998,  0.8864,  0.1121,  ...,  0.4653,  0.1209,  0.7826],\n",
      "           [ 0.4476,  0.1149,  0.2156,  ...,  1.0734, -0.5892,  0.3943]]]]]), tensor([]), tensor([604.5913]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 4.5494e-01,  6.8252e-01, -1.0295e+00,  ..., -4.4511e-01,\n",
      "            -1.2473e+00, -4.4258e-01],\n",
      "           [-3.1835e-01,  1.3214e+00,  7.4573e-01,  ...,  5.3276e-01,\n",
      "            -1.3817e+00, -1.2584e+00],\n",
      "           [-1.3514e+00,  8.9347e-02, -1.0909e+00,  ..., -6.3849e-01,\n",
      "            -1.7117e+00,  5.7828e-01],\n",
      "           ...,\n",
      "           [ 8.9937e-01, -1.8755e+00, -1.0808e+00,  ...,  9.1572e-01,\n",
      "             8.2511e-01, -7.6962e-01],\n",
      "           [-3.1405e-01,  1.0097e+00,  1.6330e+00,  ...,  2.2500e-01,\n",
      "             6.2888e-01, -5.4447e-01],\n",
      "           [ 3.0144e-01,  1.0856e+00,  5.0412e-01,  ...,  4.8532e-01,\n",
      "             2.2181e+00,  6.9426e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.1013e-02, -1.7306e+00, -2.3939e+00,  ..., -3.3684e-01,\n",
      "            -3.4820e-01, -8.5668e-01],\n",
      "           [-1.9819e+00, -1.6131e+00, -2.1940e+00,  ...,  2.0589e-01,\n",
      "            -1.2633e+00, -2.0363e-01],\n",
      "           [-1.9318e+00, -2.1073e+00, -2.8572e+00,  ...,  3.3627e-01,\n",
      "            -1.0893e+00, -1.7013e+00],\n",
      "           ...,\n",
      "           [-1.5142e+00, -1.0594e+00, -9.8924e-01,  ..., -1.8999e-01,\n",
      "             4.4653e-01,  7.1317e-02],\n",
      "           [-4.8026e-01, -2.7465e-01, -1.5799e-01,  ..., -4.6469e-01,\n",
      "             9.0899e-01,  1.5103e+00],\n",
      "           [-8.4029e-02, -5.3741e-01, -1.3949e+00,  ...,  1.3411e+00,\n",
      "            -5.1198e-01, -1.5719e+00]]],\n",
      "\n",
      "\n",
      "         [[[-7.7534e-01,  1.1568e+00,  1.3947e+00,  ...,  4.5744e-01,\n",
      "             1.8897e+00,  2.8469e+00],\n",
      "           [ 2.2483e-01,  2.2425e-01, -3.7082e-01,  ...,  1.8571e-01,\n",
      "             6.6893e-01,  9.0457e-01],\n",
      "           [ 6.8141e-01,  1.0121e+00, -1.0369e+00,  ...,  1.5191e+00,\n",
      "             2.4840e+00,  3.9388e-01],\n",
      "           ...,\n",
      "           [-5.3614e-01,  7.4291e-01,  1.6455e+00,  ..., -1.7807e+00,\n",
      "             2.3158e-01, -1.0216e+00],\n",
      "           [-1.1993e+00,  3.7603e-01,  5.7008e-01,  ...,  1.3443e+00,\n",
      "            -6.0394e-01, -3.8486e-01],\n",
      "           [ 2.1263e+00,  9.9149e-03, -3.9356e-01,  ..., -2.1466e+00,\n",
      "             1.0402e+00,  6.2924e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.2050e+00, -2.6526e-01, -1.1508e+00,  ...,  1.7889e-02,\n",
      "            -1.4336e-01,  3.2510e-01],\n",
      "           [-1.1458e+00, -1.2383e+00, -1.7214e+00,  ..., -5.5669e-01,\n",
      "            -1.1337e+00,  4.2491e-01],\n",
      "           [-2.2451e-03, -3.1297e-01, -5.4752e-01,  ..., -6.0730e-01,\n",
      "             1.1606e+00,  1.1564e+00],\n",
      "           ...,\n",
      "           [ 8.5346e-01,  6.0902e-01,  7.6311e-01,  ..., -1.0692e+00,\n",
      "            -7.6380e-01,  1.1632e+00],\n",
      "           [ 6.6095e-01,  2.5728e+00, -7.0578e-01,  ...,  1.1834e+00,\n",
      "            -1.1403e+00, -9.5787e-01],\n",
      "           [ 2.8502e+00,  2.6741e+00, -6.6741e-01,  ..., -3.6634e-01,\n",
      "            -3.2479e-01,  6.6655e-01]]],\n",
      "\n",
      "\n",
      "         [[[-9.8376e-01,  1.7453e+00,  6.0981e-02,  ...,  4.3890e-01,\n",
      "             4.1325e-01, -6.9255e-02],\n",
      "           [ 3.5482e-01,  6.7898e-01, -1.8997e-01,  ..., -1.2198e-01,\n",
      "            -8.7268e-01,  7.1815e-01],\n",
      "           [ 5.5283e-01,  1.7132e+00,  1.0413e+00,  ...,  5.7781e-01,\n",
      "            -2.0053e-01, -6.1406e-01],\n",
      "           ...,\n",
      "           [ 8.2953e-02,  2.6777e-01,  4.0110e-02,  ..., -1.2161e-01,\n",
      "             1.5336e+00, -1.7138e-01],\n",
      "           [-1.2451e-01,  1.1818e-01, -4.3606e-01,  ...,  6.7811e-01,\n",
      "             8.7900e-01, -1.1146e+00],\n",
      "           [ 1.0465e+00,  8.1350e-01, -6.7154e-01,  ..., -6.9257e-02,\n",
      "             5.7313e-01, -6.2226e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.3543e+00,  7.3952e-01,  1.2087e+00,  ...,  5.8847e-01,\n",
      "             8.9834e-01, -1.9138e-01],\n",
      "           [-6.4278e-01,  1.1150e+00,  1.3073e+00,  ...,  1.0104e+00,\n",
      "             2.1564e+00, -6.4475e-01],\n",
      "           [ 1.6977e-01, -1.1098e+00, -9.1669e-01,  ..., -9.0077e-01,\n",
      "             6.0435e-01, -5.1897e-01],\n",
      "           ...,\n",
      "           [-5.1551e-01, -1.2141e+00,  4.5467e-01,  ...,  4.2626e-01,\n",
      "            -2.9769e+00, -6.6667e-01],\n",
      "           [-2.8195e-01,  3.4299e-01, -4.4102e-01,  ..., -9.2230e-01,\n",
      "            -1.3609e+00, -2.4748e-01],\n",
      "           [-1.0486e-01, -9.2027e-01, -3.7569e-01,  ...,  2.2371e-01,\n",
      "            -2.1986e+00,  3.4393e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 9.2210e-01,  1.9910e-01, -3.6617e-02,  ...,  3.3878e-01,\n",
      "             6.2522e-01,  5.4223e-01],\n",
      "           [ 3.5956e-01,  6.4736e-01,  6.4410e-01,  ..., -1.4935e-01,\n",
      "             7.5043e-02,  9.1437e-02],\n",
      "           [ 1.1039e-01,  3.6290e-02,  4.9782e-02,  ...,  8.0331e-01,\n",
      "            -7.6622e-02,  2.1116e-01],\n",
      "           ...,\n",
      "           [-4.7382e-01,  5.5616e-02,  3.2960e-01,  ..., -4.5591e-01,\n",
      "            -1.7561e-01, -2.7722e-01],\n",
      "           [-9.5651e-02, -2.8995e-01, -2.3347e-02,  ..., -2.6290e-01,\n",
      "            -2.8639e-01, -2.0630e-01],\n",
      "           [ 2.4515e-01, -7.8565e-01,  9.7824e-02,  ..., -7.4276e-01,\n",
      "            -2.5225e-01, -4.0129e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.9248e-01,  1.0100e+00,  4.1664e-01,  ...,  3.8565e-01,\n",
      "             6.6484e-01,  3.8545e-01],\n",
      "           [ 5.2154e-01,  6.3951e-01,  4.0041e-01,  ...,  2.7791e-01,\n",
      "             3.9630e-01,  4.7115e-01],\n",
      "           [ 1.0664e+00,  5.5180e-01,  3.1119e-01,  ...,  1.2997e-01,\n",
      "             4.2900e-01,  1.7072e-01],\n",
      "           ...,\n",
      "           [-4.4396e-01,  3.5042e-01, -2.0841e-01,  ..., -2.9703e-02,\n",
      "             7.0780e-02, -1.2684e-01],\n",
      "           [ 1.3219e+00, -4.1184e-01,  7.2721e-02,  ...,  2.9237e-01,\n",
      "             4.0741e-01,  6.9813e-04],\n",
      "           [ 2.7146e-01, -4.8116e-01,  6.7590e-01,  ...,  5.6252e-01,\n",
      "            -1.5355e-01,  2.7958e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.3879e-01, -5.5831e-01, -3.6261e-01,  ..., -8.8721e-01,\n",
      "            -7.7731e-01, -8.8258e-01],\n",
      "           [ 1.6646e-01, -9.7857e-01, -7.0454e-01,  ..., -8.5999e-01,\n",
      "            -7.9913e-01, -6.7505e-01],\n",
      "           [-3.9731e-01, -1.1997e+00, -9.2234e-01,  ..., -6.8292e-01,\n",
      "            -8.0884e-01, -7.7745e-01],\n",
      "           ...,\n",
      "           [ 2.4686e-01, -2.2717e-02, -3.2757e-01,  ...,  2.5010e-02,\n",
      "             3.7111e-01,  1.0388e+00],\n",
      "           [-6.5134e-01,  2.4623e-01,  3.1064e-01,  ..., -6.7138e-03,\n",
      "            -4.4669e-01,  2.1322e-01],\n",
      "           [ 1.1434e-01, -5.8577e-02, -6.1950e-01,  ...,  4.2374e-01,\n",
      "             5.1115e-02, -6.1236e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.6229e-01,  6.5397e-02,  1.0949e-01,  ..., -8.6048e-01,\n",
      "             1.8711e-01, -8.6508e-02],\n",
      "           [ 2.1052e-01, -5.1630e-01, -4.3613e-02,  ...,  5.2735e-01,\n",
      "             9.1546e-01,  5.9667e-01],\n",
      "           [-3.6519e-01, -5.5917e-01, -1.1042e-04,  ...,  1.8770e-01,\n",
      "            -4.0686e-01, -3.5130e-02],\n",
      "           ...,\n",
      "           [ 5.6348e-01, -6.8317e-01, -2.5842e-01,  ..., -4.0967e-01,\n",
      "             2.9921e-01, -1.9246e-01],\n",
      "           [ 2.4168e-02, -5.1052e-01, -1.8009e-01,  ...,  7.0672e-03,\n",
      "            -1.0467e-01, -5.4118e-01],\n",
      "           [-1.0719e+00, -4.3989e-01,  6.5378e-01,  ...,  3.7453e-01,\n",
      "            -3.6913e-01, -5.0407e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.2725e-01, -8.0850e-02, -5.5271e-01,  ..., -6.4388e-01,\n",
      "             1.7254e-01,  4.0998e-01],\n",
      "           [-5.2702e-01, -2.0794e-01, -5.2847e-02,  ..., -9.5950e-02,\n",
      "            -2.5966e-01, -4.0312e-01],\n",
      "           [ 6.1568e-02, -2.6434e-01, -8.2798e-02,  ...,  8.1145e-02,\n",
      "             1.8333e-01, -5.8746e-02],\n",
      "           ...,\n",
      "           [-3.2906e-03,  2.1884e-01, -5.9824e-03,  ...,  6.2827e-02,\n",
      "             1.5902e-02, -9.4855e-02],\n",
      "           [-4.4806e-01, -3.2461e-01,  1.5250e-01,  ..., -6.0976e-01,\n",
      "             1.9226e-01,  7.3615e-02],\n",
      "           [-4.1948e-01,  2.3080e-01,  5.0338e-02,  ..., -1.8405e-01,\n",
      "            -2.3113e-01, -3.6974e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.4548e-01,  8.5642e-02, -7.4175e-01,  ..., -2.1336e-01,\n",
      "            -2.9218e-01, -2.5546e-01],\n",
      "           [ 9.2366e-02, -3.7771e-01, -4.6870e-01,  ..., -1.5783e-01,\n",
      "             3.2264e-01, -6.7551e-02],\n",
      "           [ 7.8441e-02,  1.2431e-01, -6.9824e-01,  ..., -4.4235e-02,\n",
      "            -2.6977e-01, -1.6010e-01],\n",
      "           ...,\n",
      "           [ 9.2260e-01, -4.3507e-01,  1.7638e-01,  ...,  4.5010e-01,\n",
      "             5.3464e-01,  3.6698e-01],\n",
      "           [ 5.0505e-01,  6.9834e-01, -2.8717e-01,  ...,  6.0458e-01,\n",
      "             8.1674e-01,  1.8416e-01],\n",
      "           [ 3.7564e-02,  4.5777e-01,  2.4921e-01,  ...,  7.7188e-02,\n",
      "             5.4290e-01,  7.6564e-01]]]]]), tensor([]), tensor([296.2668]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.3034, -1.0393, -2.2594,  ..., -0.8155, -0.0520, -0.2372],\n",
      "           [-0.1490,  1.0753,  1.1243,  ..., -0.9833, -0.4325, -0.6362],\n",
      "           [-0.8853, -1.2783, -1.2038,  ...,  1.5612, -1.2484, -0.4993],\n",
      "           ...,\n",
      "           [-0.7738, -0.7328,  1.0128,  ...,  0.3385,  0.9637,  0.7673],\n",
      "           [ 0.0068, -0.4631, -0.0222,  ...,  0.6507,  0.5471,  0.8182],\n",
      "           [ 0.8229, -2.5618,  0.5607,  ..., -0.8167,  0.6344, -0.4105]]],\n",
      "\n",
      "\n",
      "         [[[-1.5717,  0.9515, -0.6538,  ..., -0.6076, -0.0135, -1.0159],\n",
      "           [-0.0776,  0.3999, -1.0327,  ..., -1.1504, -0.4807,  0.1817],\n",
      "           [ 0.4501, -1.6333, -2.3373,  ..., -1.9475, -0.5516, -1.0647],\n",
      "           ...,\n",
      "           [-0.6581,  1.2313, -0.4807,  ...,  0.5846,  0.0518,  0.8580],\n",
      "           [ 3.6791,  0.6689, -0.6116,  ...,  0.4704, -0.2045,  0.0602],\n",
      "           [ 1.8660, -1.4646,  1.6050,  ...,  0.8826, -1.7818, -0.1120]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1025,  0.1690,  2.1310,  ...,  0.6113,  1.0224,  0.0128],\n",
      "           [ 2.5036,  0.3625,  1.0509,  ...,  0.4466,  0.9816,  0.9163],\n",
      "           [ 1.2227, -0.8227,  0.4597,  ...,  0.8368,  0.4514, -0.1710],\n",
      "           ...,\n",
      "           [ 0.2914,  0.7641,  0.1425,  ..., -0.3520, -1.1313,  0.3807],\n",
      "           [-3.1284,  1.1225,  0.9714,  ..., -0.1622, -1.9548, -1.4414],\n",
      "           [ 0.9388,  0.5981, -1.3342,  ...,  0.4451, -0.4667, -1.9379]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.5381,  0.3569,  1.1029,  ..., -1.9867,  0.4055, -1.1935],\n",
      "           [-1.6996, -3.1609, -1.7581,  ..., -0.8379, -0.2860, -0.5053],\n",
      "           [ 0.0648,  0.2145,  1.2937,  ...,  1.5513, -0.3027,  0.3816],\n",
      "           ...,\n",
      "           [ 3.3874, -0.7164,  0.5117,  ...,  0.8358,  1.2790,  1.9464],\n",
      "           [ 1.2825,  1.1897, -0.4054,  ...,  1.2449,  1.2181,  0.3104],\n",
      "           [ 0.1663,  1.5177,  2.5355,  ...,  1.9763, -0.0139, -0.0956]]],\n",
      "\n",
      "\n",
      "         [[[-0.7858, -0.5989, -1.0451,  ..., -1.3198,  1.4977,  2.0410],\n",
      "           [-1.3101, -0.0974,  0.2745,  ...,  0.0809, -0.1896, -0.3255],\n",
      "           [ 0.7717, -0.5115, -0.0740,  ..., -0.0311,  0.4382, -0.1738],\n",
      "           ...,\n",
      "           [ 1.3107,  1.4104,  0.5928,  ...,  1.5285,  0.3882,  0.6710],\n",
      "           [-1.3402,  0.6500,  0.8228,  ..., -1.3377,  1.3072,  1.6363],\n",
      "           [-0.0788,  1.0739,  0.9414,  ...,  0.0788, -0.2022, -0.0544]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4394,  0.5085, -1.3420,  ...,  0.4932,  0.7874,  0.6256],\n",
      "           [ 0.2760, -0.1866, -0.6973,  ...,  0.1285,  1.6446,  0.6537],\n",
      "           [-0.1883,  0.4806, -2.5041,  ...,  0.1304, -0.5663, -0.0149],\n",
      "           ...,\n",
      "           [ 1.0600, -1.8400, -0.1536,  ..., -0.9531, -1.1587, -1.9047],\n",
      "           [-0.9949,  0.0591, -2.2605,  ..., -0.1534, -0.1874, -2.2950],\n",
      "           [-1.5963, -0.7739, -0.6346,  ..., -1.7551, -0.3699,  1.9349]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 8.1408e-01,  5.5233e-01,  9.2444e-02,  ..., -8.1118e-01,\n",
      "             1.2444e-01,  5.0804e-01],\n",
      "           [ 6.6150e-02,  1.4334e-01, -7.0985e-01,  ...,  7.8145e-04,\n",
      "             7.9778e-02,  2.3633e-01],\n",
      "           [-5.6384e-01,  1.0705e+00,  4.9543e-01,  ..., -2.0907e-01,\n",
      "            -6.1691e-01, -2.0316e-01],\n",
      "           ...,\n",
      "           [ 1.7010e-01, -8.4460e-01, -5.6536e-01,  ..., -6.8532e-01,\n",
      "             5.2000e-01,  2.7864e-01],\n",
      "           [ 1.2168e+00,  9.2246e-01,  9.8356e-01,  ..., -3.3358e-01,\n",
      "             5.2314e-01, -6.9280e-01],\n",
      "           [ 4.8339e-01, -8.0405e-01, -5.3637e-01,  ..., -2.0467e-01,\n",
      "            -3.5774e-01, -9.0447e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.7397e-02, -4.8240e-01, -9.7032e-01,  ..., -2.6199e-01,\n",
      "             1.8974e+00,  1.1137e-01],\n",
      "           [-2.0179e-01,  4.1479e-01,  1.4792e+00,  ...,  1.1592e+00,\n",
      "            -4.4884e-01, -2.9371e-01],\n",
      "           [ 1.4301e+00,  1.2236e-01, -2.9096e-01,  ..., -6.2824e-01,\n",
      "             5.9920e-01,  6.4002e-01],\n",
      "           ...,\n",
      "           [ 2.0498e-01,  3.5679e-01, -8.4130e-01,  ...,  5.5674e-01,\n",
      "            -1.9113e-01,  1.5226e+00],\n",
      "           [ 5.8066e-01,  5.6960e-01,  1.2762e+00,  ...,  2.4888e-01,\n",
      "             1.3762e+00, -8.6651e-01],\n",
      "           [ 1.0640e+00,  5.9951e-01,  5.8069e-01,  ...,  1.0854e-02,\n",
      "             2.1245e-01, -1.1256e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.5803e+00, -9.9472e-01,  4.1626e-02,  ...,  8.8117e-01,\n",
      "             2.9906e-01, -2.5625e-01],\n",
      "           [-3.2546e-01, -8.1291e-01,  1.1493e+00,  ..., -1.0329e+00,\n",
      "            -2.3131e-01,  1.0440e+00],\n",
      "           [-1.5344e+00,  7.1469e-01, -6.0364e-01,  ...,  3.8787e-01,\n",
      "            -3.5921e-01,  7.2751e-01],\n",
      "           ...,\n",
      "           [-6.2875e-01, -1.3324e+00, -1.2146e+00,  ..., -2.1222e-01,\n",
      "            -5.4860e-01, -4.1495e-01],\n",
      "           [-1.1192e+00,  1.5506e+00, -1.1224e-01,  ...,  5.2837e-01,\n",
      "            -3.9770e-02,  1.0651e+00],\n",
      "           [-4.0200e-01, -5.5222e-01, -6.3126e-01,  ..., -1.5326e+00,\n",
      "             4.8997e-01,  2.4895e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.8405e-01, -3.5366e-01, -2.8579e-01,  ..., -1.4473e-01,\n",
      "             9.5763e-01, -2.0413e-01],\n",
      "           [-9.8118e-02,  1.8592e-01,  5.6097e-01,  ...,  8.8587e-01,\n",
      "             6.9397e-01,  5.5962e-01],\n",
      "           [ 1.9062e-01, -2.7401e-01,  4.9825e-01,  ..., -1.0339e+00,\n",
      "            -1.1622e+00,  3.4367e-01],\n",
      "           ...,\n",
      "           [-1.6048e+00, -1.5310e-02,  5.1936e-01,  ...,  3.3234e-01,\n",
      "            -6.4894e-01, -5.3379e-01],\n",
      "           [-1.3213e+00, -7.5190e-02,  7.1024e-01,  ..., -9.4651e-01,\n",
      "            -5.0471e-01,  6.5813e-01],\n",
      "           [ 1.3626e-01, -5.7582e-01,  5.9818e-01,  ..., -5.5617e-01,\n",
      "            -5.2347e-03,  8.0304e-02]]],\n",
      "\n",
      "\n",
      "         [[[-1.5425e-01, -2.6677e-01, -1.1375e-01,  ..., -1.5124e-02,\n",
      "            -6.8970e-02, -3.3258e-01],\n",
      "           [-3.1690e-01, -2.9919e-01, -1.0736e+00,  ..., -7.2380e-01,\n",
      "             9.3181e-01,  4.1329e-01],\n",
      "           [-1.4103e+00, -1.2840e-01,  4.9579e-01,  ...,  3.8343e-01,\n",
      "             8.0159e-01, -6.0278e-01],\n",
      "           ...,\n",
      "           [ 3.5158e-01,  1.5408e-01, -6.7384e-02,  ..., -3.0490e-01,\n",
      "             5.8599e-01,  7.2125e-01],\n",
      "           [-7.8435e-01, -2.1297e-01,  1.1799e-02,  ..., -5.2044e-01,\n",
      "             6.9534e-01, -2.7225e-02],\n",
      "           [ 3.9267e-01, -5.8792e-01, -9.4721e-01,  ..., -6.9216e-01,\n",
      "            -5.2126e-01, -2.7586e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.6672e-01, -3.0433e-01, -7.6477e-01,  ..., -6.5148e-01,\n",
      "            -1.0127e+00,  3.9595e-01],\n",
      "           [-3.8019e-01,  2.1960e-01,  7.7087e-01,  ..., -8.9870e-01,\n",
      "            -2.4726e-01, -1.7446e+00],\n",
      "           [ 5.8616e-01,  1.1107e+00, -1.6024e-01,  ...,  1.1209e-01,\n",
      "            -2.2484e-01,  1.2346e+00],\n",
      "           ...,\n",
      "           [-8.0073e-02, -8.6126e-01,  7.1376e-01,  ...,  2.7090e-01,\n",
      "             5.0794e-01,  7.0256e-01],\n",
      "           [ 1.5322e-01,  3.2468e-01,  1.6088e-01,  ...,  6.6050e-01,\n",
      "             9.1068e-02, -2.5153e-01],\n",
      "           [ 8.5379e-02,  1.5752e+00,  7.2683e-01,  ..., -6.0366e-01,\n",
      "            -7.8678e-01, -1.0920e+00]]]]]), tensor([]), tensor([649.4139]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 0.4273,  0.0516, -0.8307,  ..., -2.1630, -0.7876, -0.1689],\n",
      "           [-0.5261, -0.3001, -1.5976,  ..., -0.2583, -0.1849, -0.0656],\n",
      "           [-1.5264,  1.0825,  0.1063,  ..., -0.8657, -1.3858, -0.8365],\n",
      "           ...,\n",
      "           [ 0.6562, -1.7001, -0.9001,  ..., -0.1889,  1.7199,  1.0760],\n",
      "           [ 2.0615,  1.6476,  1.5193,  ...,  0.1718,  1.6152, -0.4657],\n",
      "           [ 0.7267, -1.1767, -0.7081,  ...,  0.4672,  0.2251, -1.0150]]],\n",
      "\n",
      "\n",
      "         [[[-0.9546, -1.8658, -2.4700,  ..., -1.4093,  1.8711, -0.8772],\n",
      "           [-1.1315, -0.1937,  1.2845,  ...,  0.8395, -1.5040, -1.1130],\n",
      "           [ 0.7796, -1.3625, -2.1093,  ..., -2.1008, -0.0475,  0.2380],\n",
      "           ...,\n",
      "           [ 0.7123,  0.4768, -1.3237,  ...,  1.3150, -0.4299,  3.0096],\n",
      "           [ 0.5598,  1.8137,  1.5398,  ...,  0.1840,  1.8430, -1.2602],\n",
      "           [ 2.1478,  1.0206,  0.6094,  ..., -0.2751,  0.0999, -1.9582]]],\n",
      "\n",
      "\n",
      "         [[[-1.2556, -0.5760,  1.6083,  ...,  2.8614,  2.0202,  0.9777],\n",
      "           [ 0.4182,  0.3870,  3.3639,  ..., -0.0179,  1.3502,  3.0770],\n",
      "           [-1.0473,  2.6261,  0.7575,  ...,  2.0681,  0.8948,  2.2732],\n",
      "           ...,\n",
      "           [-1.2022, -1.6484, -1.2866,  ..., -0.5678, -1.4608, -1.8654],\n",
      "           [-2.1293,  2.5307, -0.2028,  ...,  0.8476, -0.2423,  0.7898],\n",
      "           [-0.3165, -0.4777, -0.6138,  ..., -2.9092,  0.5108,  0.6883]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.4919, -0.5530, -0.1579,  ...,  0.0932,  1.4217, -0.6995],\n",
      "           [-1.3102, -0.4234, -0.0174,  ...,  0.1797, -0.4575, -0.2510],\n",
      "           [ 0.9419,  0.5448,  1.3869,  ..., -1.3724, -1.4457,  0.7389],\n",
      "           ...,\n",
      "           [-1.8032,  0.6603,  1.3664,  ...,  1.5258,  0.2805,  0.1275],\n",
      "           [-1.4874,  1.2580,  1.2424,  ..., -1.2073,  0.2091,  1.6877],\n",
      "           [ 1.8610,  0.5151,  1.0767,  ..., -0.6086,  0.6725,  1.0248]]],\n",
      "\n",
      "\n",
      "         [[[-0.0844, -0.5642,  0.1971,  ...,  0.2724,  0.2848, -0.2280],\n",
      "           [-0.2879, -0.1591, -1.4623,  ..., -0.9860,  1.7286,  1.0993],\n",
      "           [-1.9729,  0.0952,  0.9354,  ...,  0.4597,  1.1407, -0.9409],\n",
      "           ...,\n",
      "           [ 1.1320,  0.5632,  0.2260,  ...,  0.2961,  1.2561,  1.5448],\n",
      "           [-1.1179,  0.5044,  0.1817,  ..., -0.4612,  1.3734,  0.3776],\n",
      "           [ 1.2471, -0.8077, -1.1382,  ..., -0.7188, -0.4577,  0.1460]]],\n",
      "\n",
      "\n",
      "         [[[-0.4361, -0.3547, -0.6276,  ..., -0.3935, -0.6959,  1.3066],\n",
      "           [-0.6123,  0.8514,  1.6036,  ..., -1.0777, -0.0569, -2.2582],\n",
      "           [ 0.7167,  1.8301, -0.1972,  ...,  0.2564, -0.1886,  2.1283],\n",
      "           ...,\n",
      "           [-1.0379, -1.4357,  0.8138,  ..., -1.1608, -0.7359,  0.0334],\n",
      "           [-0.9601, -0.5035, -0.3054,  ..., -0.0107, -1.1081, -1.5928],\n",
      "           [-0.6248,  1.3633,  0.4461,  ..., -1.7552, -2.0470, -2.0023]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 4.9502e-01,  6.0526e-01,  5.3588e-01,  ...,  5.9450e-01,\n",
      "             5.4065e-01,  5.2583e-01],\n",
      "           [ 5.1856e-01,  3.2594e-01,  3.4988e-01,  ...,  1.6020e-01,\n",
      "             1.7039e-01,  2.6118e-01],\n",
      "           [ 4.1608e-01,  4.7851e-01,  4.6021e-01,  ...,  2.8479e-01,\n",
      "             3.1667e-01,  3.7537e-01],\n",
      "           ...,\n",
      "           [-2.6020e-01,  2.0988e-01,  3.0069e-02,  ..., -5.3522e-01,\n",
      "            -3.4421e-01, -6.1322e-01],\n",
      "           [-1.3949e-01, -1.8636e-01, -1.8841e-02,  ..., -5.4391e-01,\n",
      "            -5.8038e-01, -4.4892e-01],\n",
      "           [-7.3410e-03, -8.2087e-02, -4.8008e-02,  ..., -3.9542e-01,\n",
      "            -3.8385e-01, -3.9923e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.0656e-01,  6.8913e-01,  4.6816e-01,  ...,  5.8951e-01,\n",
      "             5.0661e-01,  5.5070e-01],\n",
      "           [ 4.4651e-01,  5.0310e-01,  6.2916e-01,  ...,  6.0679e-01,\n",
      "             5.4679e-01,  4.4777e-01],\n",
      "           [ 7.0127e-01,  7.8679e-01,  8.5684e-01,  ...,  6.7735e-01,\n",
      "             6.7250e-01,  5.5014e-01],\n",
      "           ...,\n",
      "           [-2.6274e-01, -1.7909e-02, -8.1818e-02,  ...,  1.7352e-01,\n",
      "             6.4338e-02,  3.4535e-02],\n",
      "           [ 2.2048e-01, -6.3306e-01,  2.1822e-01,  ...,  6.4777e-01,\n",
      "            -2.2181e-01, -7.0451e-02],\n",
      "           [-2.0953e-01, -1.8204e-01,  2.1682e-01,  ...,  1.9509e-01,\n",
      "             5.8063e-01, -1.5024e-02]]],\n",
      "\n",
      "\n",
      "         [[[-7.6620e-01, -6.2559e-01, -1.0071e+00,  ..., -9.8138e-01,\n",
      "            -1.0726e+00, -8.7286e-01],\n",
      "           [-5.7740e-01, -9.3559e-01, -9.4801e-01,  ..., -1.0743e+00,\n",
      "            -1.0220e+00, -9.4938e-01],\n",
      "           [-8.1883e-01, -9.4169e-01, -9.7816e-01,  ..., -8.4179e-01,\n",
      "            -8.7955e-01, -7.1961e-01],\n",
      "           ...,\n",
      "           [ 1.7297e-01, -2.4816e-01, -2.9347e-01,  ..., -4.7199e-04,\n",
      "             6.1321e-01,  5.6914e-01],\n",
      "           [ 2.3723e-01, -9.3725e-02,  1.1954e-02,  ...,  2.1992e-01,\n",
      "             5.9537e-01,  5.0351e-01],\n",
      "           [-1.8680e-01, -2.1700e-01, -2.3635e-01,  ...,  1.3838e-01,\n",
      "             5.4410e-02,  5.2693e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.4880e-03,  1.0277e-01, -2.0634e-01,  ..., -2.8762e-01,\n",
      "            -3.6114e-01, -1.4586e-01],\n",
      "           [ 7.9941e-01,  3.5902e-01,  5.9520e-01,  ...,  6.8148e-01,\n",
      "             8.7628e-01,  6.0920e-01],\n",
      "           [-4.1334e-01, -6.9398e-01, -7.0444e-01,  ..., -3.7635e-01,\n",
      "            -3.2881e-01, -3.9471e-01],\n",
      "           ...,\n",
      "           [-3.9387e-01, -3.6915e-01, -3.4823e-01,  ..., -5.0374e-01,\n",
      "            -8.2963e-01, -7.6978e-04],\n",
      "           [-3.3560e-01, -7.9925e-01, -6.8144e-02,  ..., -6.9000e-02,\n",
      "            -8.4430e-01,  2.6403e-02],\n",
      "           [-9.5909e-01, -7.3312e-01, -1.2886e-01,  ..., -3.0071e-01,\n",
      "            -9.5936e-02, -1.1258e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.5170e-01,  2.3048e-03, -2.8902e-01,  ..., -1.8921e-01,\n",
      "            -1.8588e-01, -1.7979e-01],\n",
      "           [-9.6306e-02, -2.9959e-01, -2.7363e-01,  ..., -9.3978e-02,\n",
      "            -2.2496e-01, -1.9490e-01],\n",
      "           [-2.4023e-01, -1.7040e-01, -2.4320e-01,  ..., -2.5709e-02,\n",
      "             3.5877e-02, -3.3023e-02],\n",
      "           ...,\n",
      "           [-3.7971e-01, -1.5686e-01, -2.3655e-01,  ..., -7.3711e-02,\n",
      "            -1.9958e-01,  3.9651e-02],\n",
      "           [ 2.5341e-03, -4.6691e-01, -7.5014e-02,  ...,  1.1202e-01,\n",
      "            -8.1061e-02, -2.4766e-02],\n",
      "           [-4.1418e-01, -1.6279e-01, -1.5270e-01,  ..., -1.6930e-01,\n",
      "             2.6231e-02, -2.1720e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.3880e-01,  4.2249e-03, -3.5943e-01,  ..., -4.2441e-01,\n",
      "            -4.6685e-01, -3.8415e-01],\n",
      "           [ 3.2059e-02, -3.3985e-01, -3.6362e-01,  ..., -2.0537e-01,\n",
      "            -2.8486e-01, -3.3462e-01],\n",
      "           [ 6.5158e-02, -2.6025e-02,  1.3774e-02,  ...,  7.9668e-03,\n",
      "            -2.6784e-02, -1.3075e-01],\n",
      "           ...,\n",
      "           [ 5.5462e-01,  6.6057e-02,  1.5042e-01,  ...,  7.9579e-01,\n",
      "             1.0202e+00,  6.3886e-01],\n",
      "           [ 7.7260e-01,  6.3379e-01,  3.8156e-01,  ...,  8.6655e-01,\n",
      "             1.1172e+00,  1.0092e+00],\n",
      "           [ 4.5657e-01,  6.1374e-01,  3.7309e-01,  ...,  5.8119e-01,\n",
      "             4.1747e-01,  1.0809e+00]]]]]), tensor([]), tensor([24.1269]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-2.2645e+00,  2.4607e+00, -1.1620e+00,  ..., -1.7025e+00,\n",
      "            -8.0161e-01, -1.0632e+00],\n",
      "           [ 4.3014e-01,  3.9931e-01, -8.5287e-02,  ..., -6.3353e-01,\n",
      "             3.4659e-02, -1.3165e+00],\n",
      "           [-1.3121e+00, -7.1500e-01, -2.6617e-01,  ...,  8.8293e-02,\n",
      "            -8.3493e-01, -8.0868e-01],\n",
      "           ...,\n",
      "           [-7.4975e-01, -1.6545e+00, -1.1993e-03,  ...,  4.9076e-03,\n",
      "             6.5325e-01, -6.6960e-01],\n",
      "           [-1.2082e+00, -1.2901e+00,  7.2802e-01,  ..., -6.1262e-01,\n",
      "             8.9271e-01,  1.2532e+00],\n",
      "           [-1.0068e+00, -1.1277e+00, -4.8790e-01,  ...,  8.1868e-01,\n",
      "             5.9868e-01,  4.7226e-01]]],\n",
      "\n",
      "\n",
      "         [[[-5.1822e-01, -5.6819e-01, -1.4031e-01,  ..., -1.9153e-01,\n",
      "            -1.6012e+00, -9.9281e-02],\n",
      "           [-2.3536e+00, -3.9898e-01, -1.6658e-01,  ...,  3.9360e-01,\n",
      "            -1.0892e+00, -1.6490e+00],\n",
      "           [ 7.7308e-01, -1.5228e-01,  2.4271e-01,  ..., -2.3413e+00,\n",
      "            -1.2118e+00, -9.6616e-01],\n",
      "           ...,\n",
      "           [-3.6363e-02,  4.0935e-01, -1.9196e+00,  ..., -5.9127e-01,\n",
      "             2.3172e-01,  2.2612e-01],\n",
      "           [ 3.3965e-01,  5.9148e-01,  7.6694e-01,  ..., -6.4545e-01,\n",
      "             1.0333e+00, -1.5863e+00],\n",
      "           [-8.5029e-01, -2.1520e+00, -3.4254e-01,  ..., -9.8146e-01,\n",
      "            -8.7360e-01, -2.2495e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.2410e-01,  8.0967e-01,  9.1995e-01,  ...,  1.2599e+00,\n",
      "            -8.6350e-01,  3.5367e-01],\n",
      "           [ 1.2926e+00,  1.4392e+00,  1.6988e+00,  ...,  2.2380e+00,\n",
      "             1.2421e+00,  1.9187e+00],\n",
      "           [ 1.4791e+00,  2.4857e+00,  1.3365e+00,  ...,  8.8439e-01,\n",
      "             2.2629e-01,  5.6634e-01],\n",
      "           ...,\n",
      "           [ 5.5587e-01, -5.4360e-02,  1.7537e+00,  ..., -1.5708e-01,\n",
      "            -2.2573e+00, -1.5674e+00],\n",
      "           [-1.6017e+00,  1.2418e+00,  1.5558e-01,  ..., -1.0940e+00,\n",
      "             3.4830e-01, -1.7121e-01],\n",
      "           [-6.4971e-01,  3.4001e-01,  8.7240e-01,  ...,  1.6467e-01,\n",
      "            -1.2765e+00,  4.6393e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.0968e+00,  8.1758e-02,  4.1526e-01,  ...,  1.2968e+00,\n",
      "            -1.9301e+00,  1.0024e+00],\n",
      "           [-4.1371e-01, -1.1371e+00, -7.0367e-01,  ...,  3.5345e-01,\n",
      "            -2.1328e-01, -4.1338e-01],\n",
      "           [ 8.1809e-01, -8.2333e-01, -3.4595e-01,  ..., -2.7351e+00,\n",
      "            -5.6107e-01, -4.5395e-01],\n",
      "           ...,\n",
      "           [ 7.8791e-01,  4.5170e-01,  2.2401e-01,  ...,  1.1279e+00,\n",
      "             1.2864e+00, -6.2666e-01],\n",
      "           [-2.9063e-01,  1.6199e+00,  6.7506e-01,  ...,  1.0001e+00,\n",
      "             1.1709e+00, -1.2395e-01],\n",
      "           [ 7.9517e-01,  2.4502e+00,  1.0958e+00,  ..., -1.7546e-01,\n",
      "            -8.2122e-01,  1.0626e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3958e-01, -5.7730e-01,  1.0308e-01,  ...,  8.5662e-01,\n",
      "             1.0451e+00,  5.7153e-01],\n",
      "           [ 9.0325e-01, -8.6976e-01, -7.4160e-02,  ...,  1.1680e+00,\n",
      "             3.9419e-02, -4.1543e-01],\n",
      "           [ 2.1114e-02,  2.4212e+00,  5.8637e-01,  ..., -9.5298e-01,\n",
      "            -4.2485e-01,  1.0701e+00],\n",
      "           ...,\n",
      "           [-2.0034e-01,  1.9253e+00, -7.8979e-01,  ..., -3.3822e-02,\n",
      "            -2.4467e-03,  7.0978e-01],\n",
      "           [ 2.5806e+00,  1.2061e+00, -4.4946e-01,  ..., -2.0610e+00,\n",
      "             1.0956e+00, -5.8739e-02],\n",
      "           [ 1.7870e+00,  1.4221e+00,  1.6261e+00,  ...,  7.6077e-01,\n",
      "            -1.1702e+00,  1.9519e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1805e+00, -6.9703e-02, -9.5446e-02,  ..., -2.1757e+00,\n",
      "            -7.2182e-01,  1.2801e-01],\n",
      "           [ 7.7154e-01,  1.2902e-02, -1.4150e+00,  ...,  2.0746e-02,\n",
      "             3.6286e-01, -3.5135e-01],\n",
      "           [ 7.7488e-01,  4.9933e-01,  7.6990e-01,  ..., -9.7830e-01,\n",
      "             9.3379e-01, -9.1025e-02],\n",
      "           ...,\n",
      "           [-9.1840e-01,  1.6960e-02, -2.4193e+00,  ..., -5.4658e-01,\n",
      "            -6.5604e-01,  2.7924e-01],\n",
      "           [ 2.7011e-01, -4.1880e-01, -4.1044e-01,  ...,  2.6812e-01,\n",
      "            -1.3290e+00, -1.9437e+00],\n",
      "           [-2.0224e+00, -6.2171e-01, -2.1329e+00,  ..., -2.7390e+00,\n",
      "             8.2567e-01, -1.5219e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.1151,  0.5948,  0.3378,  ...,  0.7634,  0.7584,  1.0597],\n",
      "           [ 0.2874, -0.1942,  0.1393,  ...,  0.1825,  0.3205, -0.0956],\n",
      "           [ 0.5721,  0.5321,  0.6159,  ...,  0.1114,  0.5373,  0.5732],\n",
      "           ...,\n",
      "           [-0.2375,  0.3542, -0.0200,  ..., -0.4180, -0.2768, -0.1812],\n",
      "           [-0.5262, -0.4268, -0.0282,  ..., -0.7048, -0.6165, -0.6053],\n",
      "           [-0.0989,  0.3129, -0.0988,  ..., -0.6234, -0.9098, -0.7234]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3400,  0.3917,  0.6461,  ...,  0.5424,  0.2792,  0.2511],\n",
      "           [ 0.5468,  0.4736,  0.0418,  ...,  0.9646,  0.1882,  0.3453],\n",
      "           [ 0.3280,  0.6245,  0.5040,  ...,  0.3338,  0.2899,  0.1414],\n",
      "           ...,\n",
      "           [-0.3065, -0.0153,  0.7145,  ..., -0.1041, -0.5487,  0.3550],\n",
      "           [ 0.4376, -0.2652, -0.0082,  ...,  0.7771,  0.2889,  0.4197],\n",
      "           [ 0.2162, -0.2260,  0.4155,  ...,  0.5760, -0.0371, -0.3989]]],\n",
      "\n",
      "\n",
      "         [[[-1.1392, -0.4724, -0.9259,  ..., -0.3001, -0.4854, -0.7613],\n",
      "           [-0.5861, -0.8090, -0.3713,  ..., -0.2519, -1.1563, -0.8903],\n",
      "           [-0.8329, -0.4793, -0.3824,  ..., -0.3222, -0.1151, -0.7394],\n",
      "           ...,\n",
      "           [ 0.4857, -0.2793, -0.0672,  ..., -0.0762,  0.3298,  0.2914],\n",
      "           [-0.5010,  0.1428,  0.0264,  ...,  0.4636,  0.1074, -0.1476],\n",
      "           [-0.8477, -0.3501, -0.2929,  ...,  0.7392,  0.0542,  0.3042]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.2688,  0.2264, -0.2356,  ..., -0.0456, -0.3181, -0.1949],\n",
      "           [ 0.5419,  0.2609,  0.5956,  ...,  0.3179,  1.5196,  0.3990],\n",
      "           [-0.7389, -0.3194,  0.0359,  ..., -0.5554, -0.2647,  0.0741],\n",
      "           ...,\n",
      "           [-0.0964, -0.9123, -0.7365,  ..., -0.1452,  0.0835,  0.0385],\n",
      "           [-0.1417, -0.6082,  0.4270,  ..., -0.6060, -0.5466,  0.0958],\n",
      "           [-0.5670, -0.8547, -0.2939,  ..., -0.5148, -0.4154, -0.6179]]],\n",
      "\n",
      "\n",
      "         [[[-0.7309, -0.1801, -0.0759,  ...,  0.2578, -0.1870,  0.1567],\n",
      "           [ 0.0471,  0.2609,  0.4579,  ...,  0.2072,  0.1906, -0.1384],\n",
      "           [-0.1984, -0.0845, -0.3678,  ..., -0.3838,  0.1339, -0.3654],\n",
      "           ...,\n",
      "           [-0.8711, -0.2428, -0.1214,  ..., -0.1414,  0.0893,  0.2955],\n",
      "           [ 0.4087, -0.2101, -0.5899,  ..., -0.3713, -0.1500, -0.2332],\n",
      "           [-0.2088,  0.1496, -0.5651,  ...,  0.1817, -0.0037, -0.1597]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2395, -0.0423,  0.1227,  ..., -0.3479, -0.2620, -0.5263],\n",
      "           [-0.1423,  0.1893, -0.1298,  ..., -0.0251, -0.6065, -0.3781],\n",
      "           [ 0.0830, -0.0126, -0.2533,  ..., -0.7205, -0.2054, -0.4987],\n",
      "           ...,\n",
      "           [-0.1225,  0.2558, -0.3822,  ...,  0.5961,  1.1854,  0.7343],\n",
      "           [ 0.4340,  0.6291,  0.2081,  ...,  0.8246,  0.6435,  0.6796],\n",
      "           [ 0.2157,  0.6796,  0.1571,  ...,  0.7650,  0.5837,  0.4813]]]]]), tensor([]), tensor([296.2223]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.2029,  0.4027, -0.7431,  ...,  0.4523,  0.5771,  1.7188],\n",
      "           [-0.5370, -1.8180, -0.6821,  ...,  0.0096,  0.6006, -1.2738],\n",
      "           [ 0.5910,  0.4451,  0.4709,  ..., -0.5041,  0.8851,  0.8529],\n",
      "           ...,\n",
      "           [ 0.0927,  0.3332,  0.0508,  ..., -0.3243,  1.0459,  0.7461],\n",
      "           [-1.3752, -0.9851, -0.0530,  ..., -0.4542, -0.1542, -0.5724],\n",
      "           [-0.4752,  1.2707, -0.1381,  ..., -0.5726, -1.2300, -1.3187]]],\n",
      "\n",
      "\n",
      "         [[[-1.0043, -1.0544,  0.4089,  ..., -0.5520, -1.3080, -1.0986],\n",
      "           [-0.0738, -0.4835, -1.9923,  ...,  1.0374, -1.2862, -0.3136],\n",
      "           [-1.9074, -1.3501, -2.1313,  ..., -0.7339, -0.9417, -1.0309],\n",
      "           ...,\n",
      "           [-0.3010, -0.1308,  2.7753,  ..., -0.8513, -1.0505,  1.4684],\n",
      "           [ 0.7027,  1.1071, -0.7274,  ...,  1.3094,  1.3145,  0.5304],\n",
      "           [ 1.4959, -0.2237,  0.7030,  ...,  1.7131, -0.4219, -1.3878]]],\n",
      "\n",
      "\n",
      "         [[[-1.2303,  0.7818,  0.3496,  ...,  2.4686,  1.8184,  0.4177],\n",
      "           [ 0.0682,  0.8795,  2.3248,  ...,  2.7362, -0.1796,  0.4445],\n",
      "           [-0.0344,  1.6512,  2.0993,  ...,  2.1195,  2.7005, -0.1593],\n",
      "           ...,\n",
      "           [ 1.1356, -0.2293,  0.8916,  ..., -0.0791, -0.1694, -0.8935],\n",
      "           [-2.5686,  0.7716,  0.0659,  ...,  0.7265, -1.1915, -1.0995],\n",
      "           [-2.2735, -0.2225, -0.2069,  ...,  0.8630,  0.0298,  1.7110]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.4001,  0.1105,  0.6340,  ...,  0.9372, -1.1204, -0.8603],\n",
      "           [-0.8553, -0.4677,  0.5898,  ..., -0.9982,  2.3928, -0.8382],\n",
      "           [-0.7059,  1.2189,  0.9980,  ..., -1.3440,  0.8080,  1.5682],\n",
      "           ...,\n",
      "           [ 0.9689, -1.7646, -0.9067,  ...,  0.3510,  1.3139,  0.9161],\n",
      "           [ 0.6559,  0.8357,  1.7257,  ..., -0.1586, -0.7428,  0.4983],\n",
      "           [ 1.6348,  0.4729, -0.2305,  ..., -0.0451, -0.3724, -0.7542]]],\n",
      "\n",
      "\n",
      "         [[[-2.1454, -1.0409,  0.4802,  ...,  1.4000,  0.1936,  1.2916],\n",
      "           [ 0.5442,  1.4335,  2.1410,  ...,  0.8778,  1.3357,  0.0770],\n",
      "           [-0.0881,  0.0912, -0.7297,  ..., -1.2077,  0.5644, -0.8815],\n",
      "           ...,\n",
      "           [-1.6614, -0.1387,  0.4120,  ...,  0.6121,  1.2741,  1.9261],\n",
      "           [ 1.5621,  1.1060, -1.8840,  ..., -0.7237,  0.3255,  0.1712],\n",
      "           [ 0.7175,  0.9205, -1.0678,  ...,  0.9004,  0.9367,  0.9431]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4217, -0.0415,  1.4921,  ...,  0.0349,  0.6427, -0.4306],\n",
      "           [-0.7013,  1.5839,  0.3988,  ...,  0.4320, -1.1674, -0.3988],\n",
      "           [-0.2371, -0.4184, -0.9627,  ..., -2.2597, -0.7082, -1.3721],\n",
      "           ...,\n",
      "           [-2.3772,  0.3673, -2.0886,  ...,  0.7584, -0.0879, -0.5488],\n",
      "           [-1.1259, -0.2073, -0.4208,  ..., -0.3034, -1.0392,  0.7162],\n",
      "           [-0.9301,  0.1093, -0.9978,  ..., -0.4206, -0.4432,  1.5661]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 3.3059e-01,  5.1809e-01,  5.7666e-01,  ...,  6.1003e-01,\n",
      "             6.3859e-01,  6.2702e-01],\n",
      "           [ 4.3427e-01,  3.8383e-01,  2.0115e-01,  ...,  6.4870e-02,\n",
      "            -2.5567e-02,  3.1326e-02],\n",
      "           [ 4.9367e-01,  2.9803e-01,  4.5706e-01,  ...,  1.7623e-01,\n",
      "             4.1522e-01,  2.0363e-01],\n",
      "           ...,\n",
      "           [-4.2798e-01,  1.6986e-01,  8.6940e-02,  ..., -4.0520e-01,\n",
      "            -4.4984e-01, -3.9919e-01],\n",
      "           [ 8.4468e-02, -1.7567e-01, -5.9024e-02,  ..., -3.9357e-01,\n",
      "            -4.5391e-01, -3.1574e-01],\n",
      "           [-4.3033e-02,  7.8175e-02, -5.7693e-02,  ..., -2.3889e-01,\n",
      "            -2.7648e-01, -1.3322e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.2782e-01,  8.0738e-01,  6.2634e-01,  ...,  3.3778e-01,\n",
      "             5.8781e-01,  5.9520e-01],\n",
      "           [ 4.4778e-01,  4.0589e-01,  6.6306e-01,  ...,  7.4104e-01,\n",
      "             5.9413e-01,  3.5888e-01],\n",
      "           [ 8.1451e-01,  8.7906e-01,  7.7218e-01,  ...,  5.8189e-01,\n",
      "             6.7838e-01,  4.8527e-01],\n",
      "           ...,\n",
      "           [-1.3468e-01,  1.0728e-01,  2.2446e-02,  ..., -1.3841e-01,\n",
      "            -1.2256e-04,  1.3762e-02],\n",
      "           [ 1.9259e-01, -6.8009e-01,  2.8282e-01,  ...,  1.1497e-02,\n",
      "             9.3823e-02,  1.9145e-02],\n",
      "           [-3.4950e-01,  6.8771e-02,  1.8796e-01,  ...,  2.9748e-01,\n",
      "            -3.4618e-01,  7.4694e-02]]],\n",
      "\n",
      "\n",
      "         [[[-6.5473e-01, -5.8501e-01, -9.1127e-01,  ..., -1.0116e+00,\n",
      "            -9.5235e-01, -6.9102e-01],\n",
      "           [-6.2868e-01, -8.8016e-01, -1.1062e+00,  ..., -9.6965e-01,\n",
      "            -8.7087e-01, -8.8433e-01],\n",
      "           [-7.2470e-01, -9.9200e-01, -8.5243e-01,  ..., -8.6003e-01,\n",
      "            -9.0901e-01, -7.8753e-01],\n",
      "           ...,\n",
      "           [ 1.2171e-01, -2.9148e-01, -2.4243e-01,  ...,  9.8693e-02,\n",
      "             3.3094e-01,  5.1992e-01],\n",
      "           [ 3.1131e-02, -2.8323e-02,  4.9320e-02,  ...,  2.6404e-01,\n",
      "             8.1306e-02,  2.9463e-01],\n",
      "           [-1.9313e-01, -2.5020e-01, -1.3447e-01,  ...,  3.8465e-01,\n",
      "             3.3545e-01,  1.1219e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-7.5429e-02,  7.1060e-02, -2.7207e-01,  ..., -1.9003e-01,\n",
      "             1.1360e-01,  3.1154e-01],\n",
      "           [ 8.2115e-01,  5.2078e-01,  2.6530e-01,  ...,  6.8918e-01,\n",
      "             8.1083e-01,  7.0510e-01],\n",
      "           [-3.9348e-01, -5.1077e-01, -4.2266e-01,  ...,  2.1075e-02,\n",
      "            -3.2222e-01, -5.9458e-02],\n",
      "           ...,\n",
      "           [-6.0904e-01, -3.1506e-01, -3.3996e-01,  ..., -5.4554e-01,\n",
      "            -8.6214e-01, -4.1857e-01],\n",
      "           [-3.6616e-01, -9.6004e-01, -8.4751e-03,  ..., -1.7194e-01,\n",
      "            -5.2577e-01, -2.0576e-01],\n",
      "           [-1.0118e+00, -8.8107e-01, -1.8205e-01,  ..., -2.5036e-01,\n",
      "            -3.4495e-01, -2.3062e-01]]],\n",
      "\n",
      "\n",
      "         [[[-9.3925e-02,  1.6759e-01, -1.6082e-01,  ..., -1.4783e-01,\n",
      "            -1.9740e-01, -2.7340e-01],\n",
      "           [-1.9723e-01, -7.5667e-02, -6.8179e-02,  ..., -1.7449e-01,\n",
      "            -1.3665e-01, -1.3536e-01],\n",
      "           [-9.0083e-02, -2.2682e-01, -1.9239e-01,  ...,  9.9145e-02,\n",
      "             2.0141e-01,  7.4335e-03],\n",
      "           ...,\n",
      "           [-3.2154e-01, -1.4470e-01, -2.7260e-01,  ..., -3.8328e-01,\n",
      "            -2.8848e-01, -2.3335e-01],\n",
      "           [-1.0880e-01, -3.0317e-01,  5.7328e-02,  ..., -3.6747e-02,\n",
      "            -1.1078e-01, -1.6434e-01],\n",
      "           [-2.5901e-01, -5.4679e-02, -2.1483e-01,  ..., -1.9927e-01,\n",
      "            -1.5924e-01, -3.4783e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.8600e-01,  6.9692e-02, -3.7749e-01,  ..., -3.1444e-01,\n",
      "            -5.2464e-01, -4.4760e-01],\n",
      "           [ 9.6271e-02, -1.8137e-01, -3.5758e-01,  ..., -1.0451e-01,\n",
      "            -2.6953e-01, -2.6102e-01],\n",
      "           [ 6.6886e-02,  5.9754e-03, -7.7463e-02,  ..., -1.1961e-01,\n",
      "            -2.0615e-01, -3.2014e-01],\n",
      "           ...,\n",
      "           [ 6.5102e-01,  3.1104e-02,  3.6829e-02,  ...,  6.9120e-01,\n",
      "             7.7821e-01,  5.5099e-01],\n",
      "           [ 7.5884e-01,  5.8312e-01,  4.8375e-01,  ...,  5.9656e-01,\n",
      "             8.5514e-01,  8.2393e-01],\n",
      "           [ 4.1906e-01,  7.5378e-01,  2.9320e-01,  ...,  6.2559e-01,\n",
      "             7.2790e-01,  2.5456e-01]]]]]), tensor([]), tensor([113.2757]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.8122e+00,  1.7021e-02, -4.4871e-01,  ...,  1.8688e-01,\n",
      "             5.3286e-02,  1.2219e-01],\n",
      "           [ 1.6732e-01,  4.3382e-01, -1.2477e+00,  ..., -5.5669e-01,\n",
      "            -2.0245e+00, -2.2670e+00],\n",
      "           [ 5.9814e-01, -1.1678e+00,  6.1295e-02,  ..., -8.0139e-01,\n",
      "             1.3438e+00, -1.1316e+00],\n",
      "           ...,\n",
      "           [-1.2028e+00, -9.3341e-01,  5.7228e-01,  ...,  1.3426e+00,\n",
      "             1.1018e+00,  2.1082e-01],\n",
      "           [ 1.8060e+00, -2.5318e-01, -4.9655e-01,  ...,  6.1205e-01,\n",
      "             8.5521e-01,  1.1929e+00],\n",
      "           [-3.6478e-01,  1.0665e+00,  1.3699e-01,  ...,  1.7014e+00,\n",
      "             1.4218e+00,  7.8611e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1254e+00,  6.2357e-01, -4.1857e-02,  ..., -2.6870e+00,\n",
      "            -8.4899e-01, -8.2356e-01],\n",
      "           [-8.3670e-01, -1.0424e+00,  1.6092e-01,  ...,  8.4037e-01,\n",
      "             5.6401e-01, -3.6684e-01],\n",
      "           [-4.9640e-01, -1.0275e+00, -2.6674e+00,  ..., -1.0822e+00,\n",
      "             6.0523e-01,  1.1582e-01],\n",
      "           ...,\n",
      "           [ 1.3746e+00,  8.7931e-01,  2.8136e-01,  ...,  7.3443e-01,\n",
      "            -1.4992e+00,  1.7855e+00],\n",
      "           [-2.6769e-01, -6.0556e-01,  6.5409e-02,  ..., -8.2009e-01,\n",
      "             4.9521e-01, -7.2910e-01],\n",
      "           [-2.2007e-01,  1.0822e+00,  2.6330e-02,  ...,  2.6225e+00,\n",
      "            -1.0221e+00, -5.1062e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0651e+00,  3.4289e-01,  9.6211e-01,  ..., -1.2007e-01,\n",
      "             6.4686e-01,  1.7576e+00],\n",
      "           [-3.8962e-01,  1.6996e+00, -5.9781e-01,  ...,  7.3244e-01,\n",
      "             1.9942e+00,  6.3533e-01],\n",
      "           [ 9.2644e-01, -3.2000e-01,  2.3439e+00,  ...,  7.7786e-01,\n",
      "             4.3809e-01, -5.7087e-02],\n",
      "           ...,\n",
      "           [-3.6165e-01, -3.4564e-01,  1.3248e+00,  ..., -4.8954e-01,\n",
      "            -1.0218e+00, -1.9944e+00],\n",
      "           [-1.9137e+00,  6.1107e-01,  3.1457e-01,  ...,  1.6750e+00,\n",
      "            -1.0251e+00, -1.0972e+00],\n",
      "           [ 1.3208e-01,  7.6675e-03,  8.6482e-01,  ..., -1.5259e+00,\n",
      "            -2.0264e-02,  1.7405e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.7333e+00,  7.9780e-01, -5.3633e-01,  ...,  2.8146e-01,\n",
      "             9.0723e-01,  6.4905e-01],\n",
      "           [ 9.4109e-01,  9.1498e-01, -2.4152e+00,  ..., -2.9007e-01,\n",
      "            -1.2415e+00, -7.8627e-02],\n",
      "           [ 1.0594e+00,  1.5108e+00, -2.6921e-01,  ...,  1.3800e+00,\n",
      "            -6.6918e-01,  6.1603e-01],\n",
      "           ...,\n",
      "           [-1.0903e+00,  1.0354e+00,  2.0710e-01,  ...,  5.9435e-01,\n",
      "             2.3426e-01,  1.2673e+00],\n",
      "           [-1.5153e-01, -5.6132e-01,  7.1413e-01,  ..., -4.6811e-01,\n",
      "             1.4700e+00, -3.6336e-01],\n",
      "           [ 6.2860e-01,  3.6549e-01, -6.8896e-01,  ..., -1.1169e+00,\n",
      "             5.9808e-01,  2.8141e+00]]],\n",
      "\n",
      "\n",
      "         [[[-3.2176e-02,  5.3871e-01,  6.2622e-01,  ...,  2.5942e-01,\n",
      "             4.2239e-01, -7.7667e-01],\n",
      "           [-5.2045e-01,  9.1665e-01,  4.6998e-01,  ..., -1.0399e+00,\n",
      "             4.3657e-01,  1.4801e+00],\n",
      "           [ 5.9522e-01, -3.5624e-01, -7.0340e-01,  ...,  1.3481e-01,\n",
      "             8.8190e-01,  6.5801e-03],\n",
      "           ...,\n",
      "           [ 8.0853e-01,  6.5536e-01, -5.2186e-01,  ...,  7.4956e-01,\n",
      "            -7.3831e-01, -2.3063e-01],\n",
      "           [-5.0947e-01,  2.0656e+00,  1.4117e+00,  ...,  1.4243e+00,\n",
      "             1.3456e-03,  5.1222e-01],\n",
      "           [ 1.3906e+00,  5.4278e-02, -2.4382e-02,  ..., -6.0617e-01,\n",
      "            -2.4897e-01,  3.6022e-01]]],\n",
      "\n",
      "\n",
      "         [[[-8.5391e-01,  1.3862e+00, -7.0544e-02,  ...,  7.6694e-01,\n",
      "             3.7874e-01,  6.3091e-02],\n",
      "           [ 7.1850e-01,  1.3961e+00, -5.3475e-01,  ...,  4.7410e-01,\n",
      "            -5.1619e-01,  1.0776e-01],\n",
      "           [-4.8036e-01,  3.8840e-01,  1.3446e-01,  ..., -3.4361e-01,\n",
      "            -1.1115e+00, -1.3480e+00],\n",
      "           ...,\n",
      "           [ 7.3649e-01, -2.4939e-01, -1.3158e+00,  ..., -2.6507e+00,\n",
      "            -1.8236e+00, -1.2964e+00],\n",
      "           [-5.0900e-02, -5.3662e-01,  1.1312e+00,  ..., -6.2031e-01,\n",
      "            -1.4155e-01,  2.0346e-01],\n",
      "           [-5.3942e-01,  5.6827e-01, -1.2953e+00,  ...,  1.2849e-01,\n",
      "             8.4835e-01,  2.2649e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 1.8155e-01,  2.5659e-01,  8.4705e-01,  ...,  2.6576e-01,\n",
      "             2.4722e-01,  4.0520e-01],\n",
      "           [ 5.5295e-01,  4.2475e-01,  4.1314e-01,  ..., -5.6514e-01,\n",
      "            -7.4693e-02,  2.1006e-01],\n",
      "           [ 7.6475e-02,  9.2151e-02,  1.8322e-01,  ...,  1.8538e-01,\n",
      "             8.9913e-01,  4.8732e-01],\n",
      "           ...,\n",
      "           [ 1.8044e-02,  2.8945e-01,  1.2314e-01,  ..., -4.8732e-01,\n",
      "            -7.2479e-01, -5.5274e-01],\n",
      "           [ 2.0654e-01, -5.5799e-01,  3.2380e-01,  ...,  4.0358e-01,\n",
      "            -6.2653e-01, -3.8660e-01],\n",
      "           [-2.5638e-01, -2.7545e-01, -2.4682e-01,  ...,  1.6016e-02,\n",
      "            -3.2194e-01, -4.3744e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.2291e-01,  9.1566e-01,  2.8339e-01,  ...,  7.9323e-01,\n",
      "             5.3470e-01,  6.9911e-01],\n",
      "           [ 4.8335e-01,  3.1821e-01,  1.0767e+00,  ...,  6.1415e-01,\n",
      "             3.4925e-01, -1.9832e-03],\n",
      "           [ 7.1240e-01,  9.4416e-01,  1.1402e+00,  ...,  6.2784e-01,\n",
      "            -3.0210e-01,  3.5840e-01],\n",
      "           ...,\n",
      "           [-3.1682e-01,  5.7644e-01,  2.8705e-01,  ..., -2.8424e-01,\n",
      "            -3.4095e-02, -1.6231e-01],\n",
      "           [ 4.8851e-01, -4.7918e-01, -2.2176e-02,  ..., -3.3090e-01,\n",
      "            -3.9201e-01, -1.2931e-01],\n",
      "           [-2.8023e-01,  2.8772e-02, -4.5225e-01,  ..., -2.0754e-01,\n",
      "             3.8732e-01, -7.4422e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.7131e-01, -7.8810e-02, -2.4823e-01,  ..., -8.7583e-01,\n",
      "            -7.2849e-01, -2.4168e-01],\n",
      "           [-1.9725e-01, -9.0804e-01, -6.9612e-01,  ..., -1.0478e+00,\n",
      "            -2.4018e-01, -1.0157e+00],\n",
      "           [-2.9593e-01, -7.7852e-01, -9.5502e-01,  ..., -2.6105e-01,\n",
      "            -1.0353e+00, -6.3185e-01],\n",
      "           ...,\n",
      "           [-1.7480e-01, -5.0217e-01,  4.5394e-01,  ..., -5.4872e-01,\n",
      "             3.8380e-02,  3.5681e-01],\n",
      "           [-4.1040e-03, -2.7610e-01,  4.8301e-01,  ...,  2.4777e-01,\n",
      "             2.0976e-01,  5.2800e-01],\n",
      "           [ 1.9228e-01,  1.9356e-02,  1.7295e-02,  ...,  2.7266e-01,\n",
      "             6.5489e-01, -1.2876e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-5.5240e-01, -1.7038e-01, -2.0376e-01,  ..., -2.4333e-01,\n",
      "            -3.6820e-01,  1.2740e-01],\n",
      "           [ 7.4367e-01,  1.3108e-01,  2.7506e-01,  ...,  9.1631e-01,\n",
      "             4.6775e-01,  5.8529e-01],\n",
      "           [ 6.1646e-02, -5.9481e-01, -4.0571e-01,  ..., -2.4335e-01,\n",
      "            -4.3935e-01,  3.0085e-04],\n",
      "           ...,\n",
      "           [-2.9611e-02, -6.1067e-01,  1.9484e-01,  ..., -5.0106e-01,\n",
      "            -7.9051e-01,  3.2272e-01],\n",
      "           [-2.1318e-01, -9.2010e-01, -1.6282e-01,  ..., -4.3341e-01,\n",
      "            -4.7786e-01,  7.5578e-02],\n",
      "           [-9.8896e-01, -8.9527e-01,  1.7562e-02,  ..., -4.8350e-01,\n",
      "            -9.5904e-01, -3.0115e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.8404e-01,  4.6305e-01, -4.5682e-01,  ..., -3.5733e-01,\n",
      "            -6.8816e-01, -1.3795e-01],\n",
      "           [ 2.2723e-01, -8.5291e-01, -5.5536e-01,  ...,  2.6094e-01,\n",
      "            -2.4370e-01, -5.4137e-01],\n",
      "           [-4.5987e-01, -2.0743e-02, -1.2600e-01,  ..., -1.6327e-01,\n",
      "             5.1090e-02, -1.2099e-01],\n",
      "           ...,\n",
      "           [-2.1247e-01,  2.0069e-02, -1.3474e-01,  ..., -4.6930e-01,\n",
      "            -4.6391e-01, -1.1670e-01],\n",
      "           [-1.0695e-01, -8.0188e-01, -3.7090e-01,  ..., -7.3135e-01,\n",
      "            -1.5607e-01, -1.1157e-01],\n",
      "           [-2.5422e-02,  4.4421e-01, -1.4768e-01,  ..., -1.1206e-01,\n",
      "            -5.7541e-01, -5.4269e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.7541e-01,  1.6915e-01,  4.2613e-02,  ..., -6.3852e-02,\n",
      "            -1.6568e-01, -3.5985e-01],\n",
      "           [-2.8312e-01,  1.4896e-01, -2.6201e-01,  ..., -2.7733e-01,\n",
      "            -4.4762e-01, -4.4686e-01],\n",
      "           [-1.6458e-01, -1.9592e-01, -4.0216e-01,  ...,  1.5979e-01,\n",
      "             1.5735e-01, -4.0705e-01],\n",
      "           ...,\n",
      "           [ 4.5808e-01,  1.7216e-01,  2.5523e-01,  ...,  5.9792e-01,\n",
      "             1.2849e+00,  9.0543e-02],\n",
      "           [-4.8330e-02,  9.5124e-01,  6.6170e-01,  ...,  8.9410e-01,\n",
      "             3.9379e-01, -3.9527e-02],\n",
      "           [-9.5732e-02,  4.3618e-01,  1.0532e-01,  ...,  5.7644e-01,\n",
      "             7.2485e-01,  1.3661e-02]]]]]), tensor([]), tensor([311.0839]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-9.9650e-01, -9.2656e-01,  7.3299e-01,  ..., -1.1339e+00,\n",
      "            -9.7471e-01, -4.3217e-01],\n",
      "           [ 1.6609e-01,  2.0986e-01,  1.4967e-01,  ..., -2.3259e+00,\n",
      "            -7.8710e-01, -1.5786e-01],\n",
      "           [-1.1962e+00, -1.1154e+00, -8.5940e-01,  ..., -3.1332e-01,\n",
      "             2.0330e+00,  4.6421e-01],\n",
      "           ...,\n",
      "           [ 8.9037e-01,  1.6348e-01,  3.0447e-01,  ..., -1.9276e-01,\n",
      "            -7.5207e-01, -3.5896e-02],\n",
      "           [ 9.2077e-01, -1.3093e+00,  1.1292e+00,  ...,  2.7928e+00,\n",
      "            -4.2054e-01, -6.7152e-02],\n",
      "           [-7.9069e-01, -6.6884e-01, -6.3700e-01,  ...,  1.0963e+00,\n",
      "             3.9618e-01, -1.3785e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.0797e+00,  5.3520e-01, -5.3046e-01,  ...,  5.6776e-01,\n",
      "             5.7978e-02,  6.0728e-01],\n",
      "           [ 3.2252e-02, -7.4012e-01,  1.1954e+00,  ...,  1.7486e-01,\n",
      "            -8.3985e-01, -1.6648e+00],\n",
      "           [-1.8291e-01, -3.9814e-01,  3.5016e-01,  ..., -3.3146e-01,\n",
      "            -2.9467e+00, -4.0881e-01],\n",
      "           ...,\n",
      "           [-1.4000e-01,  1.8204e+00,  1.1116e+00,  ..., -2.0641e-02,\n",
      "            -2.1144e-01,  1.2880e-01],\n",
      "           [ 9.5731e-01,  4.4769e-01, -6.2796e-01,  ..., -9.8785e-01,\n",
      "            -7.4539e-01,  7.7353e-02],\n",
      "           [-3.7294e-01,  4.5320e-01, -2.1202e+00,  ..., -2.7595e-01,\n",
      "             1.3548e+00, -6.7123e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.8226e+00,  1.7297e+00,  2.3700e+00,  ...,  4.3371e-01,\n",
      "             1.0185e+00,  2.0650e+00],\n",
      "           [ 1.2404e+00, -1.9570e-03,  7.9725e-01,  ...,  2.1570e-01,\n",
      "             2.4456e+00, -7.4888e-02],\n",
      "           [ 1.8790e+00,  4.0483e-01,  5.1461e-01,  ...,  1.8723e+00,\n",
      "            -6.3796e-01,  4.2481e-01],\n",
      "           ...,\n",
      "           [-1.0673e+00, -8.8624e-01,  2.5207e+00,  ..., -1.4198e+00,\n",
      "            -1.5407e+00, -8.8508e-01],\n",
      "           [-9.4281e-01, -5.5034e-01,  1.4635e+00,  ...,  1.2429e-01,\n",
      "            -5.0581e-01, -8.1421e-01],\n",
      "           [ 1.1529e+00,  8.3423e-01,  8.8825e-01,  ...,  5.3384e-01,\n",
      "             1.2727e+00, -7.8782e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.1054e+00, -9.3697e-01, -2.9859e-01,  ...,  2.8897e-02,\n",
      "            -4.7244e-01,  6.6845e-01],\n",
      "           [-4.3064e-01, -1.0361e+00, -1.1669e+00,  ...,  5.3450e-01,\n",
      "            -1.9154e+00, -4.6352e-01],\n",
      "           [ 1.6690e+00, -4.4358e-02,  3.3714e-01,  ...,  1.4533e-01,\n",
      "            -4.6966e-01,  6.2784e-01],\n",
      "           ...,\n",
      "           [ 1.3862e+00, -6.6207e-01,  1.7230e+00,  ..., -3.6921e-01,\n",
      "            -6.2779e-01,  1.1226e+00],\n",
      "           [ 4.1569e-01, -1.2052e-01, -2.1016e-01,  ...,  5.5557e-01,\n",
      "             1.2161e+00,  1.6064e+00],\n",
      "           [ 4.7200e-02, -3.2915e-01,  6.1950e-01,  ...,  6.2631e-02,\n",
      "            -9.9721e-01,  6.3944e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.7499e-01,  1.3482e+00, -5.5790e-01,  ..., -4.5699e-01,\n",
      "            -1.4896e+00,  1.4565e-01],\n",
      "           [ 1.0597e+00, -1.7346e+00, -8.4107e-01,  ...,  1.3089e+00,\n",
      "            -1.2020e-01, -1.2047e+00],\n",
      "           [-6.8785e-01,  5.7547e-01,  1.4626e-02,  ..., -7.5600e-01,\n",
      "             1.2440e-01, -3.9612e-01],\n",
      "           ...,\n",
      "           [ 5.1910e-01,  7.5106e-01,  3.6948e-01,  ...,  2.4432e-01,\n",
      "            -8.9509e-02,  9.6421e-02],\n",
      "           [-1.7080e-01, -9.4161e-01, -8.6422e-01,  ..., -1.3185e+00,\n",
      "             4.3786e-01,  6.3477e-01],\n",
      "           [ 1.4257e+00,  2.0784e+00,  1.7366e-01,  ...,  4.1717e-01,\n",
      "            -1.6350e+00, -7.7318e-02]]],\n",
      "\n",
      "\n",
      "         [[[-4.6784e-01,  7.0326e-01,  1.1122e+00,  ...,  9.7456e-01,\n",
      "             9.3691e-01,  6.5124e-02],\n",
      "           [-9.9541e-01,  1.2585e+00, -2.7890e-02,  ..., -3.2109e-01,\n",
      "            -4.1117e-01, -4.1912e-01],\n",
      "           [-4.4750e-01, -4.5245e-01, -1.2288e+00,  ...,  4.9206e-01,\n",
      "             7.9623e-01, -8.9412e-01],\n",
      "           ...,\n",
      "           [-4.2385e-01,  3.3753e-01,  1.3076e-01,  ..., -2.0599e+00,\n",
      "             7.5636e-01, -1.1623e+00],\n",
      "           [-2.6210e+00,  7.7530e-01,  8.2032e-01,  ..., -1.4761e+00,\n",
      "            -1.3562e+00, -2.5541e+00],\n",
      "           [-1.9528e+00, -6.5362e-01, -1.0756e+00,  ..., -1.0359e+00,\n",
      "            -9.3611e-01, -1.7710e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 5.6693e-01,  5.4346e-01,  5.7372e-01,  ...,  5.6557e-01,\n",
      "             4.9402e-01,  4.8976e-01],\n",
      "           [ 5.0465e-01,  3.3528e-01,  3.1114e-01,  ...,  1.1747e-01,\n",
      "             1.3062e-01,  2.5290e-01],\n",
      "           [ 4.4677e-01,  3.1781e-01,  3.2293e-01,  ...,  2.6167e-01,\n",
      "             2.9361e-01,  3.3730e-01],\n",
      "           ...,\n",
      "           [-2.4426e-01,  2.9091e-01,  2.2549e-02,  ..., -3.9914e-01,\n",
      "            -5.4538e-01, -4.8681e-01],\n",
      "           [-1.3624e-01, -1.2891e-01, -1.0952e-02,  ..., -5.3487e-01,\n",
      "            -5.0003e-01, -3.3847e-01],\n",
      "           [ 3.6101e-03, -4.5145e-02, -4.7227e-02,  ..., -3.5287e-01,\n",
      "            -3.5914e-01, -4.0957e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.6683e-01,  7.4587e-01,  3.9398e-01,  ...,  5.8965e-01,\n",
      "             4.6623e-01,  5.0956e-01],\n",
      "           [ 4.5078e-01,  5.7855e-01,  7.1347e-01,  ...,  5.4920e-01,\n",
      "             5.2586e-01,  4.8727e-01],\n",
      "           [ 7.9124e-01,  1.0673e+00,  1.1299e+00,  ...,  7.1256e-01,\n",
      "             5.6397e-01,  4.2323e-01],\n",
      "           ...,\n",
      "           [-2.9436e-01, -2.3702e-02, -8.6959e-02,  ..., -4.9895e-01,\n",
      "             2.3202e-01, -2.2160e-01],\n",
      "           [ 2.0743e-01, -6.5323e-01,  1.5820e-01,  ..., -2.2383e-01,\n",
      "             9.5666e-02, -1.9842e-01],\n",
      "           [-2.1102e-01, -8.0004e-02,  1.0950e-01,  ..., -5.5201e-03,\n",
      "            -9.6243e-02, -4.0067e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.0898e-01, -6.0911e-01, -9.6032e-01,  ..., -9.7857e-01,\n",
      "            -9.7890e-01, -8.6604e-01],\n",
      "           [-5.8638e-01, -9.5211e-01, -1.0396e+00,  ..., -1.0109e+00,\n",
      "            -1.0211e+00, -9.8513e-01],\n",
      "           [-6.9614e-01, -8.7530e-01, -1.0449e+00,  ..., -8.6558e-01,\n",
      "            -7.9635e-01, -7.0084e-01],\n",
      "           ...,\n",
      "           [ 2.6053e-01, -2.7711e-01, -3.5454e-01,  ..., -1.9422e-01,\n",
      "             4.5889e-01,  6.8713e-01],\n",
      "           [ 2.4016e-01, -1.0738e-01,  4.0204e-02,  ...,  1.6437e-01,\n",
      "             3.6169e-01,  8.1340e-01],\n",
      "           [-1.4637e-01, -1.5481e-01, -2.3572e-01,  ...,  1.2535e-01,\n",
      "             2.4049e-01,  4.6247e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.1491e-01,  4.0912e-02, -2.9150e-01,  ..., -2.8531e-01,\n",
      "            -2.1604e-01, -5.8185e-02],\n",
      "           [ 8.3659e-01,  2.3297e-01,  3.3885e-01,  ...,  8.1455e-01,\n",
      "             1.0138e+00,  7.1146e-01],\n",
      "           [-4.1222e-01, -5.6086e-01, -4.4479e-01,  ..., -1.8992e-01,\n",
      "            -3.2996e-01, -1.2226e-01],\n",
      "           ...,\n",
      "           [-4.9227e-01, -3.7238e-01, -3.7593e-01,  ..., -2.0246e-01,\n",
      "            -3.7103e-01, -1.0380e-01],\n",
      "           [-2.8685e-01, -8.2305e-01, -6.4369e-02,  ..., -5.8942e-01,\n",
      "            -4.0001e-01, -7.5836e-01],\n",
      "           [-9.8479e-01, -7.7923e-01, -1.6660e-01,  ..., -5.5717e-01,\n",
      "            -5.8032e-01, -2.6576e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1817e-01,  8.2102e-02, -2.9033e-01,  ..., -2.2540e-01,\n",
      "            -2.5005e-01, -1.6794e-01],\n",
      "           [-8.1968e-02, -2.9043e-01, -2.0190e-01,  ..., -1.2630e-01,\n",
      "            -1.8598e-01, -1.5179e-01],\n",
      "           [-2.2172e-01, -1.6934e-01, -9.7194e-02,  ...,  7.5445e-02,\n",
      "             1.3836e-02, -4.9220e-04],\n",
      "           ...,\n",
      "           [-3.6199e-01, -2.2361e-01, -2.7853e-01,  ..., -5.7057e-01,\n",
      "            -2.6347e-01, -1.9971e-01],\n",
      "           [-1.0722e-01, -4.9247e-01, -1.4043e-01,  ..., -3.2270e-01,\n",
      "            -1.4991e-01, -2.9987e-01],\n",
      "           [-4.7530e-01, -1.3102e-01, -2.5392e-01,  ..., -2.4177e-01,\n",
      "            -1.8470e-01, -4.1252e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.2873e-01, -1.0399e-01, -3.6645e-01,  ..., -3.8558e-01,\n",
      "            -4.9133e-01, -4.1229e-01],\n",
      "           [ 2.1329e-02, -2.8092e-01, -3.3600e-01,  ..., -2.2033e-01,\n",
      "            -2.8548e-01, -2.8280e-01],\n",
      "           [ 1.1036e-01, -2.1539e-02,  5.9226e-02,  ...,  1.2501e-02,\n",
      "            -5.6736e-02, -8.8686e-02],\n",
      "           ...,\n",
      "           [ 6.4617e-01,  2.6006e-02,  2.7535e-01,  ...,  9.2658e-01,\n",
      "             7.8215e-01,  5.5448e-01],\n",
      "           [ 7.4670e-01,  6.5429e-01,  3.5464e-01,  ...,  1.1364e+00,\n",
      "             6.6779e-01,  8.1958e-01],\n",
      "           [ 4.7891e-01,  6.5061e-01,  4.9291e-01,  ...,  8.4298e-01,\n",
      "             9.6061e-01,  4.7702e-01]]]]]), tensor([]), tensor([29.9654]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.9953e+00,  2.4394e-01, -4.2973e-01,  ..., -1.0641e+00,\n",
      "            -9.3340e-01, -1.4676e+00],\n",
      "           [ 4.7144e-01,  8.9797e-01,  6.0358e-01,  ..., -3.1560e-01,\n",
      "            -3.3901e-01,  3.3972e-01],\n",
      "           [ 1.5276e+00, -1.5801e-01, -2.0228e+00,  ..., -5.3358e-01,\n",
      "             8.7768e-01, -7.0820e-01],\n",
      "           ...,\n",
      "           [ 6.2102e-01,  6.9892e-01, -2.5611e-01,  ...,  1.2606e+00,\n",
      "             9.7784e-01,  3.2583e-01],\n",
      "           [-1.2580e+00,  1.1324e+00,  2.4625e-01,  ...,  6.1713e-01,\n",
      "             1.8564e-01, -2.0791e-01],\n",
      "           [ 1.3738e-01,  8.3598e-01,  1.5515e-01,  ...,  3.0297e-01,\n",
      "             3.3160e-01, -4.3745e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.9285e-01, -2.9119e-01, -1.7537e+00,  ..., -5.4982e-01,\n",
      "            -4.7185e-01,  7.4416e-01],\n",
      "           [-1.5141e+00, -7.1110e-01,  2.6605e-01,  ..., -3.9437e-01,\n",
      "            -1.1056e+00, -1.5392e+00],\n",
      "           [-2.0475e+00, -1.7592e+00, -5.3293e-02,  ..., -3.6453e-01,\n",
      "            -5.0281e-01, -9.9982e-01],\n",
      "           ...,\n",
      "           [-5.5751e-01, -4.4089e-02,  6.7737e-01,  ...,  6.1275e-01,\n",
      "            -5.9696e-01, -1.1598e+00],\n",
      "           [ 6.4300e-01, -7.7604e-01, -3.5105e-01,  ...,  8.7264e-01,\n",
      "             5.0155e-01,  1.0982e+00],\n",
      "           [-8.4933e-01,  3.9402e-01, -2.4400e+00,  ..., -5.0972e-01,\n",
      "             3.6475e-01, -3.4958e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1930e+00,  1.1919e+00,  1.8621e+00,  ...,  1.5042e+00,\n",
      "             2.6305e+00,  7.7061e-01],\n",
      "           [ 1.0613e+00,  2.1869e+00,  7.6633e-01,  ...,  7.0827e-01,\n",
      "             1.0306e+00,  9.5714e-01],\n",
      "           [ 2.6903e+00,  6.5062e-01,  1.4224e+00,  ...,  5.8634e-02,\n",
      "             2.0638e+00,  6.7409e-01],\n",
      "           ...,\n",
      "           [ 4.0918e+00, -1.6809e+00, -5.8670e-01,  ...,  5.6845e-01,\n",
      "             8.2796e-01,  6.6471e-01],\n",
      "           [-1.3004e+00, -2.7007e-01,  6.8976e-01,  ..., -1.7492e+00,\n",
      "            -1.5601e+00, -3.3615e-01],\n",
      "           [ 7.5261e-01,  2.8451e+00,  8.7239e-01,  ...,  9.9720e-01,\n",
      "            -4.5158e-01, -7.5331e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.2044e+00, -1.4924e+00, -1.4018e+00,  ..., -1.3794e+00,\n",
      "             4.3236e-01,  1.5048e-01],\n",
      "           [-5.6982e-01, -7.1347e-01, -2.8622e-01,  ..., -6.7988e-01,\n",
      "            -9.0111e-01, -8.4972e-01],\n",
      "           [ 2.5001e-01,  7.9141e-02,  1.1312e-03,  ...,  1.8017e+00,\n",
      "             8.6998e-01,  6.4912e-01],\n",
      "           ...,\n",
      "           [-9.0995e-01,  4.2249e-01, -1.9877e-01,  ...,  6.0664e-01,\n",
      "            -4.6423e-01, -5.1159e-02],\n",
      "           [ 1.4254e+00,  1.0981e+00,  1.0290e+00,  ...,  2.5796e-01,\n",
      "             5.2037e-01, -4.8877e-01],\n",
      "           [ 6.2480e-01,  8.8060e-01, -6.2125e-01,  ..., -1.2542e+00,\n",
      "             1.3773e+00,  7.7524e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.3681e+00,  9.2401e-01, -1.7122e-02,  ..., -3.5749e-01,\n",
      "            -1.2467e+00,  3.4481e-01],\n",
      "           [ 5.9490e-01, -1.2437e+00,  5.5683e-01,  ..., -1.9657e-01,\n",
      "            -2.8237e-01,  2.9887e-01],\n",
      "           [-1.2970e-01,  6.3332e-01,  5.7431e-01,  ..., -1.5271e+00,\n",
      "            -1.1195e-01,  1.0543e+00],\n",
      "           ...,\n",
      "           [ 4.1988e-01, -3.6597e-02, -8.2535e-01,  ...,  6.6344e-01,\n",
      "            -1.5647e+00,  6.7080e-01],\n",
      "           [-1.0787e+00,  5.1242e-01, -1.9017e+00,  ...,  1.0716e+00,\n",
      "             3.3349e-01,  1.3211e-01],\n",
      "           [-4.2674e-01,  1.0932e+00, -1.9393e+00,  ..., -1.1842e-02,\n",
      "            -1.0233e+00,  2.1237e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 6.6271e-01, -4.8756e-01,  3.0837e-01,  ..., -7.9313e-01,\n",
      "            -8.9347e-01, -1.2592e+00],\n",
      "           [ 1.8301e+00,  1.4063e+00,  2.5835e-02,  ...,  1.3100e-01,\n",
      "            -1.8376e-01,  1.2331e-01],\n",
      "           [ 1.5204e-01, -9.2602e-01,  8.2977e-01,  ..., -1.3725e-01,\n",
      "             7.3746e-01,  5.8622e-01],\n",
      "           ...,\n",
      "           [ 1.9786e+00, -1.4800e+00,  2.8694e+00,  ..., -1.1745e+00,\n",
      "            -1.1488e+00,  2.3898e-01],\n",
      "           [-6.2188e-01, -7.3708e-01, -6.1829e-01,  ..., -2.6434e+00,\n",
      "            -2.5437e-01,  3.0102e-01],\n",
      "           [-2.5970e-01,  6.0188e-01,  1.5893e+00,  ..., -3.2728e-01,\n",
      "            -1.6533e+00, -7.7793e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 5.7160e-01,  1.2746e+00,  7.3342e-01,  ...,  4.4633e-02,\n",
      "             2.1826e-01,  2.1704e-01],\n",
      "           [-9.3457e-02,  5.9687e-01,  2.8649e-01,  ...,  2.5149e-01,\n",
      "             1.3546e-01,  3.8205e-02],\n",
      "           [ 5.3629e-01,  2.1462e-01,  4.4179e-01,  ...,  1.2623e-01,\n",
      "            -2.1878e-01,  3.5345e-01],\n",
      "           ...,\n",
      "           [-8.4643e-02,  2.0462e-01, -7.3198e-01,  ..., -2.7521e-01,\n",
      "            -1.1361e+00, -6.6920e-01],\n",
      "           [-3.9796e-01, -3.0683e-01, -7.0923e-02,  ..., -1.0397e+00,\n",
      "            -7.2700e-01, -1.4718e-01],\n",
      "           [ 1.2476e-02, -4.9122e-01,  5.2396e-02,  ..., -1.0795e+00,\n",
      "            -4.0220e-02, -9.4532e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0017e-01,  8.4174e-01, -2.3077e-01,  ...,  7.2557e-01,\n",
      "             5.2396e-01,  9.1922e-01],\n",
      "           [ 1.0824e-03, -2.0058e-01,  4.7750e-01,  ..., -1.2689e-01,\n",
      "             7.0075e-02,  7.7030e-01],\n",
      "           [ 5.5489e-01,  6.1120e-01,  4.5800e-01,  ...,  8.4163e-01,\n",
      "             1.9810e-01, -4.0912e-02],\n",
      "           ...,\n",
      "           [-5.0877e-01, -3.1922e-01, -7.7004e-01,  ..., -2.7884e-01,\n",
      "             9.9975e-01, -1.9122e-01],\n",
      "           [-5.1180e-01,  1.1392e-01, -4.0117e-01,  ..., -3.1427e-01,\n",
      "            -5.8286e-02, -5.1187e-01],\n",
      "           [-2.6601e-01, -7.5670e-01,  5.0218e-01,  ..., -3.0033e-01,\n",
      "             2.2107e-01,  3.1382e-02]]],\n",
      "\n",
      "\n",
      "         [[[-9.1960e-01, -4.0470e-01, -4.5651e-01,  ..., -2.1743e-01,\n",
      "            -9.1986e-01, -3.0097e-01],\n",
      "           [ 4.6341e-02, -8.5461e-01, -4.0664e-01,  ..., -4.3588e-01,\n",
      "            -5.9523e-01, -1.0526e+00],\n",
      "           [-3.0659e-01, -1.1217e+00, -3.1642e-01,  ..., -1.2878e+00,\n",
      "            -8.8260e-01, -7.2833e-01],\n",
      "           ...,\n",
      "           [-3.3888e-01, -5.2685e-02, -6.9298e-02,  ..., -6.8500e-01,\n",
      "             5.1656e-01,  6.7106e-01],\n",
      "           [ 2.6733e-01,  4.4976e-01, -1.1435e-01,  ...,  2.4917e-01,\n",
      "             3.5232e-01,  7.6443e-02],\n",
      "           [ 1.3607e-01, -2.4754e-01,  1.0709e-01,  ..., -3.3045e-01,\n",
      "             5.2494e-01, -8.8046e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 2.6179e-01,  6.1161e-01, -4.7690e-01,  ..., -6.6117e-01,\n",
      "             2.5827e-05, -2.1720e-03],\n",
      "           [ 2.8207e-01, -8.8386e-02,  1.3495e-01,  ...,  2.0606e-01,\n",
      "             5.3161e-01,  1.0895e+00],\n",
      "           [-1.5438e-01, -4.2409e-01,  5.4646e-01,  ..., -1.8556e-01,\n",
      "            -7.7521e-01, -6.1890e-01],\n",
      "           ...,\n",
      "           [-4.7911e-01,  2.2534e-01, -2.4593e-01,  ...,  4.3910e-01,\n",
      "            -4.3934e-01, -6.9904e-01],\n",
      "           [ 4.1715e-01, -8.8912e-01,  2.3376e-01,  ..., -3.0214e-01,\n",
      "            -3.2496e-01, -1.5517e-01],\n",
      "           [ 1.4200e-01, -5.1125e-01, -1.2291e-02,  ..., -2.7333e-01,\n",
      "            -4.8746e-01, -3.8152e-01]]],\n",
      "\n",
      "\n",
      "         [[[-8.3877e-02, -6.5908e-01, -1.0511e+00,  ..., -1.2678e-01,\n",
      "            -4.0202e-01, -5.3322e-01],\n",
      "           [ 2.4841e-01,  3.1506e-01, -3.0588e-01,  ..., -3.2169e-01,\n",
      "            -5.5577e-01, -4.8347e-01],\n",
      "           [-3.8142e-01, -6.0144e-01,  2.9843e-01,  ...,  5.7625e-01,\n",
      "            -1.8893e-02,  2.1574e-01],\n",
      "           ...,\n",
      "           [ 4.4752e-01, -4.7178e-01, -4.5446e-02,  ..., -1.2704e-01,\n",
      "            -2.9408e-01, -9.3416e-01],\n",
      "           [-1.9654e-01, -4.5920e-01, -6.5177e-02,  ...,  1.2863e-01,\n",
      "            -4.3239e-01,  3.6705e-01],\n",
      "           [-2.6119e-01,  5.0616e-02, -8.0771e-01,  ..., -6.4700e-03,\n",
      "            -1.3530e-01,  1.1542e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.8716e-01,  5.0026e-01, -5.8884e-01,  ...,  3.7654e-01,\n",
      "            -5.7150e-01, -2.5344e-01],\n",
      "           [ 2.4768e-01,  6.9616e-01,  6.3588e-01,  ..., -7.9916e-02,\n",
      "             2.0279e-01,  4.6805e-01],\n",
      "           [ 2.3531e-01, -2.0943e-01,  2.4450e-01,  ...,  2.9244e-02,\n",
      "             8.1411e-02, -3.5698e-02],\n",
      "           ...,\n",
      "           [-4.2887e-02,  7.4043e-01, -1.9613e-01,  ..., -1.3650e-01,\n",
      "             1.1319e+00,  6.1702e-01],\n",
      "           [ 8.5932e-01,  6.6241e-01,  9.4286e-02,  ...,  2.7136e-01,\n",
      "             4.1021e-02,  5.4891e-01],\n",
      "           [-1.2445e-01, -7.9611e-02,  4.6742e-01,  ...,  1.8589e-01,\n",
      "             3.4416e-01, -5.5853e-01]]]]]), tensor([]), tensor([432.1060]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 1.4270e-01,  1.7462e+00,  2.8304e-01,  ..., -1.2138e+00,\n",
      "            -8.8182e-01, -7.1296e-01],\n",
      "           [-1.2367e+00,  6.2736e-01, -1.8822e-01,  ...,  1.6246e-01,\n",
      "             6.2011e-02, -4.1742e-01],\n",
      "           [ 2.8380e-01, -4.3137e-01,  1.5142e-01,  ..., -5.3308e-01,\n",
      "            -1.1145e+00, -2.9947e-02],\n",
      "           ...,\n",
      "           [ 4.4293e-01, -1.5913e-01, -1.7213e+00,  ...,  3.4660e-01,\n",
      "            -1.2570e+00, -4.8869e-01],\n",
      "           [-5.9766e-01, -2.7894e-01, -1.5575e-01,  ..., -1.1892e+00,\n",
      "            -5.1778e-01,  9.0159e-01],\n",
      "           [ 2.9912e-03, -1.0427e+00,  2.4892e-01,  ..., -1.4191e+00,\n",
      "             9.1664e-01,  5.5260e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.0932e+00,  3.4663e-01, -1.9780e+00,  ...,  4.8684e-02,\n",
      "             3.3288e-02,  9.0097e-01],\n",
      "           [-1.1816e+00, -1.6621e+00, -4.5573e-01,  ..., -1.7713e+00,\n",
      "            -1.1486e+00,  6.9579e-01],\n",
      "           [-6.3453e-01, -1.0925e+00, -1.6554e+00,  ...,  4.0699e-01,\n",
      "            -7.7543e-01, -1.0934e+00],\n",
      "           ...,\n",
      "           [-5.0848e-01, -6.7535e-01, -1.5752e+00,  ...,  3.2982e-01,\n",
      "             1.8100e+00,  5.2912e-01],\n",
      "           [-1.6404e+00,  1.5629e+00, -1.5290e+00,  ..., -9.8582e-01,\n",
      "            -9.0161e-01, -1.3329e+00],\n",
      "           [-1.1731e-01, -1.4867e+00,  7.0311e-01,  ..., -6.8765e-01,\n",
      "             2.6793e-02, -3.2880e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.0954e-01,  5.3005e-01,  1.3874e+00,  ...,  1.4833e+00,\n",
      "             3.5389e-01,  1.3536e+00],\n",
      "           [ 1.4444e+00,  4.0998e-01,  1.5259e+00,  ...,  1.1805e+00,\n",
      "             1.0233e+00, -4.4112e-01],\n",
      "           [ 1.1584e+00, -4.7638e-01,  1.5373e+00,  ..., -1.0285e+00,\n",
      "             3.2146e-02,  4.6374e-02],\n",
      "           ...,\n",
      "           [-1.1053e+00,  4.2101e-01,  6.8085e-01,  ..., -1.0615e+00,\n",
      "            -2.2231e-02,  1.4130e-01],\n",
      "           [-2.8311e-03,  1.2780e+00, -3.2965e-01,  ...,  4.3089e-01,\n",
      "             4.2677e-01, -6.4175e-01],\n",
      "           [ 6.7549e-01,  4.0862e-02,  8.5827e-01,  ..., -2.4029e+00,\n",
      "             1.2348e+00,  3.1880e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 5.2007e-02,  1.2552e+00, -4.1859e-01,  ..., -1.1835e+00,\n",
      "            -2.0474e-01, -2.9302e-01],\n",
      "           [-1.3543e+00, -1.1876e+00, -8.5937e-01,  ..., -1.1938e+00,\n",
      "            -4.1337e-02,  1.6667e+00],\n",
      "           [ 5.3103e-01, -7.6171e-02,  1.9461e+00,  ...,  6.9347e-02,\n",
      "            -8.0053e-01, -1.0281e+00],\n",
      "           ...,\n",
      "           [-1.2278e-01,  1.5408e+00,  3.2363e-01,  ...,  1.0690e+00,\n",
      "            -1.2290e+00, -9.3630e-01],\n",
      "           [ 1.7191e+00, -1.1263e-01,  7.6863e-01,  ..., -2.0419e-01,\n",
      "            -1.9470e-01, -1.2786e-02],\n",
      "           [ 2.8204e+00,  9.1371e-01,  2.1382e-01,  ...,  9.5210e-01,\n",
      "            -1.2567e-01, -5.0066e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.0327e-02, -1.7181e+00, -2.0829e+00,  ...,  1.1886e-02,\n",
      "            -3.5029e-01, -6.9169e-01],\n",
      "           [ 9.7453e-01,  1.2280e+00, -2.7659e-01,  ..., -4.7537e-01,\n",
      "            -6.8484e-01, -6.9327e-01],\n",
      "           [-4.6658e-01, -9.2034e-01,  1.0078e+00,  ...,  1.0822e+00,\n",
      "            -1.5999e-03,  5.1626e-01],\n",
      "           ...,\n",
      "           [ 1.9204e+00, -6.2071e-01,  4.4474e-01,  ...,  7.0560e-01,\n",
      "            -6.1429e-01, -1.7826e+00],\n",
      "           [-3.1510e-01,  8.0328e-02, -4.0832e-02,  ...,  8.5440e-01,\n",
      "            -7.9851e-01,  1.2681e+00],\n",
      "           [ 3.6001e-01,  3.9864e-01, -1.3116e+00,  ...,  1.9999e-01,\n",
      "             3.0467e-01,  7.6390e-01]]],\n",
      "\n",
      "\n",
      "         [[[-5.4786e-01,  1.1857e+00, -5.2207e-01,  ...,  1.4609e+00,\n",
      "            -4.1699e-01,  2.9333e-01],\n",
      "           [ 4.2591e-01,  2.3423e+00,  2.1495e+00,  ...,  7.7535e-02,\n",
      "             8.6192e-01,  1.6055e+00],\n",
      "           [ 2.1225e-01, -5.8218e-01,  2.5452e-01,  ...,  4.0552e-01,\n",
      "             2.7169e-01, -2.2536e-02],\n",
      "           ...,\n",
      "           [-1.4625e+00,  1.5414e+00, -8.7614e-01,  ..., -4.5932e-01,\n",
      "             3.3422e-01, -6.5078e-01],\n",
      "           [ 1.6292e-01,  2.8577e-02, -6.6556e-01,  ..., -5.6480e-01,\n",
      "            -1.7968e+00, -7.9086e-01],\n",
      "           [-1.4392e+00, -1.7745e+00,  9.4605e-02,  ..., -1.4467e+00,\n",
      "            -5.9077e-01, -1.4942e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 4.1044e-01,  6.9457e-01,  6.3779e-01,  ...,  4.5160e-01,\n",
      "             5.2565e-01,  3.4711e-01],\n",
      "           [-1.1634e-02,  9.1497e-02, -9.3102e-02,  ..., -1.4993e-01,\n",
      "            -5.8610e-01,  3.4974e-01],\n",
      "           [ 4.3652e-01,  4.6667e-01,  2.0307e-01,  ...,  2.1085e-01,\n",
      "            -3.6608e-03,  1.2823e-01],\n",
      "           ...,\n",
      "           [ 9.7442e-03,  1.6492e-01,  1.7008e-01,  ..., -4.0983e-01,\n",
      "            -4.5811e-01, -6.2938e-01],\n",
      "           [ 1.5543e-01,  9.5031e-02,  9.9498e-02,  ..., -2.0272e-01,\n",
      "            -8.1333e-01, -3.9859e-01],\n",
      "           [-3.0654e-02, -3.8116e-02, -3.7287e-01,  ..., -3.3276e-01,\n",
      "            -5.1783e-01, -2.4468e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 4.5624e-01,  4.4311e-01,  5.9742e-01,  ...,  7.2250e-01,\n",
      "             7.0594e-01,  3.5743e-01],\n",
      "           [ 5.6368e-01,  2.4808e-01,  3.2266e-01,  ...,  4.6242e-01,\n",
      "             7.0170e-01,  3.1996e-01],\n",
      "           [ 8.8846e-01,  1.0343e+00,  7.3830e-01,  ...,  4.1137e-01,\n",
      "             7.1371e-01,  1.0907e-01],\n",
      "           ...,\n",
      "           [-5.8276e-01,  1.5767e-01,  5.5788e-02,  ..., -3.1980e-01,\n",
      "             2.7402e-01, -3.4851e-01],\n",
      "           [ 1.7713e-01, -1.5608e-01,  9.9408e-02,  ..., -2.6366e-01,\n",
      "             9.4530e-02, -3.7267e-01],\n",
      "           [-1.4158e-01,  8.6142e-02, -3.9664e-02,  ...,  2.5482e-01,\n",
      "             3.5359e-01,  4.0055e-01]]],\n",
      "\n",
      "\n",
      "         [[[-3.7606e-01, -2.7914e-01, -9.3050e-01,  ..., -7.8989e-01,\n",
      "            -8.7698e-01, -1.3150e+00],\n",
      "           [-6.7588e-01, -9.4512e-01, -1.1438e+00,  ..., -7.0690e-01,\n",
      "            -9.4593e-01, -7.6548e-01],\n",
      "           [-6.8910e-01, -1.0388e+00, -7.7053e-01,  ..., -5.6059e-01,\n",
      "            -5.9895e-01, -5.2694e-01],\n",
      "           ...,\n",
      "           [-5.7816e-02,  8.9461e-02, -2.0642e-01,  ...,  3.5052e-01,\n",
      "             5.6647e-01,  4.8828e-01],\n",
      "           [ 1.7406e-01, -3.8043e-01, -4.2489e-02,  ..., -4.8079e-02,\n",
      "             4.1845e-01,  2.3006e-01],\n",
      "           [-1.0276e-01, -3.0761e-03,  8.9596e-02,  ...,  6.4742e-01,\n",
      "             3.3377e-01, -1.2300e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-2.0671e-02,  1.3114e-01, -2.5659e-01,  ..., -6.3608e-02,\n",
      "             3.6280e-01,  2.5449e-01],\n",
      "           [ 5.5515e-01, -4.7980e-02,  1.4795e-01,  ...,  4.4748e-01,\n",
      "             7.1842e-01,  3.9367e-01],\n",
      "           [-6.5563e-02, -6.2710e-01, -7.4417e-02,  ..., -2.7854e-01,\n",
      "             1.9159e-01, -1.8983e-01],\n",
      "           ...,\n",
      "           [-4.1740e-01, -5.4829e-01, -2.9775e-01,  ...,  1.1290e-01,\n",
      "             1.7431e-02, -4.7777e-01],\n",
      "           [-2.1011e-01, -6.0315e-01, -1.0785e-01,  ...,  3.4184e-01,\n",
      "            -1.0446e-01, -5.8119e-01],\n",
      "           [-6.7370e-01, -6.5491e-01, -7.2008e-02,  ...,  7.9687e-02,\n",
      "            -3.3872e-01, -3.1257e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.3706e-01,  1.4232e-02, -2.3607e-02,  ..., -1.6773e-01,\n",
      "            -3.0425e-01, -4.5323e-01],\n",
      "           [-1.2887e-02, -1.3594e-01, -1.0653e-01,  ...,  3.8218e-01,\n",
      "            -1.8505e-01, -4.1036e-01],\n",
      "           [-4.1020e-01, -2.3950e-01, -2.7112e-01,  ...,  3.2960e-01,\n",
      "            -4.5241e-02,  1.3766e-01],\n",
      "           ...,\n",
      "           [-5.8336e-01, -3.1233e-01,  6.3709e-02,  ..., -5.8358e-01,\n",
      "             2.5065e-01, -4.8089e-01],\n",
      "           [ 3.6636e-01, -1.2009e-02, -1.3762e-01,  ...,  1.8170e-01,\n",
      "             2.3291e-01, -2.7088e-01],\n",
      "           [-1.7796e-01, -1.8925e-01, -4.1414e-01,  ..., -8.9215e-02,\n",
      "            -2.2613e-01, -4.5789e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.4733e-01,  1.0964e-01, -3.3554e-01,  ..., -3.8639e-01,\n",
      "            -3.9403e-01, -1.6164e-01],\n",
      "           [-2.8092e-04, -3.3672e-01, -2.3577e-01,  ...,  2.0426e-01,\n",
      "            -1.1325e-01,  4.1012e-02],\n",
      "           [ 1.3032e-01,  1.7084e-01, -2.8211e-01,  ...,  1.2461e-01,\n",
      "            -9.7040e-02, -2.2323e-01],\n",
      "           ...,\n",
      "           [ 9.1153e-02, -1.6312e-01,  8.3991e-02,  ...,  6.2293e-01,\n",
      "             9.8206e-01,  2.5046e-01],\n",
      "           [ 5.8385e-01,  3.0893e-01,  5.7345e-01,  ...,  4.0487e-01,\n",
      "             8.3557e-01,  8.7823e-01],\n",
      "           [ 9.6831e-02,  4.1170e-01,  9.2316e-02,  ...,  5.0184e-01,\n",
      "             4.8135e-01, -1.4466e-02]]]]]), tensor([]), tensor([198.4497]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-5.6442e-01,  1.0244e+00,  1.2343e-01,  ..., -8.0675e-01,\n",
      "            -6.4877e-01, -1.2215e+00],\n",
      "           [-2.3967e+00, -1.1036e+00, -1.6584e+00,  ..., -1.3763e+00,\n",
      "            -3.7341e+00,  3.2651e-01],\n",
      "           [ 1.4919e-01,  5.6055e-01, -1.1916e+00,  ..., -4.4615e-01,\n",
      "            -1.4786e+00, -7.4243e-01],\n",
      "           ...,\n",
      "           [ 1.4421e+00, -6.0139e-01,  8.5896e-01,  ...,  1.9701e-01,\n",
      "             5.9672e-01, -4.6320e-01],\n",
      "           [ 1.3362e+00,  1.0881e+00,  5.1616e-01,  ...,  1.4332e+00,\n",
      "            -1.3580e+00,  3.0138e-01],\n",
      "           [-1.5499e-01,  1.9560e-01, -1.5252e+00,  ...,  4.7627e-01,\n",
      "            -4.0008e-01,  3.4383e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.7679e-01, -9.9574e-01, -4.7641e-01,  ...,  8.0067e-02,\n",
      "             2.0283e-01, -1.1124e+00],\n",
      "           [-1.2653e-02, -1.4130e+00, -1.2874e+00,  ..., -9.2811e-01,\n",
      "             6.6084e-01, -9.7635e-01],\n",
      "           [ 2.1862e-01,  1.4795e-02, -1.2386e+00,  ..., -1.2634e+00,\n",
      "             8.5815e-01, -1.4790e+00],\n",
      "           ...,\n",
      "           [-1.1841e+00,  7.6472e-01,  6.7837e-01,  ...,  1.0997e+00,\n",
      "             1.8724e+00,  7.2628e-02],\n",
      "           [-1.2000e-01,  2.2837e+00, -6.0453e-01,  ..., -3.7431e-01,\n",
      "             3.3705e-01, -1.1502e+00],\n",
      "           [ 5.9960e-01,  8.0815e-01, -1.1647e+00,  ...,  1.4181e+00,\n",
      "             1.1856e+00, -4.8401e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.0011e+00,  1.8557e+00,  8.2200e-01,  ...,  1.0403e+00,\n",
      "             9.9557e-01, -1.7997e+00],\n",
      "           [-3.8920e-01,  4.9987e-01, -8.1964e-01,  ...,  1.9419e+00,\n",
      "             8.3667e-01,  1.1483e+00],\n",
      "           [ 5.1449e-01, -4.6465e-01,  1.2651e+00,  ...,  1.4387e+00,\n",
      "             1.5764e+00,  1.1301e+00],\n",
      "           ...,\n",
      "           [-9.4095e-01,  1.5336e+00,  7.9889e-01,  ..., -7.9080e-04,\n",
      "            -6.7249e-01, -1.4785e+00],\n",
      "           [-5.6938e-01, -1.3938e+00, -2.5774e-01,  ..., -1.1501e+00,\n",
      "            -4.7511e-01, -2.9493e+00],\n",
      "           [ 3.0243e-01,  1.2562e+00,  1.6357e+00,  ...,  7.5320e-02,\n",
      "             9.3690e-01, -1.5894e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-5.7096e-01,  3.9687e-01, -1.1698e+00,  ..., -1.5053e-01,\n",
      "             1.1737e+00,  1.5169e-01],\n",
      "           [-1.1807e+00, -2.3968e+00, -9.6518e-01,  ..., -1.3445e+00,\n",
      "            -5.3212e-01, -9.7514e-01],\n",
      "           [ 1.7849e+00,  6.6899e-02,  9.7124e-01,  ..., -1.5740e-01,\n",
      "             2.4086e+00,  2.6294e-01],\n",
      "           ...,\n",
      "           [ 4.3770e-01, -3.6610e-01,  4.4872e-01,  ...,  2.0943e+00,\n",
      "             1.1807e+00,  3.3338e-01],\n",
      "           [ 1.1366e+00,  1.3027e+00, -2.5788e-01,  ...,  6.1665e-01,\n",
      "             6.2858e-01,  8.0882e-01],\n",
      "           [ 2.1153e+00,  1.0958e+00,  4.7422e-01,  ...,  1.0632e+00,\n",
      "            -2.3883e-01, -9.2860e-01]]],\n",
      "\n",
      "\n",
      "         [[[-6.8469e-01, -4.5848e-01,  5.3316e-01,  ...,  2.4463e-01,\n",
      "            -3.8973e-01, -1.1331e+00],\n",
      "           [ 2.9184e-01,  2.7781e-01,  3.8977e-01,  ...,  2.2929e+00,\n",
      "             3.7513e-01, -7.3687e-01],\n",
      "           [-1.0213e+00, -2.1872e-01, -1.1836e+00,  ...,  1.1406e+00,\n",
      "            -5.0480e-01,  6.6863e-01],\n",
      "           ...,\n",
      "           [-1.0662e+00, -4.3738e-01,  1.4019e+00,  ..., -1.7371e-01,\n",
      "             1.9296e+00, -1.3115e+00],\n",
      "           [ 2.2192e+00,  2.5186e+00, -3.2589e-01,  ...,  2.5941e+00,\n",
      "             2.3650e+00, -9.7201e-02],\n",
      "           [ 1.3086e+00, -2.7983e-01, -9.2834e-01,  ...,  6.6559e-01,\n",
      "             6.6371e-01, -6.1220e-01]]],\n",
      "\n",
      "\n",
      "         [[[-4.6447e-01,  8.5496e-01,  1.3165e-01,  ..., -4.1445e-01,\n",
      "            -1.6098e-01,  1.0256e+00],\n",
      "           [-3.5692e-01, -3.6216e-01, -7.6261e-03,  ...,  1.8529e+00,\n",
      "             5.8315e-01,  1.3806e+00],\n",
      "           [ 1.7357e-01,  4.6588e-01, -1.8520e+00,  ...,  6.9635e-01,\n",
      "            -1.9463e-01, -5.0017e-01],\n",
      "           ...,\n",
      "           [-2.6225e+00, -1.0848e+00, -5.2991e-01,  ..., -7.8796e-01,\n",
      "            -2.9977e-01, -1.8211e+00],\n",
      "           [-8.1963e-01, -1.7392e+00,  9.9756e-01,  ..., -2.3959e+00,\n",
      "            -1.0660e+00,  1.8332e-01],\n",
      "           [-1.8902e+00, -1.1840e+00, -1.6655e+00,  ..., -6.8806e-01,\n",
      "            -1.3988e+00, -1.0783e+00]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 0.2097, -0.4810,  1.1945,  ..., -0.0430,  0.2159, -0.1236],\n",
      "           [ 1.2594,  0.8961, -0.2946,  ...,  0.2963,  1.0165,  0.3755],\n",
      "           [ 0.0929,  0.5572,  0.3307,  ..., -0.3861,  0.7832, -0.1137],\n",
      "           ...,\n",
      "           [ 0.3281,  0.2193, -0.2089,  ..., -0.8952,  0.0702,  0.0983],\n",
      "           [-0.4566, -0.8383, -0.4604,  ..., -0.4059, -0.0819, -0.0104],\n",
      "           [-0.8747,  0.5368, -0.3627,  ..., -0.3431, -0.0216,  0.1629]]],\n",
      "\n",
      "\n",
      "         [[[-0.0803,  0.8558,  0.4100,  ...,  0.2120, -0.1357,  0.8039],\n",
      "           [ 0.0391,  0.2403,  0.3683,  ...,  0.6138,  0.2834,  0.1684],\n",
      "           [ 0.9513,  0.0046,  1.7332,  ...,  1.4704,  0.7696, -0.5443],\n",
      "           ...,\n",
      "           [-0.1244, -0.1214, -0.2093,  ..., -0.7064, -0.6226, -0.9205],\n",
      "           [ 0.8416, -0.5348, -0.5015,  ...,  0.3839,  0.0900,  0.4365],\n",
      "           [ 0.2075,  0.2326,  0.4630,  ...,  0.4803,  0.1892, -0.0202]]],\n",
      "\n",
      "\n",
      "         [[[-0.4345, -0.3590, -0.3967,  ..., -0.2477, -0.7893, -0.6251],\n",
      "           [ 0.2015,  0.2875, -0.4004,  ..., -0.5170, -0.7702, -0.6103],\n",
      "           [-0.2037, -0.8737,  0.3926,  ..., -0.7450, -0.1247, -0.0491],\n",
      "           ...,\n",
      "           [-0.3321,  0.0939, -0.0085,  ..., -0.4226,  1.0238, -0.0071],\n",
      "           [-0.0109, -0.3059, -0.6894,  ..., -0.1435,  0.5669,  0.4432],\n",
      "           [-0.4077,  0.3079, -0.5404,  ...,  1.1927,  0.0860,  0.2363]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.3177, -0.0801,  0.3672,  ..., -0.3918, -0.7545, -0.1359],\n",
      "           [-0.1763,  0.1578,  1.3213,  ...,  0.8733,  0.8392,  0.5785],\n",
      "           [-0.3704, -1.0743,  0.4070,  ...,  0.0185,  0.6386,  0.0840],\n",
      "           ...,\n",
      "           [ 1.1890, -0.5930,  0.1023,  ..., -0.3555,  1.1366, -0.5522],\n",
      "           [-0.4684, -0.0507, -0.5024,  ..., -0.2005, -0.4909,  0.8368],\n",
      "           [-0.8309, -0.3117, -0.1512,  ..., -0.7192,  0.0024, -0.6293]]],\n",
      "\n",
      "\n",
      "         [[[-1.1574,  0.1870, -0.4252,  ..., -0.2467,  0.3360, -0.6522],\n",
      "           [ 0.4286,  1.1198, -0.6880,  ...,  0.0321,  0.1698,  0.3585],\n",
      "           [ 0.4483, -0.3365,  0.1609,  ...,  0.1286, -0.4734, -0.1882],\n",
      "           ...,\n",
      "           [ 0.1274, -0.6973,  0.5565,  ...,  0.3345, -0.0030,  0.0815],\n",
      "           [-0.5858, -0.9384, -0.0322,  ...,  1.1541, -0.1207,  0.4914],\n",
      "           [ 0.0597, -0.1006,  0.4000,  ..., -0.9107,  0.1474,  0.0540]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3044, -0.0710,  0.3195,  ...,  0.1656, -0.4528, -0.3422],\n",
      "           [ 0.0680,  0.0780, -0.2241,  ...,  0.4946, -1.0318,  0.3098],\n",
      "           [-0.1596, -0.5126,  0.1499,  ...,  0.3102, -0.4549, -0.2518],\n",
      "           ...,\n",
      "           [ 0.2600, -0.6992, -0.0272,  ..., -0.9212,  0.6993,  0.6115],\n",
      "           [ 0.7512,  0.6740, -0.4581,  ..., -0.0683, -0.4714,  0.3458],\n",
      "           [ 0.3358,  0.5840,  0.4100,  ...,  0.6157,  0.4660,  0.0087]]]]]), tensor([]), tensor([492.9206]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-5.8637e-01, -1.9601e+00,  1.2825e+00,  ..., -1.3198e+00,\n",
      "            -5.5833e-01, -1.1775e+00],\n",
      "           [ 1.6642e+00,  1.2630e+00, -1.1578e+00,  ...,  2.8367e-01,\n",
      "             1.7556e+00,  2.5738e-01],\n",
      "           [-5.8343e-01,  4.2005e-01, -1.9515e-01,  ..., -1.4600e+00,\n",
      "             9.2465e-01, -9.5727e-01],\n",
      "           ...,\n",
      "           [ 1.2125e+00, -7.1533e-02, -3.5052e-01,  ..., -1.0233e+00,\n",
      "             1.1573e+00,  1.2107e+00],\n",
      "           [-6.9102e-01, -1.4271e+00, -9.0994e-01,  ...,  3.7552e-01,\n",
      "             8.3813e-01,  8.1701e-01],\n",
      "           [-1.8603e+00,  1.2199e+00, -6.1830e-01,  ...,  3.6943e-01,\n",
      "             9.8410e-01,  8.6127e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.4555e+00,  1.7706e-01, -2.2636e-01,  ..., -1.0023e+00,\n",
      "            -1.5154e+00,  6.9067e-01],\n",
      "           [-1.2011e+00, -6.6194e-01, -5.6877e-01,  ..., -7.7315e-02,\n",
      "            -3.9523e-01, -4.5157e-01],\n",
      "           [-4.6358e-02, -2.1715e+00,  1.1509e+00,  ...,  1.7882e+00,\n",
      "             3.5907e-01, -2.2130e+00],\n",
      "           ...,\n",
      "           [ 2.0854e-01, -2.8798e-01, -2.1190e-01,  ..., -1.3457e+00,\n",
      "            -1.1001e+00, -1.6076e+00],\n",
      "           [ 1.2582e+00,  1.2172e-01, -1.4411e+00,  ...,  5.3934e-01,\n",
      "            -4.5670e-01,  4.1828e-01],\n",
      "           [ 8.8395e-01,  7.9706e-01,  5.2014e-01,  ...,  1.5263e+00,\n",
      "             3.9949e-01, -2.9225e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 7.1544e-01,  6.5658e-01,  1.2072e+00,  ...,  1.5974e+00,\n",
      "             4.8872e-01,  6.4300e-01],\n",
      "           [ 1.6725e+00,  2.6734e+00,  1.4142e+00,  ...,  1.0054e+00,\n",
      "             5.4507e-01,  7.5237e-01],\n",
      "           [ 1.2600e+00,  5.2146e-02,  2.8486e+00,  ...,  2.1311e-01,\n",
      "             1.5591e+00,  1.2964e+00],\n",
      "           ...,\n",
      "           [-9.7500e-01,  6.1767e-01,  6.5338e-01,  ..., -5.9263e-01,\n",
      "             1.7639e+00, -1.0068e+00],\n",
      "           [-5.4871e-01, -4.4629e-01, -1.4126e+00,  ..., -6.7115e-01,\n",
      "             8.1831e-02,  3.9209e-01],\n",
      "           [-4.7365e-01,  1.2003e+00, -6.2779e-01,  ...,  1.5964e+00,\n",
      "            -4.6036e-01,  4.3287e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.3143e-01, -7.2818e-01,  1.3289e+00,  ..., -1.6345e-01,\n",
      "            -1.5456e+00, -1.8780e-01],\n",
      "           [-2.1826e+00, -7.8004e-01,  1.3392e+00,  ...,  8.0474e-01,\n",
      "            -7.2517e-02,  2.6773e-02],\n",
      "           [ 1.6287e-01, -7.0993e-01,  1.5885e+00,  ...,  3.6863e-01,\n",
      "             1.9362e+00,  5.7894e-01],\n",
      "           ...,\n",
      "           [ 3.2056e+00, -4.0907e-01,  1.1501e+00,  ..., -9.3666e-01,\n",
      "             2.6695e+00, -2.5574e-01],\n",
      "           [-2.7055e-01,  1.6412e+00, -8.5142e-01,  ..., -3.4412e-02,\n",
      "            -4.4718e-01,  2.2650e+00],\n",
      "           [ 4.4207e-01,  1.3840e+00,  1.5008e-01,  ..., -5.9778e-01,\n",
      "            -8.7727e-02, -1.8108e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.1328e+00,  1.3011e-01, -3.3884e-01,  ..., -2.1743e-01,\n",
      "             1.1886e+00, -7.7411e-01],\n",
      "           [ 1.0617e+00,  2.7135e+00, -1.1541e+00,  ...,  9.3071e-02,\n",
      "             6.5495e-01,  1.1472e+00],\n",
      "           [ 1.2016e+00, -3.3659e-01,  4.9767e-01,  ...,  3.4371e-01,\n",
      "            -9.1807e-01, -1.9605e-01],\n",
      "           ...,\n",
      "           [ 1.0207e+00, -1.0044e+00,  1.6244e+00,  ...,  1.4369e+00,\n",
      "             4.8666e-01,  7.7655e-01],\n",
      "           [-1.0850e+00, -8.1182e-01,  2.4840e-04,  ...,  2.7345e+00,\n",
      "            -2.9915e-01,  1.6434e+00],\n",
      "           [ 9.7609e-01,  4.5680e-02,  1.3171e+00,  ..., -1.2790e+00,\n",
      "             8.8393e-01,  6.9667e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 9.1241e-01, -1.4096e-01,  1.2508e+00,  ...,  1.0436e+00,\n",
      "            -5.2177e-02,  2.2108e-01],\n",
      "           [-2.7441e-02,  6.9989e-01,  1.5281e-01,  ...,  1.3203e+00,\n",
      "            -1.7462e+00,  1.1817e+00],\n",
      "           [-6.6451e-01, -1.2286e+00,  9.8946e-02,  ...,  6.6962e-01,\n",
      "            -9.3996e-01, -3.6438e-01],\n",
      "           ...,\n",
      "           [-6.8824e-01, -1.7114e+00, -5.3855e-01,  ..., -2.1448e+00,\n",
      "             4.0002e-01, -6.8336e-01],\n",
      "           [-3.5795e-02, -3.4742e-02, -1.6053e+00,  ..., -1.8665e+00,\n",
      "            -2.2012e+00, -1.3177e-01],\n",
      "           [-3.1487e-01, -1.2905e-01, -8.7095e-02,  ..., -4.0882e-01,\n",
      "            -5.3035e-01,  1.6214e-01]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[-1.2714, -0.5463,  2.7230,  ..., -0.3306,  0.3129,  0.3129],\n",
      "           [ 0.0240, -0.2969,  2.2065,  ..., -0.1640, -1.1311, -1.7564],\n",
      "           [ 0.9052, -0.3761, -0.1343,  ..., -0.0582, -0.5880, -0.5148],\n",
      "           ...,\n",
      "           [-0.0191, -0.9316, -0.5042,  ..., -0.4288,  0.4583,  1.1624],\n",
      "           [-0.0268,  1.6235,  0.2081,  ..., -0.8189, -1.2807, -0.9129],\n",
      "           [ 0.7879, -0.0951,  0.2669,  ..., -0.5461,  0.7797, -1.6576]]],\n",
      "\n",
      "\n",
      "         [[[-0.4662,  1.0565,  1.0450,  ...,  1.7132, -0.8795,  0.2381],\n",
      "           [-0.0336,  0.2562,  1.2351,  ...,  0.5327,  0.3126,  0.2725],\n",
      "           [-0.5400, -0.2716,  0.2745,  ..., -0.9531, -1.2740, -1.3760],\n",
      "           ...,\n",
      "           [ 1.5521, -1.3065, -1.7321,  ..., -0.0223,  0.7692,  0.6903],\n",
      "           [ 0.7193,  0.8291, -0.4182,  ..., -0.3801,  1.3421,  0.2697],\n",
      "           [ 0.7547,  1.9087,  0.1739,  ..., -0.3099,  0.7790,  0.6761]]],\n",
      "\n",
      "\n",
      "         [[[-0.9889,  0.4731,  0.1570,  ..., -1.6638, -0.1687,  0.0239],\n",
      "           [-1.6230,  0.6170, -0.2236,  ..., -1.5719,  0.5530,  1.3654],\n",
      "           [ 0.9821,  0.3033, -1.3579,  ...,  0.2883,  1.3462, -0.2072],\n",
      "           ...,\n",
      "           [-0.3426, -0.1919,  0.6518,  ..., -0.3754, -0.4563, -0.6586],\n",
      "           [ 0.4640, -0.0729, -0.5224,  ...,  0.2642, -1.3795, -0.4879],\n",
      "           [-0.4233, -0.5680,  1.1288,  ...,  1.1436, -2.4557,  0.3741]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.5589, -1.9282, -0.1159,  ..., -0.4360,  0.0140, -0.0484],\n",
      "           [ 1.0900,  0.6565,  0.0203,  ...,  0.3686,  0.4667,  0.5747],\n",
      "           [-0.4794,  0.6442,  0.9133,  ..., -0.1386,  0.0779,  1.0479],\n",
      "           ...,\n",
      "           [-0.5323, -1.6205,  0.3932,  ..., -1.1974,  0.7587, -0.5480],\n",
      "           [-0.1161, -0.4469, -0.0140,  ...,  0.6551, -0.0943,  1.0298],\n",
      "           [ 0.5152, -1.9731,  0.5144,  ...,  0.6908, -2.2543, -0.6798]]],\n",
      "\n",
      "\n",
      "         [[[-0.3969,  1.0027,  0.1718,  ..., -0.2557, -1.1509, -1.1860],\n",
      "           [-0.9172,  1.3845, -1.3348,  ..., -0.6075, -0.0495, -0.1755],\n",
      "           [ 0.7752, -0.4931,  1.8181,  ..., -1.2166, -0.9552,  1.0573],\n",
      "           ...,\n",
      "           [ 1.4629,  1.1965,  0.1481,  ...,  0.3822, -0.7769, -0.7670],\n",
      "           [ 0.3215, -1.5795, -1.4524,  ...,  0.0870, -0.4133,  0.0567],\n",
      "           [ 0.4063, -1.3366, -0.1972,  ...,  0.3633,  0.1250,  0.4939]]],\n",
      "\n",
      "\n",
      "         [[[-1.0791, -0.4719, -1.1798,  ...,  1.1635, -1.0898, -1.0966],\n",
      "           [-0.3555, -0.1871, -0.1860,  ..., -0.1393,  0.2637,  1.3126],\n",
      "           [-2.0084,  0.3640, -0.9679,  ..., -0.0565, -0.4745, -0.5671],\n",
      "           ...,\n",
      "           [-1.3181,  0.0506, -0.1869,  ..., -1.3173,  0.3553, -0.5781],\n",
      "           [ 0.1506, -0.1420,  0.4822,  ..., -0.2478, -0.6675, -0.4944],\n",
      "           [ 1.1284, -0.0928,  0.2019,  ...,  0.7313, -0.9108, -0.9184]]]]]), tensor([]), tensor([875.5936]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-2.0478, -1.2326,  2.4120,  ..., -1.0212, -0.3308, -0.2455],\n",
      "           [-0.4711, -0.6613,  2.1478,  ..., -0.3902, -1.4116, -2.2439],\n",
      "           [ 0.5432, -0.8338, -0.6621,  ..., -0.4575, -0.9824, -1.0034],\n",
      "           ...,\n",
      "           [ 0.3038, -1.3649, -0.5855,  ...,  0.0058,  1.1947,  1.9441],\n",
      "           [ 0.1331,  2.0736,  0.2469,  ..., -0.2980, -0.8503, -0.5013],\n",
      "           [ 0.8837, -0.0469,  0.3771,  ..., -0.0291,  1.5323, -1.5689]]],\n",
      "\n",
      "\n",
      "         [[[-1.2615,  0.4289,  0.4679,  ...,  1.1530, -1.5745, -0.3270],\n",
      "           [-0.5982, -0.3542,  0.5773,  ..., -0.1123, -0.3237, -0.2297],\n",
      "           [-1.4057, -1.4290, -0.9947,  ..., -1.8543, -2.1176, -2.0590],\n",
      "           ...,\n",
      "           [ 2.1106, -1.4581, -1.8824,  ...,  0.1713,  0.4502,  0.9952],\n",
      "           [ 0.5813,  1.5777, -0.7749,  ..., -0.7642,  1.0972, -0.3003],\n",
      "           [ 1.0922,  2.3141, -0.0253,  ..., -0.1316,  0.4125,  0.5028]]],\n",
      "\n",
      "\n",
      "         [[[-0.2746,  1.2740,  1.3922,  ..., -0.9169,  1.0324,  1.0378],\n",
      "           [-1.2090,  1.9018,  0.9662,  ..., -0.7084,  1.8155,  2.5469],\n",
      "           [ 2.0955,  1.4287, -0.4577,  ...,  1.2899,  2.5172,  0.6217],\n",
      "           ...,\n",
      "           [-0.5477,  0.0473,  1.1544,  ..., -0.1920, -0.9942, -1.2602],\n",
      "           [ 0.2185,  0.0385, -0.6278,  ...,  0.1073, -1.9439, -0.7525],\n",
      "           [-0.2962, -0.3505,  1.5964,  ...,  0.6837, -2.8389,  0.6753]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.5495, -2.2342,  0.2823,  ..., -0.3037, -0.0988, -0.2232],\n",
      "           [ 0.3774,  0.4767, -0.4706,  ..., -0.3967, -0.1064,  0.2091],\n",
      "           [-0.2791,  1.3445,  1.3535,  ...,  0.0551,  0.5479,  1.4042],\n",
      "           ...,\n",
      "           [-0.1280, -1.3418,  0.8891,  ..., -1.3490,  1.0032, -0.3034],\n",
      "           [ 0.2682,  0.4320,  0.1049,  ...,  0.8455,  0.1735,  1.2349],\n",
      "           [ 1.8001, -1.2382,  0.7173,  ...,  1.4555, -2.2648, -0.3959]]],\n",
      "\n",
      "\n",
      "         [[[-0.3044,  1.0614,  0.3768,  ..., -0.1425, -1.0302, -1.0846],\n",
      "           [-0.8316,  1.8189, -1.3215,  ..., -0.5868,  0.2352,  0.0151],\n",
      "           [ 1.1254, -0.3942,  2.1901,  ..., -1.5233, -1.0653,  1.2356],\n",
      "           ...,\n",
      "           [ 2.1117,  1.6099,  0.4559,  ...,  0.9169, -0.8972, -0.6601],\n",
      "           [ 0.4256, -1.2471, -1.6006,  ...,  0.1824, -0.5220,  0.0971],\n",
      "           [ 0.9356, -1.3962,  0.0373,  ...,  0.7813,  0.4202,  0.8250]]],\n",
      "\n",
      "\n",
      "         [[[-1.0798, -0.5415, -0.9243,  ...,  1.6342, -0.8054, -0.8206],\n",
      "           [-0.4698,  0.0838,  0.0991,  ..., -0.0447,  0.5110,  1.7595],\n",
      "           [-2.4187,  0.3300, -1.2524,  ...,  0.1057, -0.5012, -0.5748],\n",
      "           ...,\n",
      "           [-2.1788, -0.0225, -0.4159,  ..., -1.6376, -0.7135, -1.6823],\n",
      "           [-0.7166, -0.9071,  0.1117,  ..., -1.1438, -1.8024, -1.6992],\n",
      "           [ 0.7231, -0.8728, -0.2596,  ..., -0.0591, -1.6471, -1.1970]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 1.7955e-01,  3.1936e-01,  3.1129e-01,  ...,  1.0403e+00,\n",
      "             5.9221e-01,  3.4320e-01],\n",
      "           [ 1.1757e-01,  3.9157e-01,  2.9640e-01,  ...,  4.5775e-02,\n",
      "            -1.1432e-01,  5.7974e-01],\n",
      "           [-3.7691e-01,  2.5116e-01,  1.3369e-01,  ...,  4.9476e-01,\n",
      "             2.0766e-01,  1.6834e-01],\n",
      "           ...,\n",
      "           [-1.1328e-01,  1.3258e-01,  2.4350e-01,  ..., -2.7468e-01,\n",
      "            -5.5591e-01, -5.5851e-01],\n",
      "           [ 6.3414e-02, -2.3845e-01,  6.9306e-02,  ..., -4.6782e-01,\n",
      "            -2.1429e-01, -5.2528e-02],\n",
      "           [-2.7435e-01, -4.6317e-01,  1.5784e-01,  ..., -5.2539e-01,\n",
      "            -6.6457e-01, -1.4361e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.9392e-01,  4.3971e-01,  7.0195e-01,  ...,  6.9439e-01,\n",
      "             3.2839e-01,  6.3173e-02],\n",
      "           [ 6.3077e-01,  9.6529e-02,  6.7822e-01,  ...,  7.0383e-01,\n",
      "             1.9577e-01,  6.7968e-01],\n",
      "           [ 1.5205e-01,  1.1891e+00,  5.3632e-01,  ...,  5.3529e-01,\n",
      "             3.1632e-01,  7.0098e-02],\n",
      "           ...,\n",
      "           [-3.0916e-01, -3.2009e-01, -1.2852e-01,  ..., -4.9332e-01,\n",
      "             1.2615e-01, -4.4389e-01],\n",
      "           [ 2.1229e-01, -4.7133e-01, -2.4765e-01,  ..., -6.0084e-02,\n",
      "             1.3667e-01, -5.7646e-02],\n",
      "           [ 3.9754e-01, -1.1766e-01,  1.1384e-01,  ..., -5.7691e-02,\n",
      "            -3.9207e-01, -1.8740e-02]]],\n",
      "\n",
      "\n",
      "         [[[-6.3518e-01, -1.0378e-01, -8.6688e-01,  ..., -6.2221e-01,\n",
      "            -8.6801e-01, -9.5295e-01],\n",
      "           [-5.5064e-01, -1.0300e+00, -7.8284e-01,  ..., -9.1934e-01,\n",
      "            -6.6973e-01, -9.5820e-01],\n",
      "           [-9.2423e-01, -2.6257e-01, -1.1830e+00,  ..., -5.1494e-01,\n",
      "            -8.4116e-01, -5.6143e-01],\n",
      "           ...,\n",
      "           [-1.9607e-01, -3.5459e-01,  6.4756e-02,  ..., -1.9674e-01,\n",
      "             1.1614e-01,  5.7080e-01],\n",
      "           [ 1.6958e-01, -1.4573e-01, -2.7659e-01,  ...,  2.6488e-02,\n",
      "             5.7735e-01,  5.6067e-01],\n",
      "           [-3.0672e-01, -2.0953e-01, -2.6267e-01,  ..., -3.2251e-01,\n",
      "             1.4028e-01, -5.9404e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 3.3877e-02, -3.6849e-01, -2.7779e-01,  ..., -2.8151e-01,\n",
      "             2.0106e-01, -6.5489e-02],\n",
      "           [ 6.9012e-01,  1.0254e-01,  4.5831e-01,  ...,  4.7510e-01,\n",
      "             5.5196e-01,  1.3457e-01],\n",
      "           [-7.1180e-01, -5.7262e-01, -1.0101e-01,  ..., -2.4070e-01,\n",
      "            -2.0977e-01, -3.5863e-01],\n",
      "           ...,\n",
      "           [-2.1148e-02, -4.7385e-01, -4.3459e-02,  ..., -1.8921e-01,\n",
      "            -5.9544e-01, -3.5298e-01],\n",
      "           [-1.5981e-01, -8.5716e-01, -4.8495e-02,  ..., -2.2017e-01,\n",
      "            -1.3258e-01, -6.4868e-01],\n",
      "           [-9.8579e-01, -8.3898e-01,  9.3083e-03,  ..., -3.9378e-01,\n",
      "            -1.9143e-01, -1.7604e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.7426e-01, -6.9133e-02, -1.7248e-01,  ..., -5.3636e-01,\n",
      "            -1.3725e-01, -1.7389e-01],\n",
      "           [-2.9008e-01, -1.2893e-01,  6.7714e-02,  ..., -7.5448e-02,\n",
      "            -1.8603e-01, -8.8949e-02],\n",
      "           [-1.3238e-01,  2.3660e-02, -3.9671e-01,  ..., -3.4192e-02,\n",
      "            -3.3256e-01, -1.3348e-01],\n",
      "           ...,\n",
      "           [ 7.2957e-02, -1.9596e-01, -2.6563e-01,  ..., -8.1952e-01,\n",
      "            -1.3800e-01, -1.2068e-01],\n",
      "           [ 3.4068e-01,  1.4232e-02,  7.3862e-02,  ..., -2.1340e-01,\n",
      "            -2.3161e-01, -1.1689e-01],\n",
      "           [-7.0761e-01, -1.6628e-01, -6.9325e-02,  ..., -3.1448e-01,\n",
      "            -8.6069e-02, -3.0886e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.7015e-01, -3.3936e-02, -5.7819e-01,  ..., -5.1155e-01,\n",
      "            -3.1780e-01, -2.1165e-01],\n",
      "           [ 1.6852e-01, -5.8647e-01, -2.9122e-01,  ..., -6.7282e-01,\n",
      "            -1.5983e-01, -4.5406e-01],\n",
      "           [ 3.0452e-01, -7.0881e-02,  7.9650e-02,  ...,  5.6638e-02,\n",
      "             5.1238e-04,  1.8483e-01],\n",
      "           ...,\n",
      "           [ 5.9825e-01,  5.2948e-01,  1.9276e-01,  ...,  3.2284e-01,\n",
      "             7.4926e-01,  1.5073e-01],\n",
      "           [ 4.5167e-01,  5.7506e-01,  2.6397e-01,  ...,  7.4413e-01,\n",
      "             6.1067e-01,  5.6817e-01],\n",
      "           [ 1.9828e-02,  1.8037e-01,  3.8656e-01,  ...,  8.1152e-01,\n",
      "             5.8710e-01,  5.3508e-01]]]]]), tensor([]), tensor([224.4982]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[-1.6773, -0.8173, -1.1222,  ...,  1.9104,  0.0857, -0.8057],\n",
      "           [-1.5492,  0.1677, -0.3434,  ..., -0.5828, -1.1767,  1.3719],\n",
      "           [-3.4807, -0.5151, -1.3975,  ...,  0.6837, -0.5597, -1.0142],\n",
      "           ...,\n",
      "           [ 0.7961, -0.4902,  0.9368,  ...,  0.7095,  0.2003, -0.5400],\n",
      "           [ 0.7286, -0.3353,  0.3019,  ...,  0.2070,  1.7231,  1.5249],\n",
      "           [-1.3497, -1.6171,  1.0263,  ..., -0.1528, -0.6969,  1.0182]]],\n",
      "\n",
      "\n",
      "         [[[-0.3977, -0.8568,  0.8744,  ...,  0.0124, -1.2065, -2.0054],\n",
      "           [ 0.6287, -1.7101,  0.0817,  ...,  0.5187, -1.1402,  1.1913],\n",
      "           [-2.7931,  1.1002, -1.9768,  ..., -1.1806, -1.7061, -1.7235],\n",
      "           ...,\n",
      "           [-0.6256, -1.7025, -0.8142,  ...,  0.1220,  0.4591, -0.4625],\n",
      "           [-0.2630,  0.3022, -2.0214,  ...,  1.2049,  0.3931,  0.6226],\n",
      "           [ 3.0242, -0.0307, -0.4315,  ..., -0.6847, -1.4830,  0.7137]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6543,  2.4336,  0.5849,  ...,  1.8420,  0.8033, -0.0087],\n",
      "           [ 0.3436,  0.1469,  1.3815,  ...,  0.7980,  1.7738,  0.3956],\n",
      "           [-0.5959,  2.8503, -0.5393,  ...,  1.5811,  0.3716,  0.8745],\n",
      "           ...,\n",
      "           [-1.4100, -0.5672,  1.9068,  ..., -0.4537, -1.3579, -0.0186],\n",
      "           [-0.2769, -0.1903, -1.3810,  ..., -0.6926,  1.0301, -0.3469],\n",
      "           [-0.4732,  0.2268, -0.1561,  ..., -2.1912, -0.7053, -0.4750]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.6874, -1.1289,  0.7319,  ...,  0.0908,  0.7228, -0.4336],\n",
      "           [-0.8044, -1.5876, -1.3386,  ..., -1.4543, -1.0790, -1.8896],\n",
      "           [-1.1841,  0.3139,  1.2553,  ...,  0.0810,  0.5619, -0.6818],\n",
      "           ...,\n",
      "           [ 1.9076,  0.2636,  1.5958,  ...,  0.3016, -0.2310, -0.6031],\n",
      "           [ 0.8032, -0.0757,  0.0418,  ...,  0.8006,  1.2121,  0.0919],\n",
      "           [ 0.0722,  0.1479,  0.4448,  ...,  0.6712,  1.8311, -0.1708]]],\n",
      "\n",
      "\n",
      "         [[[-0.7181, -0.5448,  0.7871,  ..., -1.3066,  0.8521,  0.2881],\n",
      "           [-0.9247,  0.1225,  0.7447,  ...,  0.0165, -0.0496,  0.3542],\n",
      "           [ 0.2656,  0.8740, -1.4332,  ..., -0.2397, -1.6646, -0.1530],\n",
      "           ...,\n",
      "           [ 1.8265,  0.0242, -0.2859,  ..., -0.9259,  0.2890,  0.5891],\n",
      "           [ 1.6835,  2.2144,  0.4686,  ...,  0.9263, -0.4238,  0.9439],\n",
      "           [-1.1279, -0.1975,  0.7863,  ..., -0.5611,  0.7211,  1.6437]]],\n",
      "\n",
      "\n",
      "         [[[-0.9685,  0.1832, -0.9764,  ..., -0.5889,  0.7953,  0.9799],\n",
      "           [ 0.5937, -0.9908, -0.1053,  ..., -2.3732,  0.5845, -0.6845],\n",
      "           [ 0.6287, -0.5782,  0.1150,  ...,  0.0442,  0.1836,  1.2904],\n",
      "           ...,\n",
      "           [ 0.3107,  1.9506, -0.1611,  ..., -1.7345, -0.0394, -2.4066],\n",
      "           [-1.0477, -0.2949, -0.3463,  ..., -1.9597, -0.7306, -1.8112],\n",
      "           [-1.9719, -1.9371, -0.0222,  ..., -0.1676, -1.2693,  1.1642]]]]]), tensor([]))\n",
      "새로운 마이크로 배치: features=(tensor([[[[[ 5.6052e-01,  4.6700e-01,  5.4273e-01,  ...,  6.3654e-01,\n",
      "             5.8638e-01,  5.0512e-01],\n",
      "           [ 4.6899e-01,  3.3481e-01,  2.4978e-01,  ...,  5.1773e-02,\n",
      "             1.1416e-01,  2.2791e-01],\n",
      "           [ 3.9007e-01,  4.1939e-01,  5.0001e-01,  ...,  2.1651e-01,\n",
      "             2.5273e-01,  3.3570e-01],\n",
      "           ...,\n",
      "           [-2.5123e-01,  2.3277e-01, -2.2298e-02,  ..., -5.2902e-01,\n",
      "            -3.9875e-01, -5.7755e-01],\n",
      "           [-5.8658e-02, -1.3133e-01, -3.7900e-02,  ..., -4.6807e-01,\n",
      "            -5.6052e-01, -5.3904e-01],\n",
      "           [ 3.2854e-02, -6.9221e-02, -2.0234e-03,  ..., -4.4106e-01,\n",
      "            -4.4518e-01, -5.0798e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.0778e-01,  7.2853e-01,  5.4158e-01,  ...,  5.8908e-01,\n",
      "             5.2137e-01,  4.9410e-01],\n",
      "           [ 4.6853e-01,  5.5610e-01,  5.1060e-01,  ...,  6.4065e-01,\n",
      "             6.7905e-01,  4.4194e-01],\n",
      "           [ 8.2670e-01,  1.0717e+00,  1.0964e+00,  ...,  8.7727e-01,\n",
      "             6.4695e-01,  4.8203e-01],\n",
      "           ...,\n",
      "           [-2.6777e-01,  1.6299e-02,  4.8472e-03,  ...,  1.9092e-01,\n",
      "             1.2544e-01,  2.3731e-02],\n",
      "           [ 2.0557e-01, -6.1261e-01,  1.7299e-01,  ...,  7.0966e-01,\n",
      "            -6.8452e-02, -9.6819e-02],\n",
      "           [-1.1768e-01, -1.1557e-01,  1.6326e-01,  ...,  3.3569e-01,\n",
      "             6.3172e-01, -3.7309e-01]]],\n",
      "\n",
      "\n",
      "         [[[-7.1940e-01, -5.8880e-01, -9.1238e-01,  ..., -1.0408e+00,\n",
      "            -9.7221e-01, -8.9979e-01],\n",
      "           [-6.3380e-01, -9.6584e-01, -1.0047e+00,  ..., -1.0788e+00,\n",
      "            -1.0341e+00, -9.3232e-01],\n",
      "           [-8.3169e-01, -9.4303e-01, -8.9384e-01,  ..., -8.3158e-01,\n",
      "            -8.6840e-01, -6.9277e-01],\n",
      "           ...,\n",
      "           [ 1.4548e-01, -2.5892e-01, -3.6294e-01,  ..., -9.5684e-02,\n",
      "             5.9519e-01,  5.5885e-01],\n",
      "           [ 2.4426e-01, -1.2835e-01, -3.0094e-02,  ...,  1.9777e-01,\n",
      "             4.2380e-01,  6.3275e-01],\n",
      "           [-1.9326e-01, -2.1873e-01, -2.8143e-01,  ...,  6.3762e-02,\n",
      "             3.3115e-02,  3.9714e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.3480e-01,  2.5486e-02, -2.9378e-01,  ..., -1.8846e-01,\n",
      "            -1.6752e-01, -4.3090e-02],\n",
      "           [ 7.1175e-01,  2.6097e-01,  6.6398e-01,  ...,  7.1457e-01,\n",
      "             8.4937e-01,  6.2501e-01],\n",
      "           [-2.4535e-01, -3.2687e-01, -2.8011e-01,  ..., -1.8823e-01,\n",
      "            -3.2899e-01, -1.4335e-01],\n",
      "           ...,\n",
      "           [-4.2959e-01, -3.8572e-01, -3.1638e-01,  ..., -4.5797e-01,\n",
      "            -6.3750e-01,  1.0618e-01],\n",
      "           [-3.1534e-01, -8.2280e-01, -6.6383e-02,  ..., -3.4129e-01,\n",
      "            -7.9043e-01, -1.9542e-01],\n",
      "           [-9.2544e-01, -7.6705e-01, -1.4721e-01,  ..., -5.0871e-01,\n",
      "            -3.2117e-01, -1.1923e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.5323e-01, -1.0759e-02, -2.5368e-01,  ..., -2.3075e-01,\n",
      "            -2.3965e-01, -2.1208e-01],\n",
      "           [-9.6972e-02, -2.3758e-01, -2.1248e-01,  ..., -5.4569e-02,\n",
      "            -2.0590e-01, -1.7838e-01],\n",
      "           [-2.1544e-01, -4.5685e-02, -2.1295e-02,  ...,  5.4902e-02,\n",
      "            -4.3478e-03, -6.0301e-02],\n",
      "           ...,\n",
      "           [-3.3185e-01, -1.7309e-01, -1.8113e-01,  ..., -2.3397e-01,\n",
      "            -2.3905e-01,  5.5047e-02],\n",
      "           [-8.7361e-02, -5.1944e-01, -7.8530e-02,  ..., -1.8581e-03,\n",
      "            -2.9752e-01, -2.6642e-01],\n",
      "           [-4.4303e-01, -1.0434e-01, -1.1177e-01,  ..., -1.9175e-01,\n",
      "             4.6893e-02, -3.7211e-01]]],\n",
      "\n",
      "\n",
      "         [[[-2.3767e-01, -6.4396e-02, -3.2629e-01,  ..., -2.0984e-01,\n",
      "            -4.1509e-01, -3.4226e-01],\n",
      "           [-5.2814e-02, -3.4451e-01, -3.3446e-01,  ..., -1.5749e-01,\n",
      "            -3.2112e-01, -3.2521e-01],\n",
      "           [ 2.2110e-02,  7.6813e-02, -8.6412e-02,  ...,  8.0680e-02,\n",
      "            -1.0307e-03, -8.8237e-02],\n",
      "           ...,\n",
      "           [ 4.7573e-01,  4.6549e-02,  1.6579e-01,  ...,  1.0129e+00,\n",
      "             9.9009e-01,  5.1424e-01],\n",
      "           [ 8.0668e-01,  6.0413e-01,  3.5173e-01,  ...,  1.1020e+00,\n",
      "             9.9316e-01,  9.5469e-01],\n",
      "           [ 5.6494e-01,  5.2161e-01,  3.8581e-01,  ...,  6.4569e-01,\n",
      "             5.9061e-01,  9.1762e-01]]]]]), tensor([]), tensor([39.9078]), tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
      "         [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]), tensor([1]), tensor([]), tensor([])), labels=(tensor([[[[[ 9.3301e-01, -1.9861e+00, -1.0571e+00,  ...,  3.3244e-01,\n",
      "             4.5435e-01, -1.0565e+00],\n",
      "           [-5.9173e-01,  1.3668e+00, -1.0235e+00,  ..., -2.1565e+00,\n",
      "            -1.4508e+00, -2.0343e-02],\n",
      "           [-7.4691e-01, -6.2235e-01,  6.6004e-01,  ..., -2.0545e+00,\n",
      "            -1.5016e+00, -6.8196e-02],\n",
      "           ...,\n",
      "           [-3.4984e-01, -5.0892e-01, -1.0995e+00,  ...,  6.3474e-01,\n",
      "             1.6863e+00,  1.5214e+00],\n",
      "           [ 1.2283e+00,  5.1446e-01, -1.5620e-01,  ...,  1.5082e+00,\n",
      "             3.9111e-01,  4.0264e-02],\n",
      "           [ 3.0246e-01, -3.7388e-01,  7.9164e-01,  ..., -3.3890e-01,\n",
      "             1.3793e+00, -1.2304e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.6717e+00, -2.0353e+00, -3.6154e-01,  ..., -4.7326e-01,\n",
      "            -1.3183e+00, -8.1097e-01],\n",
      "           [-4.1687e-01, -4.1883e-02, -1.6229e+00,  ...,  1.1316e+00,\n",
      "             2.2094e+00, -8.4997e-01],\n",
      "           [ 1.8700e-01, -3.7630e-01, -1.3447e+00,  ...,  1.1766e+00,\n",
      "            -4.1938e-01, -1.2901e+00],\n",
      "           ...,\n",
      "           [-7.2263e-02,  1.1777e+00,  1.2514e+00,  ...,  4.4193e-01,\n",
      "             1.1768e+00,  1.6850e+00],\n",
      "           [-1.6778e-01,  8.5126e-01, -6.6697e-01,  ...,  7.4747e-02,\n",
      "             2.2949e+00,  2.9775e-02],\n",
      "           [ 1.7601e+00,  3.1332e-01, -1.4926e+00,  ..., -1.2654e+00,\n",
      "             4.0246e-01,  9.2640e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4987e+00,  1.5638e+00,  1.6881e+00,  ..., -2.3850e-01,\n",
      "             1.9525e+00, -4.5116e-01],\n",
      "           [ 2.1647e-03,  9.5309e-01,  9.8081e-01,  ...,  6.4731e-01,\n",
      "             2.0873e+00,  2.9962e+00],\n",
      "           [ 1.2438e+00,  1.8734e+00,  4.1105e+00,  ...,  6.9299e-01,\n",
      "             2.0104e-01,  2.6251e+00],\n",
      "           ...,\n",
      "           [-4.0822e-01, -3.2058e-01, -6.4482e-01,  ..., -1.1854e-01,\n",
      "            -5.8178e-01, -1.0978e+00],\n",
      "           [-7.9623e-01, -9.1259e-02, -9.6946e-01,  ...,  7.8935e-01,\n",
      "             8.4870e-01, -6.6170e-02],\n",
      "           [-5.6439e-01,  1.4468e-01, -6.8207e-01,  ..., -4.9352e-02,\n",
      "            -2.2584e+00, -4.3816e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.5950e+00, -8.8074e-01, -6.0750e-02,  ...,  6.9647e-01,\n",
      "             1.2374e+00,  8.6614e-01],\n",
      "           [-2.6688e+00, -1.1455e+00, -6.6061e-01,  ..., -1.0061e+00,\n",
      "            -2.2643e+00, -5.7930e-01],\n",
      "           [ 1.9480e+00,  1.1507e+00,  3.7906e-02,  ...,  1.2491e-01,\n",
      "             3.8770e-01, -6.6632e-01],\n",
      "           ...,\n",
      "           [-3.3490e-01, -1.1527e-02,  1.4807e+00,  ...,  8.3094e-01,\n",
      "             2.1637e+00, -4.8677e-01],\n",
      "           [ 3.2596e-02,  2.1888e-01,  4.5018e-01,  ..., -2.9818e-02,\n",
      "             1.9932e+00,  1.7314e-01],\n",
      "           [ 1.1849e+00,  5.6057e-01,  5.6581e-01,  ...,  2.5667e-01,\n",
      "             1.7852e-01,  2.9554e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4084e-01, -1.9055e+00,  4.3118e-01,  ...,  2.6222e-03,\n",
      "             5.7776e-02, -3.9070e-01],\n",
      "           [ 1.3134e-01,  5.5654e-01,  2.4307e-01,  ...,  1.8683e+00,\n",
      "            -1.3988e-01, -8.6247e-01],\n",
      "           [ 1.0005e+00,  8.3817e-01,  1.1620e+00,  ...,  1.4802e-01,\n",
      "             1.2501e+00,  3.6371e-01],\n",
      "           ...,\n",
      "           [ 1.2274e+00,  7.6635e-01,  1.2178e+00,  ..., -7.7513e-01,\n",
      "             3.3288e-01,  1.2011e+00],\n",
      "           [-6.3244e-01, -6.3294e-01, -1.5419e-01,  ..., -2.5515e-01,\n",
      "            -2.0495e+00, -1.0351e+00],\n",
      "           [ 3.8020e-01,  2.3437e+00,  2.0301e+00,  ...,  1.1185e+00,\n",
      "            -2.7993e-01,  1.6278e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1288e+00,  1.5706e-01,  1.0078e+00,  ...,  2.5787e+00,\n",
      "             1.1216e+00,  4.2340e-01],\n",
      "           [-6.7685e-01,  5.8251e-01,  8.1512e-01,  ...,  4.5970e-01,\n",
      "            -1.3156e+00, -1.1615e+00],\n",
      "           [ 7.1864e-01,  1.0728e+00, -1.1704e+00,  ...,  1.6959e-01,\n",
      "             2.8852e-01,  8.8819e-01],\n",
      "           ...,\n",
      "           [-2.5345e+00, -5.1636e-01, -1.3045e+00,  ..., -1.4092e+00,\n",
      "            -3.0344e+00, -1.8994e-01],\n",
      "           [ 1.0552e+00, -1.0851e+00, -8.9526e-01,  ..., -1.6236e-01,\n",
      "            -2.1536e+00, -9.9149e-01],\n",
      "           [ 1.4757e+00, -2.7039e+00, -1.0558e+00,  ..., -1.0991e+00,\n",
      "            -2.2154e+00,  4.5437e-01]]]]]), tensor([]))\n"
     ]
    }
   ],
   "source": [
    "for micro_batch_features, micro_batch_labels in train_dataloader._pull_batches_from_dataloader():\n",
    "    # 여기서 micro_batch를 사용하여 원하는 작업을 수행합니다.\n",
    "    # 예를 들어, 모델의 forward pass를 실행하거나 손실을 계산할 수 있습니다.\n",
    "    print(f\"새로운 마이크로 배치: features={micro_batch_features}, labels={micro_batch_labels}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)# print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_engine.micro_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[[[ 0.3444,  0.8501,  0.0978,  ...,  0.4475,  0.0255,  0.2550],\n",
       "             [ 0.4779,  1.0152, -0.5630,  ..., -0.8839,  0.6759,  0.3996],\n",
       "             [-0.2255, -0.3382, -0.0933,  ...,  0.1754,  0.6982, -1.1214],\n",
       "             ...,\n",
       "             [-0.9603,  0.2485,  0.6157,  ..., -0.6411, -0.3045,  0.0230],\n",
       "             [-0.8618,  0.0832, -0.8533,  ..., -0.3310,  0.1440,  0.0946],\n",
       "             [-0.6234, -0.7607, -1.4489,  ..., -1.2311,  0.2502, -0.7860]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.5539, -0.3784,  0.3700,  ...,  0.1250, -0.5988, -1.0986],\n",
       "             [ 1.5587, -0.1881,  0.4646,  ..., -0.3396,  0.3344, -0.5544],\n",
       "             [-0.4905,  0.4524,  0.7750,  ...,  0.6366,  0.1686, -0.0189],\n",
       "             ...,\n",
       "             [-0.9213, -1.4901,  0.4809,  ..., -1.1218,  0.5831, -0.5164],\n",
       "             [-0.1459,  0.3132,  1.1969,  ...,  0.1329,  0.1824,  0.0871],\n",
       "             [-1.0931, -0.4163,  0.6122,  ..., -0.5046, -0.5727, -0.1456]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.0651, -0.9561,  0.1544,  ..., -1.1411, -0.2259,  0.0308],\n",
       "             [-0.4188,  0.3058, -0.6135,  ..., -1.0970,  0.1245,  0.0238],\n",
       "             [-0.6171, -1.3021, -0.9767,  ...,  0.0167,  0.2617, -0.8506],\n",
       "             ...,\n",
       "             [ 0.0313,  0.5177, -0.2299,  ..., -0.8321, -0.6621,  0.5652],\n",
       "             [ 0.7522,  0.0963,  0.3570,  ...,  0.3588, -0.1661,  0.4702],\n",
       "             [-0.2645, -0.9195,  0.0655,  ...,  0.2973, -0.4645,  0.2978]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-0.3128, -0.2215, -0.5487,  ...,  0.2708, -0.1805, -0.0050],\n",
       "             [ 0.5401,  0.8183, -0.6391,  ..., -0.6529, -0.6168, -0.4258],\n",
       "             [ 0.5031,  0.2292, -0.6266,  ..., -0.4910, -0.2080,  0.2016],\n",
       "             ...,\n",
       "             [-0.0338,  0.1570,  0.0620,  ..., -0.6011,  0.0790, -0.1415],\n",
       "             [-0.3951, -0.0352, -0.8726,  ...,  0.5902, -0.1773, -0.2887],\n",
       "             [ 0.1581, -1.2457,  0.6309,  ...,  0.1520, -0.4723, -0.4066]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.5647, -0.0925, -1.2302,  ..., -1.2819,  0.6150,  0.5317],\n",
       "             [ 0.3342, -0.7117, -0.1305,  ...,  0.6691, -0.9321,  0.0829],\n",
       "             [-0.0093, -0.4201, -0.4699,  ..., -0.3685, -1.2137,  0.9463],\n",
       "             ...,\n",
       "             [-0.5443,  0.4623, -0.4866,  ...,  0.5768,  0.3421, -0.6708],\n",
       "             [ 0.0745, -0.4683,  0.5892,  ...,  0.3810,  0.4396,  0.2916],\n",
       "             [ 0.5655,  0.5906, -0.3401,  ...,  0.0883,  0.4900, -0.3094]]],\n",
       "  \n",
       "  \n",
       "           [[[ 0.4816, -0.4064, -0.3464,  ..., -0.4675, -1.5924,  0.0126],\n",
       "             [ 0.0099,  0.4263, -0.9737,  ...,  1.1576,  0.3166,  0.1696],\n",
       "             [ 2.0608, -0.8264, -0.7554,  ...,  0.1188, -0.0053,  0.3385],\n",
       "             ...,\n",
       "             [ 0.4284, -0.0815,  0.5396,  ..., -0.5107,  1.5224,  0.3471],\n",
       "             [ 0.1877,  0.1162,  0.5460,  ..., -0.0071,  0.8720,  0.0074],\n",
       "             [ 0.4606,  0.0305, -0.0189,  ..., -0.1530, -0.3797, -0.2236]]]]]),\n",
       "  tensor([]),\n",
       "  tensor([552.5996]),\n",
       "  tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           ...,\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]),\n",
       "  tensor([1]),\n",
       "  tensor([]),\n",
       "  tensor([])),\n",
       " (tensor([[[[[-0.3900,  0.6240, -0.8623,  ..., -0.3040, -0.9892, -0.4631],\n",
       "             [-0.0260,  1.1136, -1.7201,  ..., -1.9067,  0.9476,  0.2395],\n",
       "             [-1.1020, -1.4636, -0.8385,  ..., -0.2644,  0.7234, -2.6533],\n",
       "             ...,\n",
       "             [-1.2197,  0.0285,  1.0513,  ..., -0.5520,  0.6008,  1.1079],\n",
       "             [-1.3884,  0.4438, -1.5498,  ...,  0.2749,  1.0695,  1.0216],\n",
       "             [-1.1764, -1.2004, -2.4845,  ..., -1.3977,  1.1629, -0.7934]]],\n",
       "  \n",
       "  \n",
       "           [[[-2.2448, -1.8395, -0.1989,  ..., -0.9982, -2.2174, -2.9722],\n",
       "             [ 1.8927, -1.1917, -0.3498,  ..., -1.7224, -0.2512, -1.6578],\n",
       "             [-2.1293, -0.9083, -0.7184,  ..., -0.1867, -1.0430, -0.8507],\n",
       "             ...,\n",
       "             [-1.3740, -2.7992,  0.7640,  ..., -1.8267,  0.7237, -0.1414],\n",
       "             [-0.7884,  1.5188,  1.7917,  ...,  0.6229,  0.1045,  0.5533],\n",
       "             [-1.4995, -0.5694,  0.7337,  ..., -1.2744, -1.6315, -0.6199]]],\n",
       "  \n",
       "  \n",
       "           [[[ 1.2883, -0.5478,  2.1050,  ..., -0.2082,  1.4991,  1.7524],\n",
       "             [ 0.3301,  2.5114,  0.9014,  ..., -0.0658,  2.1267,  1.9177],\n",
       "             [ 0.3470, -0.5184,  0.0461,  ...,  1.6054,  2.0471, -0.1671],\n",
       "             ...,\n",
       "             [-0.1784,  1.3391,  0.2405,  ..., -1.2118, -2.1343, -0.1514],\n",
       "             [ 0.9197,  0.3498,  0.5805,  ...,  0.5225, -1.5202, -0.3857],\n",
       "             [-0.1164, -1.2065,  0.5312,  ..., -0.4037, -1.4003,  0.3141]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-0.8939, -0.1775, -0.1142,  ...,  1.0388, -0.3772, -0.0333],\n",
       "             [-0.6328,  0.5964, -2.2045,  ..., -2.6422, -2.5519, -1.8911],\n",
       "             [ 1.3845,  1.4262, -0.2041,  ..., -0.6467,  0.0925,  0.5643],\n",
       "             ...,\n",
       "             [ 0.7306,  1.2592,  0.8476,  ..., -0.8757,  1.1626,  1.1946],\n",
       "             [-0.1267,  1.4307, -1.4813,  ...,  0.9878,  0.7099,  0.6771],\n",
       "             [ 2.1033, -0.6766,  1.3280,  ...,  0.7844, -0.0815,  0.3303]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.8131, -0.2497, -1.5685,  ..., -1.8831,  1.6939,  1.3988],\n",
       "             [ 0.8357, -0.9663, -0.1111,  ...,  1.3575, -1.3434,  0.3853],\n",
       "             [ 0.3206, -0.5081, -0.6788,  ..., -0.8355, -2.3310,  1.9005],\n",
       "             ...,\n",
       "             [-0.3436,  1.2176, -0.5232,  ...,  2.0925,  0.6344, -0.8421],\n",
       "             [ 0.2133,  0.0335,  1.1322,  ...,  1.4034,  0.7335,  0.8054],\n",
       "             [ 1.8551,  1.2832, -0.1730,  ...,  0.3227,  0.9752, -0.2929]]],\n",
       "  \n",
       "  \n",
       "           [[[ 1.1521, -0.5858,  0.0347,  ..., -0.1832, -1.9627,  0.8072],\n",
       "             [-0.0707,  1.4393, -1.2088,  ...,  2.3220,  1.1015,  0.7519],\n",
       "             [ 3.4716, -1.5706, -1.2691,  ...,  0.1457, -0.1583,  0.7453],\n",
       "             ...,\n",
       "             [-0.1839, -0.3156,  0.5530,  ..., -1.0724,  0.7719, -0.7377],\n",
       "             [-0.9201, -0.9486,  0.3610,  ..., -0.9250, -0.1794, -1.6118],\n",
       "             [-0.0085, -1.0677, -0.7460,  ..., -1.6163, -2.3305, -1.4973]]]]]),\n",
       "  tensor([])))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[[[-4.4184e-01, -1.1272e+00,  8.2826e-01,  ..., -1.5475e+00,\n",
       "              -2.0637e-01,  2.1849e-01],\n",
       "             [ 2.8786e-01, -5.1555e-01, -1.4576e+00,  ..., -5.3281e-01,\n",
       "               2.5052e-01,  1.5377e+00],\n",
       "             [ 1.9329e+00, -4.3864e-01, -5.1611e-01,  ...,  1.2913e+00,\n",
       "              -8.6932e-01, -4.0749e-01],\n",
       "             ...,\n",
       "             [-7.3774e-01,  8.5847e-01,  3.6995e-02,  ...,  1.0429e+00,\n",
       "               7.2542e-01,  9.8676e-02],\n",
       "             [-6.5629e-01, -1.2510e+00, -5.0444e-01,  ...,  9.6587e-01,\n",
       "              -2.4553e-01, -3.1203e-02],\n",
       "             [ 1.1551e-01, -3.2697e-01,  1.1071e+00,  ..., -7.5701e-01,\n",
       "               2.0280e-01,  1.8850e+00]]],\n",
       "  \n",
       "  \n",
       "           [[[-4.5225e-02,  2.8829e-01,  4.2269e-01,  ..., -4.1657e-02,\n",
       "               1.4137e+00,  2.0755e-01],\n",
       "             [-9.4544e-01, -6.7728e-01,  1.5226e+00,  ...,  1.7127e+00,\n",
       "               9.5879e-01,  1.5252e+00],\n",
       "             [-1.6084e+00, -3.9692e-01, -1.5844e-01,  ...,  9.3294e-01,\n",
       "              -2.8396e-01,  2.5365e-01],\n",
       "             ...,\n",
       "             [-1.5547e-01, -8.1506e-01,  4.6259e-01,  ..., -8.5734e-01,\n",
       "               1.2174e+00, -7.8289e-01],\n",
       "             [-2.1116e-01, -7.4991e-02, -5.4180e-01,  ...,  2.5262e-02,\n",
       "              -1.1037e-01, -7.4737e-01],\n",
       "             [ 5.2652e-01,  1.1236e+00,  6.3861e-01,  ..., -8.7254e-01,\n",
       "              -7.6479e-01, -1.2733e-01]]],\n",
       "  \n",
       "  \n",
       "           [[[-1.9440e+00, -1.4637e+00, -2.0572e+00,  ...,  7.3114e-01,\n",
       "               3.6307e-01,  1.1984e+00],\n",
       "             [ 3.0202e-01,  4.9582e-01, -4.6264e-01,  ..., -9.4059e-01,\n",
       "              -7.2834e-01,  1.8946e-02],\n",
       "             [-4.3071e-01, -5.5705e-01,  9.6018e-01,  ...,  5.2125e-01,\n",
       "               1.8650e+00,  8.0650e-01],\n",
       "             ...,\n",
       "             [ 2.2051e+00,  3.1901e-01, -1.2058e+00,  ...,  2.5320e-01,\n",
       "              -7.7498e-01, -9.0527e-02],\n",
       "             [ 8.2736e-01, -7.1969e-01,  1.5903e+00,  ...,  1.3941e+00,\n",
       "               1.0602e+00,  1.1402e-01],\n",
       "             [ 5.4628e-01, -1.1386e+00, -2.8825e-01,  ...,  2.3103e-02,\n",
       "              -6.0307e-01,  1.7307e+00]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-5.6314e-01,  6.3045e-01,  9.9633e-02,  ...,  1.2590e+00,\n",
       "              -1.1824e+00,  1.6755e+00],\n",
       "             [-8.8736e-02,  1.0503e+00, -8.7721e-01,  ...,  8.7027e-01,\n",
       "               7.5916e-01,  6.2577e-01],\n",
       "             [-6.2425e-01,  1.5884e+00,  1.1324e+00,  ..., -2.5899e+00,\n",
       "               3.7107e-01, -8.8986e-01],\n",
       "             ...,\n",
       "             [ 7.0760e-01, -1.6204e-03, -4.3400e-01,  ..., -4.2284e-01,\n",
       "               1.7304e-01, -1.2572e+00],\n",
       "             [-2.3170e-01, -1.2664e+00, -4.9335e-01,  ..., -1.2782e+00,\n",
       "               3.7180e-01,  4.7024e-01],\n",
       "             [-1.2122e+00, -5.6485e-03,  4.7356e-01,  ...,  4.4837e-01,\n",
       "              -1.1747e+00,  6.4219e-01]]],\n",
       "  \n",
       "  \n",
       "           [[[-2.0372e-01,  3.7216e-01, -3.2876e-01,  ..., -1.3901e+00,\n",
       "              -4.3947e-01,  2.9877e-01],\n",
       "             [-3.2864e-02,  7.8074e-02,  9.8098e-02,  ..., -7.0869e-01,\n",
       "               1.2408e-01, -5.4018e-01],\n",
       "             [-5.2866e-01,  1.5806e+00, -2.0302e+00,  ...,  9.7213e-02,\n",
       "               7.1356e-02,  1.0646e+00],\n",
       "             ...,\n",
       "             [ 4.5942e-01,  2.6660e-01, -6.5890e-01,  ...,  1.2560e+00,\n",
       "               2.3011e+00,  2.3921e-01],\n",
       "             [-6.1311e-01,  8.4831e-02, -6.7098e-01,  ..., -5.0818e-01,\n",
       "              -6.5450e-02,  1.1738e+00],\n",
       "             [ 1.1799e+00, -4.9060e-01, -3.0128e-01,  ...,  1.5039e+00,\n",
       "               2.7625e-01,  1.8050e+00]]],\n",
       "  \n",
       "  \n",
       "           [[[-2.6076e-01,  5.3367e-01, -7.6364e-01,  ..., -1.5309e-01,\n",
       "              -4.3927e-01,  1.9966e+00],\n",
       "             [ 7.8684e-01,  1.6632e-01,  1.1100e+00,  ..., -7.3036e-01,\n",
       "              -2.4745e-01,  4.8068e-01],\n",
       "             [ 5.1017e-01, -1.1027e+00, -1.6228e-01,  ..., -2.5118e-01,\n",
       "               6.6615e-01,  1.5657e+00],\n",
       "             ...,\n",
       "             [ 7.5035e-01,  7.4717e-01,  2.2856e+00,  ...,  8.0942e-01,\n",
       "              -9.9947e-01, -2.6840e-01],\n",
       "             [ 1.2572e+00,  1.1792e+00, -6.7895e-01,  ..., -6.3373e-01,\n",
       "               4.0908e-01, -6.2577e-02],\n",
       "             [ 4.5302e-01, -2.4267e-01, -4.5562e-01,  ..., -4.7886e-01,\n",
       "              -7.5577e-01,  7.0161e-01]]]]]),\n",
       "  tensor([]),\n",
       "  tensor([963.7830]),\n",
       "  tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           ...,\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "           [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]),\n",
       "  tensor([1]),\n",
       "  tensor([]),\n",
       "  tensor([])),\n",
       " (tensor([[[[[-1.0065, -1.6711,  0.2038,  ..., -2.2251, -0.8547, -0.3690],\n",
       "             [-0.1777, -0.8388, -1.8108,  ..., -0.6508,  0.1255,  1.3532],\n",
       "             [ 1.6054, -0.8760, -0.9287,  ...,  0.9854, -1.2034, -0.7526],\n",
       "             ...,\n",
       "             [-0.4745,  0.5750,  0.0469,  ...,  1.4389,  1.3911,  0.5778],\n",
       "             [-0.5690, -1.1888, -0.5205,  ...,  1.4439,  0.3204,  0.6363],\n",
       "             [ 0.1143, -0.2682,  1.2157,  ..., -0.3857,  0.6835,  2.3454]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.6777, -0.3951, -0.1740,  ..., -0.8550,  0.7866, -0.3137],\n",
       "             [-1.5538, -1.2167,  0.9090,  ...,  1.1462,  0.3944,  1.1321],\n",
       "             [-2.6137, -1.4941, -1.0750,  ...,  0.2777, -0.9158, -0.1585],\n",
       "             ...,\n",
       "             [ 0.2303, -0.8473,  0.5622,  ..., -0.7593,  1.2231, -0.6435],\n",
       "             [-0.4395,  0.5435, -0.7917,  ..., -0.2036, -0.1426, -0.8159],\n",
       "             [ 0.8101,  1.2345,  0.4555,  ..., -0.9397, -0.9064, -0.6456]]],\n",
       "  \n",
       "  \n",
       "           [[[-1.2100, -0.8549, -1.0282,  ...,  1.7867,  1.5115,  2.2617],\n",
       "             [ 0.9373,  1.6501,  0.5404,  ...,  0.1324,  0.4154,  1.0487],\n",
       "             [ 0.3877,  0.4631,  2.0908,  ...,  1.3752,  2.8392,  1.5958],\n",
       "             ...,\n",
       "             [ 2.1559,  0.5575, -0.8818,  ...,  0.2701, -1.3790, -0.7615],\n",
       "             [ 0.5540, -0.6378,  1.6376,  ...,  1.3375,  0.5171, -0.2624],\n",
       "             [ 0.7364, -0.9168, -0.0575,  ..., -0.8044, -0.6731,  1.3735]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-0.6754,  0.6635,  0.3855,  ...,  1.2308, -1.3156,  1.4026],\n",
       "             [-0.8896,  0.7111, -1.2675,  ...,  0.1943,  0.0036,  0.1420],\n",
       "             [-0.1166,  2.4113,  1.6553,  ..., -2.5594,  0.6831, -0.7008],\n",
       "             ...,\n",
       "             [ 1.2429,  0.4580, -0.0425,  ..., -0.3033,  0.4438, -0.4749],\n",
       "             [ 0.2003, -0.4350, -0.4389,  ..., -1.5395,  0.3938,  0.9537],\n",
       "             [-0.1238,  0.9051,  0.6651,  ...,  0.6057, -0.9094,  0.6169]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.1066,  0.2972, -0.1214,  ..., -1.2893, -0.2069,  0.5801],\n",
       "             [ 0.0572,  0.2513,  0.2397,  ..., -0.7292,  0.2608, -0.3210],\n",
       "             [-0.3934,  1.7323, -1.9851,  ...,  0.0243, -0.0332,  1.1650],\n",
       "             ...,\n",
       "             [ 0.8742,  0.5163, -0.4603,  ...,  1.8264,  2.5278,  0.5550],\n",
       "             [-0.5616,  0.6248, -0.6161,  ..., -0.2614, -0.1982,  1.4116],\n",
       "             [ 1.6793, -0.3786, -0.0719,  ...,  1.8170,  0.5026,  1.6761]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.1101,  0.6232, -0.4577,  ...,  0.2270, -0.0681,  2.4087],\n",
       "             [ 0.7620,  0.4845,  1.4491,  ..., -0.5803, -0.0253,  0.7637],\n",
       "             [ 0.3067, -1.1701, -0.2632,  ..., -0.1424,  0.6780,  1.7758],\n",
       "             ...,\n",
       "             [ 0.1340,  0.7169,  2.1780,  ...,  0.7889, -2.0377, -1.0794],\n",
       "             [ 0.5247,  0.5360, -1.0972,  ..., -0.9958, -0.4397, -0.9709],\n",
       "             [-0.0205, -0.9247, -0.9108,  ..., -1.1428, -1.4698,  0.2377]]]]]),\n",
       "  tensor([])))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((tensor([[[[[ 0.5029, -0.6719, -0.1165,  ..., -0.2498, -1.0033,  0.0513],\n",
      "           [-0.3093, -0.9440, -0.3851,  ...,  0.6261, -0.2868, -0.3529],\n",
      "           [ 0.3373,  0.1198, -0.7629,  ...,  0.4753, -0.0343, -0.4412],\n",
      "           ...,\n",
      "           [ 0.4332,  0.6598, -0.7912,  ..., -0.1079, -1.5893, -0.2849],\n",
      "           [-0.4316,  0.9296,  0.5756,  ..., -0.3770, -0.6223,  0.0524],\n",
      "           [-1.6205,  0.1285,  0.3675,  ..., -0.5046,  0.7960, -0.6776]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1261,  0.5905,  0.3174,  ...,  0.5060, -0.2114, -0.8570],\n",
      "           [ 1.3833,  1.4262,  0.5179,  ..., -0.3579,  0.4883,  0.3209],\n",
      "           [-0.3900, -0.0787, -0.1272,  ...,  0.3446, -0.0275,  0.9814],\n",
      "           ...,\n",
      "           [-1.0312, -0.8147, -1.0036,  ...,  0.7246, -0.3195,  0.9741],\n",
      "           [-0.1439, -0.1230,  0.8771,  ..., -0.6778, -0.1177, -0.7173],\n",
      "           [ 0.6979,  0.0600, -0.0976,  ..., -0.2695,  0.2549, -0.4113]]],\n",
      "\n",
      "\n",
      "         [[[-0.7986, -0.1794,  0.0828,  ...,  0.0598,  0.0042, -0.5939],\n",
      "           [ 0.6598, -0.8077, -0.4133,  ...,  0.0201,  0.0671, -0.9179],\n",
      "           [-0.7850, -0.3504,  0.2400,  ..., -0.9535, -0.6789, -0.1005],\n",
      "           ...,\n",
      "           [ 0.6446, -0.4976, -0.4552,  ..., -0.6544, -0.6219,  0.0602],\n",
      "           [-1.2924,  0.8068,  1.1440,  ...,  0.5273, -0.1482,  0.0187],\n",
      "           [ 0.7822, -0.1770, -0.8628,  ..., -0.2682, -0.2733,  0.0959]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.4038, -0.4985,  0.4915,  ...,  0.2747,  0.2073, -0.2096],\n",
      "           [-0.0219,  0.1097, -0.2019,  ..., -0.5526,  0.6158,  0.8739],\n",
      "           [-0.6048,  0.4057, -0.8515,  ...,  0.2525, -0.1496, -0.2406],\n",
      "           ...,\n",
      "           [-0.5761, -0.7326,  0.1473,  ..., -0.5967, -0.4881, -0.1025],\n",
      "           [-0.2169, -0.0605,  0.8747,  ..., -0.1263,  0.1834, -1.6326],\n",
      "           [-0.5812, -0.9099, -0.1049,  ..., -0.4120,  0.5265,  0.7849]]],\n",
      "\n",
      "\n",
      "         [[[-0.0144,  0.8384,  0.5255,  ..., -0.0579,  0.9677, -0.0185],\n",
      "           [-0.1735,  0.7094, -0.1544,  ...,  0.4037,  0.2498,  0.4713],\n",
      "           [ 0.7148, -0.8550,  0.1454,  ...,  0.3159,  0.0073, -0.4926],\n",
      "           ...,\n",
      "           [-0.2641, -0.4861,  0.5500,  ..., -0.5709, -0.6733,  0.0666],\n",
      "           [-0.0355, -0.0237,  0.4290,  ...,  0.7680,  0.3915, -0.1125],\n",
      "           [-0.5517, -0.4155, -0.0474,  ..., -1.7340,  0.4793, -0.6765]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0405, -0.4959,  0.1154,  ..., -1.0264, -0.7730, -1.2635],\n",
      "           [ 0.3456,  1.0925, -0.2405,  ..., -0.3801, -0.2600, -0.3971],\n",
      "           [-0.2405, -0.4101,  0.1986,  ...,  0.1388,  1.0017, -0.2758],\n",
      "           ...,\n",
      "           [-0.2635, -0.0964, -1.5262,  ...,  0.9765,  0.6758,  1.5063],\n",
      "           [ 0.3101,  0.1164, -0.5202,  ..., -0.2049,  0.5091,  0.3371],\n",
      "           [ 0.6474,  0.0650, -0.1408,  ...,  0.7461,  0.0163, -0.0862]]]]]), tensor([]), tensor([569.5222]), tensor([[[-0.0007, -0.0096,  0.0084,  ...,  0.0006, -0.0261, -0.0021],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645]]]), tensor([1]), tensor([]), tensor([])), (tensor([[[[[-4.9187e-03, -2.1044e+00, -1.2807e+00,  ..., -1.5337e+00,\n",
      "            -2.8364e+00, -9.6512e-01],\n",
      "           [-1.2702e+00, -2.2536e+00, -1.2851e+00,  ...,  7.6940e-01,\n",
      "            -9.0391e-01, -1.0453e+00],\n",
      "           [-1.4647e-01, -4.9856e-01, -2.1295e+00,  ...,  2.0684e-01,\n",
      "            -5.8191e-01, -1.3024e+00],\n",
      "           ...,\n",
      "           [ 1.2071e+00,  6.4196e-01, -1.4335e+00,  ...,  4.9599e-01,\n",
      "            -1.7581e+00,  3.3206e-01],\n",
      "           [-5.9627e-01,  1.8549e+00,  1.0053e+00,  ...,  1.4863e-01,\n",
      "            -4.2155e-01,  9.5952e-01],\n",
      "           [-2.8181e+00,  3.0107e-01,  7.4274e-01,  ..., -6.2007e-02,\n",
      "             2.1703e+00, -6.3159e-01]]],\n",
      "\n",
      "\n",
      "         [[[-8.8992e-01, -2.1927e-01, -6.7502e-01,  ..., -2.5798e-01,\n",
      "            -1.4900e+00, -2.5302e+00],\n",
      "           [ 1.4382e+00,  1.4997e+00, -3.1326e-01,  ..., -1.6024e+00,\n",
      "            -3.3899e-01, -2.5008e-01],\n",
      "           [-2.3266e+00, -1.9346e+00, -2.2019e+00,  ..., -4.0127e-01,\n",
      "            -1.1233e+00,  9.7201e-01],\n",
      "           ...,\n",
      "           [-1.3430e+00, -1.4617e+00, -1.7527e+00,  ...,  1.1883e+00,\n",
      "            -5.4184e-01,  2.1618e+00],\n",
      "           [-6.4423e-01,  8.1667e-01,  9.9024e-01,  ..., -1.7103e+00,\n",
      "            -4.1326e-02, -1.1888e+00],\n",
      "           [ 1.7768e+00,  2.7976e-01, -4.8759e-01,  ..., -3.4870e-01,\n",
      "             2.1736e-01, -1.1196e+00]]],\n",
      "\n",
      "\n",
      "         [[[-7.1936e-02,  7.9007e-01,  1.9023e+00,  ...,  1.8655e+00,\n",
      "             1.8004e+00,  5.3273e-01],\n",
      "           [ 2.1929e+00,  4.0818e-01,  1.1246e+00,  ...,  1.8419e+00,\n",
      "             1.9077e+00, -2.3088e-02],\n",
      "           [ 1.2336e-02,  1.0678e+00,  2.1626e+00,  ..., -1.1594e-01,\n",
      "             2.7228e-01,  1.1707e+00],\n",
      "           ...,\n",
      "           [ 8.7385e-01, -4.1905e-01, -1.1362e-01,  ..., -7.0628e-01,\n",
      "            -1.7239e+00, -1.0205e+00],\n",
      "           [-2.6828e+00,  1.5710e+00,  1.9603e+00,  ...,  7.9746e-01,\n",
      "            -1.1190e+00, -7.5531e-01],\n",
      "           [ 1.7509e+00,  1.4235e-01, -1.0876e+00,  ..., -1.8663e+00,\n",
      "            -1.0579e+00, -1.5921e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-8.7107e-01, -8.8743e-01,  1.3186e+00,  ...,  9.8675e-01,\n",
      "             3.2018e-01, -6.8866e-01],\n",
      "           [-1.3286e+00, -5.6104e-01, -1.5553e+00,  ..., -2.2037e+00,\n",
      "            -5.3368e-01,  5.0631e-01],\n",
      "           [-1.7817e-01,  2.0905e+00, -6.2538e-01,  ...,  1.0060e+00,\n",
      "             3.1664e-01, -5.8357e-02],\n",
      "           ...,\n",
      "           [-2.4276e-01, -4.9131e-01,  8.7350e-01,  ..., -8.5853e-01,\n",
      "            -7.9877e-02,  2.7505e-01],\n",
      "           [ 2.1218e-01,  1.5728e+00,  1.7182e+00,  ...,  1.2597e-01,\n",
      "             1.0211e+00, -3.0619e+00],\n",
      "           [ 9.0026e-01, -8.6626e-02, -4.8665e-02,  ..., -2.2410e-01,\n",
      "             6.4005e-01,  1.7153e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 1.5027e-01,  1.2586e+00,  1.3367e+00,  ...,  2.4384e-01,\n",
      "             2.2280e+00,  3.6979e-01],\n",
      "           [-6.4763e-02,  1.5956e+00, -4.7083e-02,  ...,  7.9857e-01,\n",
      "             7.5530e-01,  1.4109e+00],\n",
      "           [ 1.4833e+00, -1.2025e+00,  5.7427e-01,  ...,  4.6809e-01,\n",
      "            -1.5438e-01, -8.9240e-01],\n",
      "           ...,\n",
      "           [ 2.2063e-01, -4.6735e-01,  1.2941e+00,  ..., -4.5475e-01,\n",
      "            -7.6936e-01,  5.8183e-01],\n",
      "           [ 1.4153e-02,  8.7903e-01,  9.2582e-01,  ...,  1.5267e+00,\n",
      "             9.2871e-01,  3.2511e-02],\n",
      "           [-2.3272e-01, -5.9453e-01,  3.0572e-01,  ..., -2.7824e+00,\n",
      "             8.5071e-01, -9.7651e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 3.6027e-01, -7.7049e-01,  8.5796e-01,  ..., -1.1868e+00,\n",
      "            -4.9265e-01, -1.5314e+00],\n",
      "           [ 4.7024e-01,  2.4495e+00,  3.0613e-02,  ..., -4.0877e-01,\n",
      "            -7.5501e-04, -1.8592e-01],\n",
      "           [-7.3018e-01, -7.9361e-01,  3.5754e-01,  ...,  4.7816e-01,\n",
      "             1.9606e+00, -2.9235e-01],\n",
      "           ...,\n",
      "           [-1.4848e+00, -2.6937e-01, -2.9894e+00,  ...,  1.3067e+00,\n",
      "            -6.8110e-01,  1.2568e+00],\n",
      "           [-8.2786e-01, -9.9184e-01, -1.5566e+00,  ..., -1.3748e+00,\n",
      "            -1.0114e+00, -5.5562e-01],\n",
      "           [ 3.3063e-01, -1.0689e+00, -1.0051e+00,  ...,  2.2688e-01,\n",
      "            -1.1202e+00, -6.9185e-01]]]]]), tensor([])))]\n",
      "len:1\n"
     ]
    }
   ],
   "source": [
    "# while True에서 여러번 소환?\n",
    "def get_data_iterator_for_step(dataloader, engine, num_micro_batches=None):\n",
    "    num_micro_batches = num_micro_batches or engine.micro_batches\n",
    "    if not (engine.is_first_stage() or engine.is_last_stage()):\n",
    "        return None\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    items = [next(dataloader_iter) for _ in range(num_micro_batches)]\n",
    "    # print(items)\n",
    "    print(f'len:{len(items)}')\n",
    "    return iter(items)\n",
    "iterator = get_data_iterator_for_step(train_dataloader, model_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[[[ 0.5029, -0.6719, -0.1165,  ..., -0.2498, -1.0033,  0.0513],\n",
       "             [-0.3093, -0.9440, -0.3851,  ...,  0.6261, -0.2868, -0.3529],\n",
       "             [ 0.3373,  0.1198, -0.7629,  ...,  0.4753, -0.0343, -0.4412],\n",
       "             ...,\n",
       "             [ 0.4332,  0.6598, -0.7912,  ..., -0.1079, -1.5893, -0.2849],\n",
       "             [-0.4316,  0.9296,  0.5756,  ..., -0.3770, -0.6223,  0.0524],\n",
       "             [-1.6205,  0.1285,  0.3675,  ..., -0.5046,  0.7960, -0.6776]]],\n",
       "  \n",
       "  \n",
       "           [[[ 0.1261,  0.5905,  0.3174,  ...,  0.5060, -0.2114, -0.8570],\n",
       "             [ 1.3833,  1.4262,  0.5179,  ..., -0.3579,  0.4883,  0.3209],\n",
       "             [-0.3900, -0.0787, -0.1272,  ...,  0.3446, -0.0275,  0.9814],\n",
       "             ...,\n",
       "             [-1.0312, -0.8147, -1.0036,  ...,  0.7246, -0.3195,  0.9741],\n",
       "             [-0.1439, -0.1230,  0.8771,  ..., -0.6778, -0.1177, -0.7173],\n",
       "             [ 0.6979,  0.0600, -0.0976,  ..., -0.2695,  0.2549, -0.4113]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.7986, -0.1794,  0.0828,  ...,  0.0598,  0.0042, -0.5939],\n",
       "             [ 0.6598, -0.8077, -0.4133,  ...,  0.0201,  0.0671, -0.9179],\n",
       "             [-0.7850, -0.3504,  0.2400,  ..., -0.9535, -0.6789, -0.1005],\n",
       "             ...,\n",
       "             [ 0.6446, -0.4976, -0.4552,  ..., -0.6544, -0.6219,  0.0602],\n",
       "             [-1.2924,  0.8068,  1.1440,  ...,  0.5273, -0.1482,  0.0187],\n",
       "             [ 0.7822, -0.1770, -0.8628,  ..., -0.2682, -0.2733,  0.0959]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-0.4038, -0.4985,  0.4915,  ...,  0.2747,  0.2073, -0.2096],\n",
       "             [-0.0219,  0.1097, -0.2019,  ..., -0.5526,  0.6158,  0.8739],\n",
       "             [-0.6048,  0.4057, -0.8515,  ...,  0.2525, -0.1496, -0.2406],\n",
       "             ...,\n",
       "             [-0.5761, -0.7326,  0.1473,  ..., -0.5967, -0.4881, -0.1025],\n",
       "             [-0.2169, -0.0605,  0.8747,  ..., -0.1263,  0.1834, -1.6326],\n",
       "             [-0.5812, -0.9099, -0.1049,  ..., -0.4120,  0.5265,  0.7849]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.0144,  0.8384,  0.5255,  ..., -0.0579,  0.9677, -0.0185],\n",
       "             [-0.1735,  0.7094, -0.1544,  ...,  0.4037,  0.2498,  0.4713],\n",
       "             [ 0.7148, -0.8550,  0.1454,  ...,  0.3159,  0.0073, -0.4926],\n",
       "             ...,\n",
       "             [-0.2641, -0.4861,  0.5500,  ..., -0.5709, -0.6733,  0.0666],\n",
       "             [-0.0355, -0.0237,  0.4290,  ...,  0.7680,  0.3915, -0.1125],\n",
       "             [-0.5517, -0.4155, -0.0474,  ..., -1.7340,  0.4793, -0.6765]]],\n",
       "  \n",
       "  \n",
       "           [[[ 0.0405, -0.4959,  0.1154,  ..., -1.0264, -0.7730, -1.2635],\n",
       "             [ 0.3456,  1.0925, -0.2405,  ..., -0.3801, -0.2600, -0.3971],\n",
       "             [-0.2405, -0.4101,  0.1986,  ...,  0.1388,  1.0017, -0.2758],\n",
       "             ...,\n",
       "             [-0.2635, -0.0964, -1.5262,  ...,  0.9765,  0.6758,  1.5063],\n",
       "             [ 0.3101,  0.1164, -0.5202,  ..., -0.2049,  0.5091,  0.3371],\n",
       "             [ 0.6474,  0.0650, -0.1408,  ...,  0.7461,  0.0163, -0.0862]]]]]),\n",
       "  tensor([]),\n",
       "  tensor([569.5222]),\n",
       "  tensor([[[-0.0007, -0.0096,  0.0084,  ...,  0.0006, -0.0261, -0.0021],\n",
       "           [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
       "           [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
       "           ...,\n",
       "           [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
       "           [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
       "           [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645]]]),\n",
       "  tensor([1]),\n",
       "  tensor([]),\n",
       "  tensor([])),\n",
       " (tensor([[[[[-4.9187e-03, -2.1044e+00, -1.2807e+00,  ..., -1.5337e+00,\n",
       "              -2.8364e+00, -9.6512e-01],\n",
       "             [-1.2702e+00, -2.2536e+00, -1.2851e+00,  ...,  7.6940e-01,\n",
       "              -9.0391e-01, -1.0453e+00],\n",
       "             [-1.4647e-01, -4.9856e-01, -2.1295e+00,  ...,  2.0684e-01,\n",
       "              -5.8191e-01, -1.3024e+00],\n",
       "             ...,\n",
       "             [ 1.2071e+00,  6.4196e-01, -1.4335e+00,  ...,  4.9599e-01,\n",
       "              -1.7581e+00,  3.3206e-01],\n",
       "             [-5.9627e-01,  1.8549e+00,  1.0053e+00,  ...,  1.4863e-01,\n",
       "              -4.2155e-01,  9.5952e-01],\n",
       "             [-2.8181e+00,  3.0107e-01,  7.4274e-01,  ..., -6.2007e-02,\n",
       "               2.1703e+00, -6.3159e-01]]],\n",
       "  \n",
       "  \n",
       "           [[[-8.8992e-01, -2.1927e-01, -6.7502e-01,  ..., -2.5798e-01,\n",
       "              -1.4900e+00, -2.5302e+00],\n",
       "             [ 1.4382e+00,  1.4997e+00, -3.1326e-01,  ..., -1.6024e+00,\n",
       "              -3.3899e-01, -2.5008e-01],\n",
       "             [-2.3266e+00, -1.9346e+00, -2.2019e+00,  ..., -4.0127e-01,\n",
       "              -1.1233e+00,  9.7201e-01],\n",
       "             ...,\n",
       "             [-1.3430e+00, -1.4617e+00, -1.7527e+00,  ...,  1.1883e+00,\n",
       "              -5.4184e-01,  2.1618e+00],\n",
       "             [-6.4423e-01,  8.1667e-01,  9.9024e-01,  ..., -1.7103e+00,\n",
       "              -4.1326e-02, -1.1888e+00],\n",
       "             [ 1.7768e+00,  2.7976e-01, -4.8759e-01,  ..., -3.4870e-01,\n",
       "               2.1736e-01, -1.1196e+00]]],\n",
       "  \n",
       "  \n",
       "           [[[-7.1936e-02,  7.9007e-01,  1.9023e+00,  ...,  1.8655e+00,\n",
       "               1.8004e+00,  5.3273e-01],\n",
       "             [ 2.1929e+00,  4.0818e-01,  1.1246e+00,  ...,  1.8419e+00,\n",
       "               1.9077e+00, -2.3088e-02],\n",
       "             [ 1.2336e-02,  1.0678e+00,  2.1626e+00,  ..., -1.1594e-01,\n",
       "               2.7228e-01,  1.1707e+00],\n",
       "             ...,\n",
       "             [ 8.7385e-01, -4.1905e-01, -1.1362e-01,  ..., -7.0628e-01,\n",
       "              -1.7239e+00, -1.0205e+00],\n",
       "             [-2.6828e+00,  1.5710e+00,  1.9603e+00,  ...,  7.9746e-01,\n",
       "              -1.1190e+00, -7.5531e-01],\n",
       "             [ 1.7509e+00,  1.4235e-01, -1.0876e+00,  ..., -1.8663e+00,\n",
       "              -1.0579e+00, -1.5921e-01]]],\n",
       "  \n",
       "  \n",
       "           ...,\n",
       "  \n",
       "  \n",
       "           [[[-8.7107e-01, -8.8743e-01,  1.3186e+00,  ...,  9.8675e-01,\n",
       "               3.2018e-01, -6.8866e-01],\n",
       "             [-1.3286e+00, -5.6104e-01, -1.5553e+00,  ..., -2.2037e+00,\n",
       "              -5.3368e-01,  5.0631e-01],\n",
       "             [-1.7817e-01,  2.0905e+00, -6.2538e-01,  ...,  1.0060e+00,\n",
       "               3.1664e-01, -5.8357e-02],\n",
       "             ...,\n",
       "             [-2.4276e-01, -4.9131e-01,  8.7350e-01,  ..., -8.5853e-01,\n",
       "              -7.9877e-02,  2.7505e-01],\n",
       "             [ 2.1218e-01,  1.5728e+00,  1.7182e+00,  ...,  1.2597e-01,\n",
       "               1.0211e+00, -3.0619e+00],\n",
       "             [ 9.0026e-01, -8.6626e-02, -4.8665e-02,  ..., -2.2410e-01,\n",
       "               6.4005e-01,  1.7153e+00]]],\n",
       "  \n",
       "  \n",
       "           [[[ 1.5027e-01,  1.2586e+00,  1.3367e+00,  ...,  2.4384e-01,\n",
       "               2.2280e+00,  3.6979e-01],\n",
       "             [-6.4763e-02,  1.5956e+00, -4.7083e-02,  ...,  7.9857e-01,\n",
       "               7.5530e-01,  1.4109e+00],\n",
       "             [ 1.4833e+00, -1.2025e+00,  5.7427e-01,  ...,  4.6809e-01,\n",
       "              -1.5438e-01, -8.9240e-01],\n",
       "             ...,\n",
       "             [ 2.2063e-01, -4.6735e-01,  1.2941e+00,  ..., -4.5475e-01,\n",
       "              -7.6936e-01,  5.8183e-01],\n",
       "             [ 1.4153e-02,  8.7903e-01,  9.2582e-01,  ...,  1.5267e+00,\n",
       "               9.2871e-01,  3.2511e-02],\n",
       "             [-2.3272e-01, -5.9453e-01,  3.0572e-01,  ..., -2.7824e+00,\n",
       "               8.5071e-01, -9.7651e-01]]],\n",
       "  \n",
       "  \n",
       "           [[[ 3.6027e-01, -7.7049e-01,  8.5796e-01,  ..., -1.1868e+00,\n",
       "              -4.9265e-01, -1.5314e+00],\n",
       "             [ 4.7024e-01,  2.4495e+00,  3.0613e-02,  ..., -4.0877e-01,\n",
       "              -7.5501e-04, -1.8592e-01],\n",
       "             [-7.3018e-01, -7.9361e-01,  3.5754e-01,  ...,  4.7816e-01,\n",
       "               1.9606e+00, -2.9235e-01],\n",
       "             ...,\n",
       "             [-1.4848e+00, -2.6937e-01, -2.9894e+00,  ...,  1.3067e+00,\n",
       "              -6.8110e-01,  1.2568e+00],\n",
       "             [-8.2786e-01, -9.9184e-01, -1.5566e+00,  ..., -1.3748e+00,\n",
       "              -1.0114e+00, -5.5562e-01],\n",
       "             [ 3.3063e-01, -1.0689e+00, -1.0051e+00,  ...,  2.2688e-01,\n",
       "              -1.1202e+00, -6.9185e-01]]]]]),\n",
       "  tensor([])))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((tensor([[[[[ 5.3262e-01,  4.9765e-01,  5.9124e-01,  ...,  5.3911e-01,\n",
      "             6.2714e-01,  5.3613e-01],\n",
      "           [ 4.7501e-01,  2.3452e-01,  3.3921e-01,  ...,  1.7577e-01,\n",
      "             1.4129e-01,  2.3345e-01],\n",
      "           [ 3.4892e-01,  3.0049e-01,  4.0883e-01,  ...,  3.5826e-01,\n",
      "             2.9869e-01,  3.4040e-01],\n",
      "           ...,\n",
      "           [-3.0232e-01,  2.8215e-01, -1.1968e-03,  ..., -5.7461e-01,\n",
      "            -5.2173e-01, -3.4637e-01],\n",
      "           [-1.3405e-01, -1.7874e-01, -8.6372e-03,  ..., -4.6875e-01,\n",
      "            -5.7241e-01, -3.0967e-01],\n",
      "           [ 7.4727e-03, -8.0741e-02, -9.0266e-02,  ..., -4.3962e-01,\n",
      "            -4.0525e-01, -3.6172e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 5.9593e-01,  6.4836e-01,  5.8073e-01,  ...,  7.2969e-01,\n",
      "             5.8419e-01,  5.4942e-01],\n",
      "           [ 4.9547e-01,  4.8948e-01,  6.9749e-01,  ...,  6.2860e-01,\n",
      "             4.6664e-01,  4.9095e-01],\n",
      "           [ 9.1137e-01,  1.1060e+00,  1.1997e+00,  ...,  7.6577e-01,\n",
      "             5.8604e-01,  5.0604e-01],\n",
      "           ...,\n",
      "           [-2.9380e-01, -3.3059e-02, -7.5140e-02,  ..., -2.1373e-01,\n",
      "             1.8493e-01, -3.0606e-01],\n",
      "           [ 2.5606e-01, -4.6351e-01,  3.2273e-01,  ...,  1.6225e-02,\n",
      "             2.8928e-01, -7.0245e-03],\n",
      "           [-2.1908e-01, -1.2659e-01,  1.5680e-01,  ...,  1.8018e-01,\n",
      "            -6.6866e-03,  9.7862e-02]]],\n",
      "\n",
      "\n",
      "         [[[-7.5536e-01, -6.5079e-01, -1.0352e+00,  ..., -8.1459e-01,\n",
      "            -1.0643e+00, -9.0367e-01],\n",
      "           [-5.7388e-01, -1.0413e+00, -1.0145e+00,  ..., -9.5265e-01,\n",
      "            -1.0817e+00, -8.7552e-01],\n",
      "           [-7.8172e-01, -7.8509e-01, -9.9048e-01,  ..., -8.6303e-01,\n",
      "            -8.7589e-01, -7.5085e-01],\n",
      "           ...,\n",
      "           [ 1.1732e-01, -1.7933e-01, -3.4366e-01,  ...,  1.4142e-01,\n",
      "             3.1997e-01,  7.1873e-01],\n",
      "           [ 2.5808e-01, -7.2622e-02,  5.9704e-02,  ...,  4.3177e-02,\n",
      "             3.1702e-01,  7.5105e-01],\n",
      "           [-2.0390e-01, -2.1502e-01, -2.5791e-01,  ...,  3.3816e-01,\n",
      "             2.0686e-01, -5.7593e-02]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 1.9369e-01,  6.8301e-02, -3.1116e-01,  ..., -1.1609e-01,\n",
      "             1.2608e-01,  1.3038e-01],\n",
      "           [ 8.3077e-01,  3.0292e-01,  4.0100e-01,  ...,  7.9216e-01,\n",
      "             7.7874e-01,  4.9935e-01],\n",
      "           [-2.7665e-01, -4.7421e-01, -4.0415e-01,  ..., -2.8092e-01,\n",
      "            -4.4345e-01, -9.5321e-02],\n",
      "           ...,\n",
      "           [-4.1501e-01, -4.0653e-01, -3.4404e-01,  ..., -3.0593e-01,\n",
      "             5.7284e-02, -5.1441e-01],\n",
      "           [-4.3912e-01, -8.4037e-01, -1.5237e-01,  ..., -1.6277e-01,\n",
      "            -2.4764e-01, -3.4146e-01],\n",
      "           [-1.0022e+00, -7.9013e-01, -9.5446e-02,  ..., -4.3684e-02,\n",
      "            -1.1059e-01, -9.9951e-02]]],\n",
      "\n",
      "\n",
      "         [[[-3.8066e-02,  1.0086e-01, -2.2551e-01,  ..., -2.1803e-01,\n",
      "            -2.8032e-01, -2.3205e-01],\n",
      "           [-1.4432e-01, -2.0057e-01, -1.5892e-01,  ..., -8.7669e-02,\n",
      "            -3.1871e-01, -2.0591e-01],\n",
      "           [-1.1286e-01, -1.4256e-01, -9.0401e-02,  ...,  9.5751e-02,\n",
      "             4.6124e-02,  6.3237e-02],\n",
      "           ...,\n",
      "           [-4.1401e-01, -1.4486e-01, -2.3341e-01,  ..., -2.6529e-01,\n",
      "             3.6954e-02, -3.6106e-01],\n",
      "           [-2.7040e-02, -4.7974e-01, -3.3995e-02,  ..., -1.4857e-01,\n",
      "            -2.2461e-02, -3.5811e-01],\n",
      "           [-3.8813e-01, -1.2168e-01, -2.4140e-01,  ..., -3.1479e-02,\n",
      "            -1.0018e-01, -3.0554e-01]]],\n",
      "\n",
      "\n",
      "         [[[-1.1500e-01, -1.5325e-02, -3.6546e-01,  ..., -2.7878e-01,\n",
      "            -4.0238e-01, -3.6526e-01],\n",
      "           [ 1.1635e-01, -1.6704e-01, -2.6076e-01,  ..., -1.7993e-01,\n",
      "            -2.2004e-01, -2.1635e-01],\n",
      "           [ 1.9310e-01,  1.7995e-01,  1.2879e-01,  ..., -7.0812e-02,\n",
      "            -6.7800e-02, -1.5157e-02],\n",
      "           ...,\n",
      "           [ 6.3266e-01,  7.6749e-02,  2.1575e-01,  ...,  8.5260e-01,\n",
      "             6.7112e-01,  7.6408e-01],\n",
      "           [ 7.6073e-01,  6.8104e-01,  3.3953e-01,  ...,  5.0852e-01,\n",
      "             7.5512e-01,  6.7069e-01],\n",
      "           [ 4.8739e-01,  6.5535e-01,  4.2229e-01,  ...,  5.5483e-01,\n",
      "             5.9541e-01,  3.2103e-01]]]]]), tensor([]), tensor([24.6559]), tensor([[[-0.0007, -0.0096,  0.0084,  ...,  0.0006, -0.0261, -0.0021],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645]]]), tensor([1]), tensor([]), tensor([])), (tensor([[[[[ 2.9244e-01, -1.3779e+00, -6.8075e-01,  ..., -1.2567e+00,\n",
      "            -1.1901e-01, -5.3774e-01],\n",
      "           [ 1.1801e+00, -1.7510e+00,  7.5326e-01,  ..., -1.1708e+00,\n",
      "            -7.6634e-01, -2.3223e+00],\n",
      "           [-2.2635e-02, -1.2221e+00,  2.0757e+00,  ..., -4.9659e-01,\n",
      "             1.1538e+00, -4.2148e-01],\n",
      "           ...,\n",
      "           [-6.0254e-01,  1.4360e-01, -1.0230e+00,  ...,  2.1520e-01,\n",
      "             1.7981e+00,  1.5370e+00],\n",
      "           [ 6.9608e-02, -1.9219e-01, -3.4229e-01,  ...,  2.3918e+00,\n",
      "            -4.9574e-01,  2.0240e+00],\n",
      "           [-6.3335e-01, -7.0138e-01, -5.3481e-01,  ...,  1.7362e-01,\n",
      "             3.2742e-01,  1.5920e+00]]],\n",
      "\n",
      "\n",
      "         [[[-1.5579e+00, -6.8641e-01, -9.3626e-01,  ...,  2.6057e-01,\n",
      "             5.6193e-01,  8.5704e-02],\n",
      "           [-1.3558e+00, -2.1084e+00,  4.4644e-04,  ..., -1.2096e+00,\n",
      "            -1.8821e+00,  8.4248e-01],\n",
      "           [-1.0307e+00, -2.9983e-01, -1.1006e+00,  ...,  8.0666e-01,\n",
      "            -9.8067e-01, -9.4392e-01],\n",
      "           ...,\n",
      "           [ 4.0123e-01,  3.4414e-01,  8.7224e-01,  ...,  1.0972e+00,\n",
      "             5.0153e-01,  2.3099e+00],\n",
      "           [ 6.1610e-01,  2.0316e+00,  9.3875e-01,  ...,  2.4271e-02,\n",
      "             1.5737e+00, -1.7014e+00],\n",
      "           [ 1.3056e+00,  1.7021e+00, -4.5467e-01,  ...,  5.3102e-01,\n",
      "             1.3394e-02, -2.2320e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.1548e-01,  1.2507e+00,  2.0188e+00,  ...,  2.0683e+00,\n",
      "             2.1436e-01, -9.5940e-01],\n",
      "           [ 1.2508e+00,  8.6737e-01,  2.2106e+00,  ...,  3.2291e-01,\n",
      "            -2.1204e-01,  2.8070e+00],\n",
      "           [ 7.2572e-01,  2.6925e+00, -3.7135e-01,  ..., -3.3922e-01,\n",
      "             1.3308e+00, -1.3903e-01],\n",
      "           ...,\n",
      "           [-6.4343e-01,  1.7781e+00,  3.5607e-01,  ..., -4.2428e-01,\n",
      "            -1.2623e+00, -1.1099e+00],\n",
      "           [-4.0633e-01,  6.2263e-01,  3.5387e-01,  ..., -1.2530e+00,\n",
      "             2.4409e-01, -1.5521e+00],\n",
      "           [-1.0681e+00,  1.8812e+00,  5.4678e-02,  ..., -8.5434e-01,\n",
      "             1.0788e+00, -1.7642e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-3.4133e-01,  1.2964e+00, -9.7235e-01,  ...,  2.5474e-01,\n",
      "             1.3987e-01, -7.3270e-01],\n",
      "           [-2.3277e-01,  1.5991e+00, -1.0771e+00,  ..., -8.2738e-01,\n",
      "            -4.7183e-01, -1.9511e+00],\n",
      "           [ 5.4659e-01,  1.1014e+00,  8.1706e-01,  ..., -4.9910e-01,\n",
      "             4.8513e-01,  1.1432e+00],\n",
      "           ...,\n",
      "           [ 4.8018e-01,  8.8134e-02,  9.2866e-01,  ..., -1.7981e+00,\n",
      "            -7.6984e-01,  4.9413e-01],\n",
      "           [-8.0044e-01,  4.0194e-01, -8.3588e-01,  ...,  2.3671e-01,\n",
      "            -7.5683e-01, -1.2213e+00],\n",
      "           [-5.8229e-01,  1.6633e+00,  2.4328e-01,  ...,  1.1625e-01,\n",
      "             5.2335e-01, -9.4793e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 2.9649e+00,  1.1626e+00, -1.2021e+00,  ..., -1.6807e+00,\n",
      "             1.0718e-01,  3.6179e-01],\n",
      "           [ 4.6104e-01,  1.6281e-01, -8.5253e-02,  ...,  1.2987e+00,\n",
      "            -3.6850e-01,  5.3528e-01],\n",
      "           [ 2.8921e-01, -7.6324e-01, -6.9053e-01,  ..., -1.6127e+00,\n",
      "            -2.6844e-01,  1.1199e+00],\n",
      "           ...,\n",
      "           [-1.3711e+00,  3.1423e+00,  1.3962e-01,  ...,  1.4613e+00,\n",
      "             2.5192e+00, -6.8729e-01],\n",
      "           [ 4.0228e-01,  5.7419e-01,  7.8847e-01,  ..., -1.2963e+00,\n",
      "             1.5036e+00, -2.3839e+00],\n",
      "           [ 5.7925e-01,  1.2813e-01,  4.5048e-02,  ...,  1.8783e-01,\n",
      "            -3.9346e-01,  5.4028e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 1.0728e+00, -7.4598e-01,  2.6367e-01,  ..., -1.4048e-01,\n",
      "            -9.5631e-01,  1.0379e+00],\n",
      "           [ 4.0927e-01,  2.8807e+00, -6.9037e-01,  ..., -9.8175e-01,\n",
      "             1.0878e+00,  7.1796e-01],\n",
      "           [ 1.2814e-01,  1.0610e-01,  1.8159e+00,  ...,  2.9638e-01,\n",
      "             2.4558e-01,  5.8643e-02],\n",
      "           ...,\n",
      "           [ 1.6012e+00, -3.2529e-01, -1.3609e-01,  ..., -1.3939e+00,\n",
      "             3.8406e-02, -1.2581e+00],\n",
      "           [-1.0947e+00,  6.3525e-01, -1.7147e+00,  ...,  1.5887e-01,\n",
      "            -7.6332e-01, -7.4045e-01],\n",
      "           [ 7.7450e-01,  6.6008e-01, -3.6480e-01,  ...,  2.0353e-01,\n",
      "             1.2514e+00, -7.0657e-01]]]]]), tensor([])))]\n",
      "len:1\n"
     ]
    }
   ],
   "source": [
    "iterator = get_data_iterator_for_step(train_dataloader, model_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((tensor([[[[[ 0.5744, -0.1828,  0.8496,  ...,  1.2347,  0.8520,  0.3122],\n",
      "           [ 0.6717,  0.3666,  0.3638,  ...,  0.9949,  0.6811,  0.1449],\n",
      "           [-0.1916,  0.0726, -0.3867,  ...,  0.0361,  0.2259,  0.0833],\n",
      "           ...,\n",
      "           [-0.0026,  0.8182,  0.0270,  ..., -0.1506, -0.5610, -0.4067],\n",
      "           [ 0.1010,  0.2583,  0.8889,  ..., -0.4869,  0.1156, -0.4459],\n",
      "           [ 0.6933,  0.7526,  0.4905,  ..., -0.7324, -0.2341, -0.3625]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0896,  0.0279,  0.1084,  ..., -0.1409, -0.2226, -0.4556],\n",
      "           [ 0.4602, -0.2805,  0.4794,  ..., -0.2197,  0.5158,  0.4718],\n",
      "           [ 0.6086,  0.4471,  0.2602,  ...,  1.1807,  0.0458,  0.3953],\n",
      "           ...,\n",
      "           [-0.4304,  0.1101,  0.4628,  ...,  0.0415, -0.0827, -0.5299],\n",
      "           [-0.3249, -0.6434,  0.6133,  ..., -0.3774, -0.0131, -0.6909],\n",
      "           [-0.7122, -0.7202, -0.7741,  ...,  0.3204,  0.2405,  0.5310]]],\n",
      "\n",
      "\n",
      "         [[[-0.3077,  0.1747, -0.5024,  ..., -0.5817,  0.2240, -0.1477],\n",
      "           [-0.6240, -0.6751, -0.3284,  ..., -1.1124, -1.0684, -0.1124],\n",
      "           [-0.5218, -0.1642, -1.1154,  ...,  0.0354, -0.2258, -0.6146],\n",
      "           ...,\n",
      "           [ 0.6320, -0.6069,  0.2991,  ..., -0.3992,  0.9345, -0.3831],\n",
      "           [ 0.3892,  0.9780,  1.0495,  ..., -0.5572,  1.2143,  0.1890],\n",
      "           [-0.7299,  0.1718,  1.1766,  ...,  1.0016,  0.9505,  0.1881]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1143,  0.4534, -0.2385,  ..., -0.0679, -0.3398,  0.0599],\n",
      "           [ 0.6596,  0.7643,  1.0554,  ...,  0.5836,  0.0615,  0.2003],\n",
      "           [-0.8049, -0.6262, -0.5971,  ..., -0.5387, -0.8081, -0.2073],\n",
      "           ...,\n",
      "           [-0.1726, -0.5717,  0.1775,  ..., -0.5274, -0.4544, -0.4338],\n",
      "           [-0.0677, -0.5180,  0.6981,  ...,  0.2655, -0.1276, -0.7359],\n",
      "           [-0.4985, -1.1558, -0.7351,  ..., -0.2481, -0.5620,  0.1295]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0424,  0.2128, -0.2857,  ..., -0.5647, -0.1570, -0.0186],\n",
      "           [ 0.7783, -0.7020, -0.0954,  ...,  1.0836, -0.9314, -0.3876],\n",
      "           [-0.0592,  0.0084,  0.4654,  ...,  0.7654,  0.1174, -0.2624],\n",
      "           ...,\n",
      "           [ 0.4082, -0.7153, -0.3783,  ..., -0.0374,  0.8494, -0.2227],\n",
      "           [-0.0251, -0.4771,  0.5004,  ...,  0.4616, -0.5473,  0.5187],\n",
      "           [ 0.2745, -0.0870,  0.2398,  ..., -0.5664,  0.8329, -0.1298]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2297, -0.6815,  0.4735,  ...,  0.0798,  0.1223, -1.1166],\n",
      "           [-0.0494,  0.8353,  0.1876,  ...,  0.1361, -0.1348, -0.4397],\n",
      "           [-0.3443, -0.7099, -0.4354,  ...,  1.0415,  0.3635,  0.5291],\n",
      "           ...,\n",
      "           [-0.4037,  0.3385,  0.0382,  ...,  0.6393,  0.8029,  1.0280],\n",
      "           [ 0.2430,  0.1142,  0.2910,  ...,  0.5171,  0.4067, -0.2775],\n",
      "           [ 0.6299,  0.2014,  0.4289,  ..., -0.0289, -0.2622,  0.0368]]]]]), tensor([]), tensor([477.3527]), tensor([[[-0.0007, -0.0096,  0.0084,  ...,  0.0006, -0.0261, -0.0021],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         ...,\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645],\n",
      "         [ 0.0025, -0.0225,  0.0211,  ...,  0.0016, -0.0013, -0.0645]]]), tensor([1]), tensor([]), tensor([])), (tensor([[[[[ 0.0662, -1.4487,  0.4872,  ...,  1.4057,  0.6727, -0.4223],\n",
      "           [ 0.4693,  0.1289,  0.1953,  ...,  1.6637,  1.1058, -0.2902],\n",
      "           [-1.2896, -0.6234, -1.5905,  ..., -0.4913, -0.1079, -0.4850],\n",
      "           ...,\n",
      "           [ 0.5744,  1.2076,  0.0047,  ...,  0.6240,  0.1841,  0.0268],\n",
      "           [ 0.3863,  0.8817,  1.8355,  ..., -0.0385,  1.2760,  0.1687],\n",
      "           [ 1.4203,  1.7308,  1.1042,  ..., -0.4297,  0.5670, -0.0108]]],\n",
      "\n",
      "\n",
      "         [[[-1.0848, -1.3486, -0.8678,  ..., -1.6642, -1.6382, -1.9864],\n",
      "           [-0.1498, -1.3281, -0.1667,  ..., -1.7572,  0.0139,  0.1372],\n",
      "           [-0.5073, -0.9828, -1.5450,  ...,  1.0512, -1.2175,  0.0712],\n",
      "           ...,\n",
      "           [-0.4699,  0.1762,  0.9624,  ...,  0.7141, -0.6409, -0.5216],\n",
      "           [-1.1942, -0.1778,  0.8859,  ..., -0.9097, -0.2334, -0.7369],\n",
      "           [-1.0617, -1.3500, -2.1213,  ...,  0.4049,  0.1635,  1.2373]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9715,  1.7480,  1.1117,  ...,  1.0266,  2.6534,  1.6983],\n",
      "           [-0.0236,  0.8055,  1.5991,  ..., -0.0913, -0.0322,  1.8577],\n",
      "           [ 0.5812,  1.7563, -0.4339,  ...,  1.9805,  1.3451,  0.2983],\n",
      "           ...,\n",
      "           [ 1.0230, -0.7766,  1.4465,  ..., -0.6070,  0.5617, -2.3960],\n",
      "           [ 0.2680,  2.2483,  2.1740,  ..., -1.2421,  1.0889, -0.6321],\n",
      "           [-1.1283,  0.8605,  2.9680,  ...,  0.6065,  1.3999,  0.2522]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0430,  1.1068, -0.1143,  ...,  0.2808, -0.6596, -0.0068],\n",
      "           [-0.5105,  0.8098,  0.8268,  ..., -0.4736, -1.5394, -0.9706],\n",
      "           [-0.7126, -0.1631, -0.1298,  ..., -0.4428, -0.9815, -0.2737],\n",
      "           ...,\n",
      "           [ 0.6310, -0.0730,  1.1887,  ..., -1.2455, -1.0377,  1.2107],\n",
      "           [ 0.7228,  0.7225,  1.5852,  ...,  0.3619,  0.1197, -0.2144],\n",
      "           [ 1.0808, -0.6304, -1.3633,  ..., -0.0245, -0.3361,  0.9842]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4007,  0.2822, -0.0626,  ..., -0.6899,  0.2793,  0.4396],\n",
      "           [ 1.9502, -0.8872,  0.3755,  ...,  2.3503, -1.6024, -0.4533],\n",
      "           [ 0.3132,  0.4834,  1.4319,  ...,  1.5089,  0.1132, -0.3318],\n",
      "           ...,\n",
      "           [ 1.6175, -1.0722, -0.3631,  ...,  0.8740,  1.7494,  0.2554],\n",
      "           [ 0.0374,  0.0145,  1.1131,  ...,  1.7156, -1.1225,  1.7602],\n",
      "           [ 1.4752,  0.0370,  0.9511,  ..., -0.8210,  1.9703,  0.1089]]],\n",
      "\n",
      "\n",
      "         [[[ 0.8071, -1.2255,  1.6764,  ...,  0.9099,  1.0636, -1.5101],\n",
      "           [-0.2002,  2.3581,  0.9585,  ...,  0.5964,  0.3153, -0.3774],\n",
      "           [-1.1176, -1.5322, -0.9661,  ...,  2.1073,  0.7202,  1.2172],\n",
      "           ...,\n",
      "           [-2.0353,  0.5268, -0.3993,  ...,  1.2437, -0.4856,  0.2129],\n",
      "           [-0.9877, -1.1253, -0.1370,  ...,  0.2731, -1.4905, -2.7770],\n",
      "           [ 0.2677, -0.8984,  0.0394,  ..., -1.7801, -2.5551, -1.9367]]]]]), tensor([])))]\n",
      "len:1\n"
     ]
    }
   ],
   "source": [
    "iterator = get_data_iterator_for_step(train_dataloader, model_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "itlist = list(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((tensor([[[[[ 0.3448,  0.5015,  1.2092,  ...,  0.5659,  0.5613, -0.0419],\n",
       "              [ 1.0114,  0.2145, -0.0619,  ...,  0.4562, -0.5243,  0.4744],\n",
       "              [-0.1145, -0.0342,  0.6882,  ...,  0.5282,  0.4754, -0.9076],\n",
       "              ...,\n",
       "              [ 0.5800,  0.1906, -0.3855,  ...,  0.0986, -0.4552, -0.7416],\n",
       "              [-0.0584,  0.3256,  0.3482,  ...,  0.6530, -0.3901,  0.0379],\n",
       "              [ 0.1577,  0.3543,  1.1418,  ..., -0.2288, -0.0543,  0.2668]]],\n",
       "   \n",
       "   \n",
       "            [[[-0.5139,  0.6462,  0.1930,  ...,  1.1360,  0.3455,  0.0742],\n",
       "              [-0.3564,  0.2595,  0.0198,  ...,  0.1526,  0.2134, -0.1726],\n",
       "              [ 0.3350,  0.8517,  0.9200,  ...,  0.3818,  0.0239,  0.1575],\n",
       "              ...,\n",
       "              [-0.0751,  0.3615,  0.1785,  ..., -0.6599, -0.2503,  0.6783],\n",
       "              [ 0.5326, -0.9518,  0.2780,  ...,  1.0535,  0.1717, -0.6223],\n",
       "              [ 0.1177,  0.1694,  0.4905,  ...,  0.3653, -0.0230, -0.6489]]],\n",
       "   \n",
       "   \n",
       "            [[[-0.1527,  0.3296, -0.3397,  ..., -0.3063, -0.6688, -1.0034],\n",
       "              [-0.3935, -0.8665, -1.1399,  ..., -0.7754, -0.1788, -0.9513],\n",
       "              [-0.3045, -1.0040, -0.8661,  ..., -0.1487, -0.6985, -0.2331],\n",
       "              ...,\n",
       "              [ 0.4406,  0.4410, -0.8032,  ...,  0.8722,  0.7645,  0.1268],\n",
       "              [ 0.7604,  0.2657,  0.1301,  ..., -0.3519,  0.1505, -0.3628],\n",
       "              [-0.3970,  0.1484, -0.9676,  ...,  1.1248,  0.0451,  0.7593]]],\n",
       "   \n",
       "   \n",
       "            ...,\n",
       "   \n",
       "   \n",
       "            [[[ 0.3641, -0.2901,  0.2494,  ..., -0.3433,  0.4351, -0.3999],\n",
       "              [ 0.6871,  0.6682,  0.3798,  ..., -0.4131,  0.2969,  0.2982],\n",
       "              [ 0.2001, -0.5910,  0.0026,  ..., -0.0289, -1.0415, -0.2966],\n",
       "              ...,\n",
       "              [ 0.6174, -0.5783,  0.9051,  ...,  0.1780, -0.2314,  0.0854],\n",
       "              [-0.5757,  0.0633,  0.6802,  ..., -0.4142, -0.3682, -0.3021],\n",
       "              [-1.1232, -0.4517, -0.3767,  ...,  0.0638,  0.2157, -0.4234]]],\n",
       "   \n",
       "   \n",
       "            [[[ 1.0095,  0.5221, -0.1100,  ..., -0.0884, -0.5136,  0.0484],\n",
       "              [-0.3768, -0.3090, -0.3697,  ..., -0.0616,  0.2950, -0.3404],\n",
       "              [ 0.9297, -0.3297, -0.5020,  ...,  0.0708, -0.3563,  0.0122],\n",
       "              ...,\n",
       "              [-0.0512,  1.3123,  0.6404,  ...,  0.0964, -0.8334,  0.0772],\n",
       "              [ 0.0209, -0.1904,  0.2589,  ..., -1.2912,  0.3299,  0.6599],\n",
       "              [-0.5252,  0.3337,  0.0620,  ...,  0.0364, -0.3951, -0.2014]]],\n",
       "   \n",
       "   \n",
       "            [[[-0.4250, -0.3024, -0.2823,  ..., -0.2799, -1.2278, -0.5419],\n",
       "              [ 0.1623,  0.2942, -1.3964,  ..., -0.4180, -0.1018, -0.7342],\n",
       "              [-0.1080,  0.4263, -0.0266,  ..., -0.0872,  0.1090,  0.1247],\n",
       "              ...,\n",
       "              [-0.0894,  0.0447, -0.4255,  ...,  0.8004,  1.3639,  1.3644],\n",
       "              [ 0.4201, -0.3476, -0.3812,  ...,  1.0501,  0.7412,  0.2622],\n",
       "              [ 0.3750,  0.8007, -0.2216,  ..., -0.0942,  0.3005,  0.6831]]]]]),\n",
       "   tensor([]),\n",
       "   tensor([437.5417]),\n",
       "   tensor([[[-0.0007, -0.0096,  0.0083,  ...,  0.0006, -0.0262, -0.0022],\n",
       "            [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "            [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "            ...,\n",
       "            [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "            [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664],\n",
       "            [ 0.0025, -0.0232,  0.0214,  ...,  0.0016, -0.0006, -0.0664]]]),\n",
       "   tensor([1]),\n",
       "   tensor([]),\n",
       "   tensor([])),\n",
       "  (tensor([[[[[-4.3738e-01, -6.0269e-02,  1.4633e+00,  ..., -6.7772e-03,\n",
       "               -6.4594e-03, -1.2602e+00],\n",
       "              [ 1.2838e+00, -2.6491e-01, -8.7733e-01,  ...,  6.8307e-01,\n",
       "               -1.5503e+00,  3.2960e-01],\n",
       "              [-1.1490e+00, -1.0376e+00,  6.6986e-01,  ...,  6.3568e-01,\n",
       "                5.4595e-01, -2.8305e+00],\n",
       "              ...,\n",
       "              [ 1.9375e+00, -1.2291e-01, -9.3550e-01,  ...,  1.2885e+00,\n",
       "                1.9701e-01, -6.9314e-01],\n",
       "              [ 4.9481e-02,  1.1264e+00,  7.6644e-01,  ...,  2.6556e+00,\n",
       "                1.2488e-01,  8.2157e-01],\n",
       "              [ 3.2918e-01,  9.6494e-01,  2.7076e+00,  ...,  6.1933e-01,\n",
       "                8.9226e-01,  1.0137e+00]]],\n",
       "   \n",
       "   \n",
       "            [[[-2.6517e+00, -4.8330e-02, -5.6131e-01,  ...,  1.0465e+00,\n",
       "               -6.1594e-01, -1.0191e+00],\n",
       "              [-2.0745e+00, -3.0695e-01, -1.3557e+00,  ..., -1.0269e+00,\n",
       "               -7.1562e-01, -1.3605e+00],\n",
       "              [-1.1790e+00, -1.9070e-01, -4.0692e-01,  ..., -7.8013e-01,\n",
       "               -1.5194e+00, -6.7107e-01],\n",
       "              ...,\n",
       "              [ 3.9855e-01,  7.7660e-01,  3.8915e-01,  ..., -2.6642e-01,\n",
       "               -7.2686e-01,  1.8885e+00],\n",
       "              [ 7.1168e-01, -8.6975e-01,  2.0102e-01,  ...,  2.2137e+00,\n",
       "               -3.4217e-01, -9.2590e-01],\n",
       "              [ 7.3929e-01,  5.4334e-01,  5.7215e-01,  ...,  7.5261e-01,\n",
       "               -1.2240e+00, -1.4792e+00]]],\n",
       "   \n",
       "   \n",
       "            [[[ 1.4128e+00,  2.2993e+00,  1.5072e+00,  ...,  1.7522e+00,\n",
       "                8.3785e-01, -1.5743e-01],\n",
       "              [ 4.4503e-01,  3.7056e-01, -2.5755e-01,  ...,  6.6017e-01,\n",
       "                2.0590e+00,  1.4735e-01],\n",
       "              [ 8.5793e-01,  3.2172e-03,  3.0898e-01,  ...,  1.7360e+00,\n",
       "                4.7909e-01,  1.2172e+00],\n",
       "              ...,\n",
       "              [ 6.8389e-01,  1.5535e+00, -9.2411e-01,  ...,  1.8813e+00,\n",
       "                1.1199e-01, -1.4056e+00],\n",
       "              [ 1.1568e+00,  8.3240e-01,  2.7134e-01,  ..., -9.3943e-01,\n",
       "               -8.0139e-01, -2.0319e+00],\n",
       "              [-4.8401e-01,  8.7443e-01, -1.6772e+00,  ...,  1.1947e+00,\n",
       "               -6.9383e-01,  8.7889e-01]]],\n",
       "   \n",
       "   \n",
       "            ...,\n",
       "   \n",
       "   \n",
       "            [[[ 5.6141e-01, -5.6589e-01,  1.1858e+00,  ..., -3.4847e-01,\n",
       "                1.0350e+00, -9.7634e-01],\n",
       "              [-3.9528e-01,  5.8307e-01, -4.0144e-01,  ..., -2.9193e+00,\n",
       "               -1.3125e+00, -1.0038e+00],\n",
       "              [ 1.8696e+00,  2.9192e-01,  1.3249e+00,  ...,  4.9701e-01,\n",
       "               -1.8755e+00, -4.5321e-01],\n",
       "              ...,\n",
       "              [ 2.4770e+00, -6.7435e-02,  2.9637e+00,  ...,  4.3517e-01,\n",
       "               -8.0919e-01,  1.6108e+00],\n",
       "              [-4.4353e-01,  2.1347e+00,  1.6733e+00,  ..., -5.3169e-01,\n",
       "               -1.1235e+00,  1.8003e-01],\n",
       "              [-2.6133e-01,  9.5450e-01, -6.5234e-01,  ..., -2.3774e-01,\n",
       "                4.9500e-01, -3.8997e-01]]],\n",
       "   \n",
       "   \n",
       "            [[[ 2.5941e+00,  9.6501e-01,  2.0798e-01,  ...,  3.1137e-01,\n",
       "               -5.4396e-01,  6.5794e-01],\n",
       "              [-5.5201e-01, -8.8745e-02, -3.9588e-01,  ..., -7.5572e-03,\n",
       "                1.0259e+00, -4.6402e-01],\n",
       "              [ 2.4584e+00, -3.8364e-01, -6.7525e-01,  ...,  1.5194e-02,\n",
       "               -1.0174e+00,  8.8104e-02],\n",
       "              ...,\n",
       "              [ 6.9573e-01,  3.4331e+00,  1.9233e+00,  ...,  1.2167e+00,\n",
       "               -1.9550e+00,  7.4536e-01],\n",
       "              [ 1.3103e-01,  6.6455e-01,  6.6369e-01,  ..., -2.3723e+00,\n",
       "                7.6301e-01,  1.6894e+00],\n",
       "              [-2.0430e-01,  1.0044e+00,  6.3652e-01,  ...,  4.4260e-01,\n",
       "               -1.1130e+00, -4.8467e-01]]],\n",
       "   \n",
       "   \n",
       "            [[[-6.0145e-01, -4.8082e-01,  1.4363e-01,  ...,  1.8562e-01,\n",
       "               -1.9401e+00, -3.2955e-01],\n",
       "              [ 1.6402e-01,  1.3344e+00, -2.4322e+00,  ..., -5.1766e-01,\n",
       "                4.4575e-01, -1.0614e+00],\n",
       "              [-5.1881e-01,  8.7866e-01, -2.2318e-01,  ..., -2.1433e-01,\n",
       "                1.7993e-01,  5.0623e-01],\n",
       "              ...,\n",
       "              [-1.5683e+00, -1.2510e-01, -1.4943e+00,  ...,  7.7629e-01,\n",
       "                1.2964e+00,  1.2411e+00],\n",
       "              [-6.9151e-01, -2.2711e+00, -1.6948e+00,  ...,  1.3509e+00,\n",
       "               -6.3718e-01, -1.2880e+00],\n",
       "              [-2.8422e-01,  3.6453e-01, -1.4680e+00,  ..., -1.2297e+00,\n",
       "               -6.9860e-01,  2.1188e-01]]]]]),\n",
       "   tensor([])))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "step = 1\n",
    "# make sure to do this before calling model_engine.set_dataloader(), as that method creates an iterator\n",
    "# which starts creating dataloader internal state\n",
    "if resume_from_checkpoint:\n",
    "    load_path, client_state = model_engine.load_checkpoint(\n",
    "        run_dir,\n",
    "        load_module_strict=False,\n",
    "        load_lr_scheduler_states='force_constant_lr' not in config,\n",
    "    )\n",
    "    dist.barrier()  # just so the print below doesn't get swamped\n",
    "    assert load_path is not None\n",
    "    train_dataloader.load_state_dict(client_state['custom_loader'])\n",
    "    step = client_state['step'] + 1\n",
    "    del client_state\n",
    "    if is_main_process():\n",
    "        print(f'Resuming training from checkpoint. Resuming at epoch: {train_dataloader.epoch}, step: {step}')\n",
    "\n",
    "if 'force_constant_lr' in config:\n",
    "    model_engine.lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = config['force_constant_lr']\n",
    "\n",
    "steps_per_epoch = len(train_dataloader) // model_engine.gradient_accumulation_steps()\n",
    "model_engine.total_steps = steps_per_epoch * config['epochs']\n",
    "\n",
    "eval_dataloaders = {\n",
    "    # Set num_dataloader_workers=0 so dataset iteration is completely deterministic.\n",
    "    # We want the exact same noise for each image, each time, for a stable validation loss.\n",
    "    name: dataset_util.PipelineDataLoader(eval_data, model_engine, config['eval_gradient_accumulation_steps'], model, num_dataloader_workers=0)\n",
    "    for name, eval_data in eval_data_map.items()\n",
    "}\n",
    "from saver import Saver\n",
    "is_adapter = False\n",
    "epoch = train_dataloader.epoch\n",
    "tb_writer = SummaryWriter(log_dir=run_dir) if is_main_process() else None\n",
    "saver = Saver(args, config, is_adapter, run_dir, model, train_dataloader, model_engine, pipeline_model)\n",
    "\n",
    "disable_block_swap_for_eval = config.get('disable_block_swap_for_eval', False)\n",
    "if config['eval_before_first_step'] and not resume_from_checkpoint:\n",
    "    evaluate(model, model_engine, eval_dataloaders, tb_writer, 0, config['eval_gradient_accumulation_steps'], disable_block_swap_for_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 06:06:02,542] [INFO] [logging.py:107:log_dist] [Rank 0] step=1, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 1 loss: 0.1454 iter time (s): 1.310 samples/sec: 0.764\n",
      "loss:0.1453944593667984\n",
      "num_steps:1\n",
      "[2025-07-26 06:06:03,291] [INFO] [logging.py:107:log_dist] [Rank 0] step=2, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 2 loss: nan iter time (s): 0.741 samples/sec: 1.350\n",
      "loss:nan\n",
      "num_steps:2\n",
      "[2025-07-26 06:06:04,043] [INFO] [logging.py:107:log_dist] [Rank 0] step=3, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 3 loss: nan iter time (s): 0.739 samples/sec: 1.353\n",
      "loss:nan\n",
      "num_steps:3\n",
      "[2025-07-26 06:06:04,791] [INFO] [logging.py:107:log_dist] [Rank 0] step=4, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 4 loss: nan iter time (s): 0.738 samples/sec: 1.355\n",
      "loss:nan\n",
      "num_steps:4\n",
      "[2025-07-26 06:06:05,534] [INFO] [logging.py:107:log_dist] [Rank 0] step=5, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 5 loss: nan iter time (s): 0.732 samples/sec: 1.367\n",
      "loss:nan\n",
      "num_steps:5\n",
      "[2025-07-26 06:06:06,278] [INFO] [logging.py:107:log_dist] [Rank 0] step=6, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 6 loss: nan iter time (s): 0.737 samples/sec: 1.357\n",
      "loss:nan\n",
      "num_steps:6\n",
      "[2025-07-26 06:06:07,025] [INFO] [logging.py:107:log_dist] [Rank 0] step=7, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 7 loss: nan iter time (s): 0.733 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:7\n",
      "[2025-07-26 06:06:07,773] [INFO] [logging.py:107:log_dist] [Rank 0] step=8, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 8 loss: nan iter time (s): 0.735 samples/sec: 1.361\n",
      "loss:nan\n",
      "num_steps:8\n",
      "[2025-07-26 06:06:08,518] [INFO] [logging.py:107:log_dist] [Rank 0] step=9, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 9 loss: nan iter time (s): 0.737 samples/sec: 1.357\n",
      "loss:nan\n",
      "num_steps:9\n",
      "[2025-07-26 06:06:09,273] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 10 loss: nan iter time (s): 0.738 samples/sec: 1.355\n",
      "loss:nan\n",
      "num_steps:10\n",
      "[2025-07-26 06:06:10,014] [INFO] [logging.py:107:log_dist] [Rank 0] step=11, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 11 loss: nan iter time (s): 0.733 samples/sec: 1.364\n",
      "loss:nan\n",
      "num_steps:11\n",
      "[2025-07-26 06:06:10,767] [INFO] [logging.py:107:log_dist] [Rank 0] step=12, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 12 loss: nan iter time (s): 0.741 samples/sec: 1.350\n",
      "loss:nan\n",
      "num_steps:12\n",
      "[2025-07-26 06:06:11,541] [INFO] [logging.py:107:log_dist] [Rank 0] step=13, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 13 loss: nan iter time (s): 0.766 samples/sec: 1.305\n",
      "loss:nan\n",
      "num_steps:13\n",
      "[2025-07-26 06:06:12,308] [INFO] [logging.py:107:log_dist] [Rank 0] step=14, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 14 loss: nan iter time (s): 0.755 samples/sec: 1.324\n",
      "loss:nan\n",
      "num_steps:14\n",
      "[2025-07-26 06:06:13,086] [INFO] [logging.py:107:log_dist] [Rank 0] step=15, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 15 loss: nan iter time (s): 0.771 samples/sec: 1.297\n",
      "loss:nan\n",
      "num_steps:15\n",
      "[2025-07-26 06:06:13,837] [INFO] [logging.py:107:log_dist] [Rank 0] step=16, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 16 loss: nan iter time (s): 0.743 samples/sec: 1.346\n",
      "loss:nan\n",
      "num_steps:16\n",
      "[2025-07-26 06:06:14,614] [INFO] [logging.py:107:log_dist] [Rank 0] step=17, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 17 loss: nan iter time (s): 0.763 samples/sec: 1.310\n",
      "loss:nan\n",
      "num_steps:17\n",
      "[2025-07-26 06:06:15,356] [INFO] [logging.py:107:log_dist] [Rank 0] step=18, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 18 loss: nan iter time (s): 0.739 samples/sec: 1.353\n",
      "loss:nan\n",
      "num_steps:18\n",
      "[2025-07-26 06:06:16,098] [INFO] [logging.py:107:log_dist] [Rank 0] step=19, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 19 loss: nan iter time (s): 0.735 samples/sec: 1.361\n",
      "loss:nan\n",
      "num_steps:19\n",
      "[2025-07-26 06:06:16,842] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 20 loss: nan iter time (s): 0.735 samples/sec: 1.360\n",
      "loss:nan\n",
      "num_steps:20\n",
      "[2025-07-26 06:06:17,582] [INFO] [logging.py:107:log_dist] [Rank 0] step=21, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 21 loss: nan iter time (s): 0.733 samples/sec: 1.364\n",
      "loss:nan\n",
      "num_steps:21\n",
      "[2025-07-26 06:06:18,326] [INFO] [logging.py:107:log_dist] [Rank 0] step=22, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 22 loss: nan iter time (s): 0.735 samples/sec: 1.360\n",
      "loss:nan\n",
      "num_steps:22\n",
      "[2025-07-26 06:06:19,158] [INFO] [logging.py:107:log_dist] [Rank 0] step=23, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 23 loss: nan iter time (s): 0.815 samples/sec: 1.228\n",
      "loss:nan\n",
      "num_steps:23\n",
      "[2025-07-26 06:06:19,893] [INFO] [logging.py:107:log_dist] [Rank 0] step=24, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 24 loss: nan iter time (s): 0.734 samples/sec: 1.363\n",
      "loss:nan\n",
      "num_steps:24\n",
      "[2025-07-26 06:06:20,653] [INFO] [logging.py:107:log_dist] [Rank 0] step=25, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 25 loss: nan iter time (s): 0.746 samples/sec: 1.341\n",
      "loss:nan\n",
      "num_steps:25\n",
      "[2025-07-26 06:06:21,416] [INFO] [logging.py:107:log_dist] [Rank 0] step=26, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 26 loss: nan iter time (s): 0.755 samples/sec: 1.325\n",
      "loss:nan\n",
      "num_steps:26\n",
      "[2025-07-26 06:06:22,180] [INFO] [logging.py:107:log_dist] [Rank 0] step=27, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 27 loss: nan iter time (s): 0.757 samples/sec: 1.320\n",
      "loss:nan\n",
      "num_steps:27\n",
      "[2025-07-26 06:06:22,939] [INFO] [logging.py:107:log_dist] [Rank 0] step=28, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 28 loss: nan iter time (s): 0.747 samples/sec: 1.338\n",
      "loss:nan\n",
      "num_steps:28\n",
      "[2025-07-26 06:06:23,696] [INFO] [logging.py:107:log_dist] [Rank 0] step=29, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 29 loss: nan iter time (s): 0.751 samples/sec: 1.332\n",
      "loss:nan\n",
      "num_steps:29\n",
      "[2025-07-26 06:06:24,462] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 30 loss: nan iter time (s): 0.758 samples/sec: 1.319\n",
      "loss:nan\n",
      "num_steps:30\n",
      "[2025-07-26 06:06:25,227] [INFO] [logging.py:107:log_dist] [Rank 0] step=31, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 31 loss: nan iter time (s): 0.736 samples/sec: 1.359\n",
      "loss:nan\n",
      "num_steps:31\n",
      "[2025-07-26 06:06:25,971] [INFO] [logging.py:107:log_dist] [Rank 0] step=32, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 32 loss: nan iter time (s): 0.732 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:32\n",
      "[2025-07-26 06:06:26,709] [INFO] [logging.py:107:log_dist] [Rank 0] step=33, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 33 loss: nan iter time (s): 0.731 samples/sec: 1.368\n",
      "loss:nan\n",
      "num_steps:33\n",
      "[2025-07-26 06:06:27,450] [INFO] [logging.py:107:log_dist] [Rank 0] step=34, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 34 loss: nan iter time (s): 0.732 samples/sec: 1.366\n",
      "loss:nan\n",
      "num_steps:34\n",
      "[2025-07-26 06:06:28,187] [INFO] [logging.py:107:log_dist] [Rank 0] step=35, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 35 loss: nan iter time (s): 0.731 samples/sec: 1.367\n",
      "loss:nan\n",
      "num_steps:35\n",
      "[2025-07-26 06:06:28,930] [INFO] [logging.py:107:log_dist] [Rank 0] step=36, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 36 loss: nan iter time (s): 0.732 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:36\n",
      "[2025-07-26 06:06:29,673] [INFO] [logging.py:107:log_dist] [Rank 0] step=37, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 37 loss: nan iter time (s): 0.731 samples/sec: 1.368\n",
      "loss:nan\n",
      "num_steps:37\n",
      "[2025-07-26 06:06:30,413] [INFO] [logging.py:107:log_dist] [Rank 0] step=38, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 38 loss: nan iter time (s): 0.730 samples/sec: 1.369\n",
      "loss:nan\n",
      "num_steps:38\n",
      "[2025-07-26 06:06:31,152] [INFO] [logging.py:107:log_dist] [Rank 0] step=39, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 39 loss: nan iter time (s): 0.732 samples/sec: 1.366\n",
      "loss:nan\n",
      "num_steps:39\n",
      "[2025-07-26 06:06:31,895] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 40 loss: nan iter time (s): 0.732 samples/sec: 1.366\n",
      "loss:nan\n",
      "num_steps:40\n",
      "[2025-07-26 06:06:32,634] [INFO] [logging.py:107:log_dist] [Rank 0] step=41, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 41 loss: nan iter time (s): 0.733 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:41\n",
      "[2025-07-26 06:06:33,377] [INFO] [logging.py:107:log_dist] [Rank 0] step=42, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 42 loss: nan iter time (s): 0.732 samples/sec: 1.366\n",
      "loss:nan\n",
      "num_steps:42\n",
      "[2025-07-26 06:06:34,117] [INFO] [logging.py:107:log_dist] [Rank 0] step=43, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 43 loss: nan iter time (s): 0.733 samples/sec: 1.364\n",
      "loss:nan\n",
      "num_steps:43\n",
      "[2025-07-26 06:06:34,867] [INFO] [logging.py:107:log_dist] [Rank 0] step=44, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 44 loss: nan iter time (s): 0.733 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:44\n",
      "[2025-07-26 06:06:35,609] [INFO] [logging.py:107:log_dist] [Rank 0] step=45, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 45 loss: nan iter time (s): 0.733 samples/sec: 1.364\n",
      "loss:nan\n",
      "num_steps:45\n",
      "[2025-07-26 06:06:36,353] [INFO] [logging.py:107:log_dist] [Rank 0] step=46, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 46 loss: nan iter time (s): 0.734 samples/sec: 1.363\n",
      "loss:nan\n",
      "num_steps:46\n",
      "[2025-07-26 06:06:37,093] [INFO] [logging.py:107:log_dist] [Rank 0] step=47, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 47 loss: nan iter time (s): 0.733 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:47\n",
      "[2025-07-26 06:06:37,837] [INFO] [logging.py:107:log_dist] [Rank 0] step=48, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 48 loss: nan iter time (s): 0.733 samples/sec: 1.365\n",
      "loss:nan\n",
      "num_steps:48\n",
      "[2025-07-26 06:06:38,602] [INFO] [logging.py:107:log_dist] [Rank 0] step=49, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 49 loss: nan iter time (s): 0.759 samples/sec: 1.317\n",
      "loss:nan\n",
      "num_steps:49\n",
      "Started new epoch: 2\n",
      "[2025-07-26 06:06:39,523] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 50 loss: nan iter time (s): 0.792 samples/sec: 1.262\n",
      "loss:nan\n",
      "num_steps:1\n",
      "[2025-07-26 06:06:40,281] [INFO] [logging.py:107:log_dist] [Rank 0] step=51, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 51 loss: nan iter time (s): 0.743 samples/sec: 1.346\n",
      "loss:nan\n",
      "num_steps:2\n",
      "[2025-07-26 06:06:41,039] [INFO] [logging.py:107:log_dist] [Rank 0] step=52, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 52 loss: nan iter time (s): 0.752 samples/sec: 1.329\n",
      "loss:nan\n",
      "num_steps:3\n",
      "[2025-07-26 06:06:41,797] [INFO] [logging.py:107:log_dist] [Rank 0] step=53, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 53 loss: nan iter time (s): 0.751 samples/sec: 1.332\n",
      "loss:nan\n",
      "num_steps:4\n",
      "[2025-07-26 06:06:42,541] [INFO] [logging.py:107:log_dist] [Rank 0] step=54, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 54 loss: nan iter time (s): 0.734 samples/sec: 1.362\n",
      "loss:nan\n",
      "num_steps:5\n",
      "[2025-07-26 06:06:43,284] [INFO] [logging.py:107:log_dist] [Rank 0] step=55, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 55 loss: nan iter time (s): 0.736 samples/sec: 1.359\n",
      "loss:nan\n",
      "num_steps:6\n",
      "[2025-07-26 06:06:44,033] [INFO] [logging.py:107:log_dist] [Rank 0] step=56, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 56 loss: nan iter time (s): 0.737 samples/sec: 1.356\n",
      "loss:nan\n",
      "num_steps:7\n",
      "[2025-07-26 06:06:44,780] [INFO] [logging.py:107:log_dist] [Rank 0] step=57, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 57 loss: nan iter time (s): 0.738 samples/sec: 1.354\n",
      "loss:nan\n",
      "num_steps:8\n",
      "[2025-07-26 06:06:45,528] [INFO] [logging.py:107:log_dist] [Rank 0] step=58, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 58 loss: nan iter time (s): 0.738 samples/sec: 1.354\n",
      "loss:nan\n",
      "num_steps:9\n",
      "[2025-07-26 06:06:46,275] [INFO] [logging.py:107:log_dist] [Rank 0] step=59, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 59 loss: nan iter time (s): 0.739 samples/sec: 1.354\n",
      "loss:nan\n",
      "num_steps:10\n",
      "[2025-07-26 06:06:47,025] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 60 loss: nan iter time (s): 0.739 samples/sec: 1.352\n",
      "loss:nan\n",
      "num_steps:11\n",
      "[2025-07-26 06:06:47,774] [INFO] [logging.py:107:log_dist] [Rank 0] step=61, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 61 loss: nan iter time (s): 0.739 samples/sec: 1.354\n",
      "loss:nan\n",
      "num_steps:12\n",
      "[2025-07-26 06:06:48,526] [INFO] [logging.py:107:log_dist] [Rank 0] step=62, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 62 loss: nan iter time (s): 0.740 samples/sec: 1.351\n",
      "loss:nan\n",
      "num_steps:13\n",
      "[2025-07-26 06:06:49,275] [INFO] [logging.py:107:log_dist] [Rank 0] step=63, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 63 loss: nan iter time (s): 0.739 samples/sec: 1.353\n",
      "loss:nan\n",
      "num_steps:14\n",
      "[2025-07-26 06:06:50,026] [INFO] [logging.py:107:log_dist] [Rank 0] step=64, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 64 loss: nan iter time (s): 0.741 samples/sec: 1.350\n",
      "loss:nan\n",
      "num_steps:15\n",
      "[2025-07-26 06:06:50,777] [INFO] [logging.py:107:log_dist] [Rank 0] step=65, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 65 loss: nan iter time (s): 0.741 samples/sec: 1.350\n",
      "loss:nan\n",
      "num_steps:16\n",
      "[2025-07-26 06:06:51,526] [INFO] [logging.py:107:log_dist] [Rank 0] step=66, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 66 loss: nan iter time (s): 0.740 samples/sec: 1.352\n",
      "loss:nan\n",
      "num_steps:17\n",
      "[2025-07-26 06:06:52,307] [INFO] [logging.py:107:log_dist] [Rank 0] step=67, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 67 loss: nan iter time (s): 0.767 samples/sec: 1.304\n",
      "loss:nan\n",
      "num_steps:18\n",
      "[2025-07-26 06:06:53,067] [INFO] [logging.py:107:log_dist] [Rank 0] step=68, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 68 loss: nan iter time (s): 0.745 samples/sec: 1.343\n",
      "loss:nan\n",
      "num_steps:19\n",
      "[2025-07-26 06:06:53,822] [INFO] [logging.py:107:log_dist] [Rank 0] step=69, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 69 loss: nan iter time (s): 0.745 samples/sec: 1.342\n",
      "loss:nan\n",
      "num_steps:20\n",
      "[2025-07-26 06:06:54,611] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 70 loss: nan iter time (s): 0.764 samples/sec: 1.310\n",
      "loss:nan\n",
      "num_steps:21\n",
      "[2025-07-26 06:06:55,382] [INFO] [logging.py:107:log_dist] [Rank 0] step=71, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 71 loss: nan iter time (s): 0.763 samples/sec: 1.311\n",
      "loss:nan\n",
      "num_steps:22\n",
      "[2025-07-26 06:06:56,146] [INFO] [logging.py:107:log_dist] [Rank 0] step=72, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 72 loss: nan iter time (s): 0.750 samples/sec: 1.333\n",
      "loss:nan\n",
      "num_steps:23\n",
      "[2025-07-26 06:06:56,909] [INFO] [logging.py:107:log_dist] [Rank 0] step=73, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 73 loss: nan iter time (s): 0.759 samples/sec: 1.318\n",
      "loss:nan\n",
      "num_steps:24\n",
      "[2025-07-26 06:06:57,690] [INFO] [logging.py:107:log_dist] [Rank 0] step=74, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 74 loss: nan iter time (s): 0.770 samples/sec: 1.298\n",
      "loss:nan\n",
      "num_steps:25\n",
      "[2025-07-26 06:06:58,454] [INFO] [logging.py:107:log_dist] [Rank 0] step=75, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 75 loss: nan iter time (s): 0.752 samples/sec: 1.330\n",
      "loss:nan\n",
      "num_steps:26\n",
      "[2025-07-26 06:06:59,206] [INFO] [logging.py:107:log_dist] [Rank 0] step=76, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 76 loss: nan iter time (s): 0.740 samples/sec: 1.352\n",
      "loss:nan\n",
      "num_steps:27\n",
      "[2025-07-26 06:06:59,959] [INFO] [logging.py:107:log_dist] [Rank 0] step=77, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 77 loss: nan iter time (s): 0.744 samples/sec: 1.345\n",
      "loss:nan\n",
      "num_steps:28\n",
      "[2025-07-26 06:07:00,710] [INFO] [logging.py:107:log_dist] [Rank 0] step=78, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 78 loss: nan iter time (s): 0.741 samples/sec: 1.349\n",
      "loss:nan\n",
      "num_steps:29\n",
      "[2025-07-26 06:07:01,473] [INFO] [logging.py:107:log_dist] [Rank 0] step=79, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 79 loss: nan iter time (s): 0.751 samples/sec: 1.331\n",
      "loss:nan\n",
      "num_steps:30\n",
      "[2025-07-26 06:07:02,241] [INFO] [logging.py:107:log_dist] [Rank 0] step=80, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 80 loss: nan iter time (s): 0.760 samples/sec: 1.316\n",
      "loss:nan\n",
      "num_steps:31\n",
      "[2025-07-26 06:07:03,013] [INFO] [logging.py:107:log_dist] [Rank 0] step=81, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 81 loss: nan iter time (s): 0.765 samples/sec: 1.307\n",
      "loss:nan\n",
      "num_steps:32\n",
      "[2025-07-26 06:07:03,793] [INFO] [logging.py:107:log_dist] [Rank 0] step=82, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 82 loss: nan iter time (s): 0.770 samples/sec: 1.298\n",
      "loss:nan\n",
      "num_steps:33\n",
      "[2025-07-26 06:07:04,569] [INFO] [logging.py:107:log_dist] [Rank 0] step=83, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 83 loss: nan iter time (s): 0.768 samples/sec: 1.302\n",
      "loss:nan\n",
      "num_steps:34\n",
      "[2025-07-26 06:07:05,352] [INFO] [logging.py:107:log_dist] [Rank 0] step=84, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 84 loss: nan iter time (s): 0.767 samples/sec: 1.303\n",
      "loss:nan\n",
      "num_steps:35\n",
      "[2025-07-26 06:07:06,099] [INFO] [logging.py:107:log_dist] [Rank 0] step=85, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 85 loss: nan iter time (s): 0.744 samples/sec: 1.345\n",
      "loss:nan\n",
      "num_steps:36\n",
      "[2025-07-26 06:07:06,852] [INFO] [logging.py:107:log_dist] [Rank 0] step=86, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 86 loss: nan iter time (s): 0.743 samples/sec: 1.345\n",
      "loss:nan\n",
      "num_steps:37\n",
      "[2025-07-26 06:07:07,604] [INFO] [logging.py:107:log_dist] [Rank 0] step=87, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 87 loss: nan iter time (s): 0.744 samples/sec: 1.344\n",
      "loss:nan\n",
      "num_steps:38\n",
      "[2025-07-26 06:07:08,359] [INFO] [logging.py:107:log_dist] [Rank 0] step=88, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 88 loss: nan iter time (s): 0.744 samples/sec: 1.344\n",
      "loss:nan\n",
      "num_steps:39\n",
      "[2025-07-26 06:07:09,122] [INFO] [logging.py:107:log_dist] [Rank 0] step=89, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 89 loss: nan iter time (s): 0.751 samples/sec: 1.331\n",
      "loss:nan\n",
      "num_steps:40\n",
      "[2025-07-26 06:07:09,900] [INFO] [logging.py:107:log_dist] [Rank 0] step=90, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 90 loss: nan iter time (s): 0.763 samples/sec: 1.311\n",
      "loss:nan\n",
      "num_steps:41\n",
      "[2025-07-26 06:07:10,676] [INFO] [logging.py:107:log_dist] [Rank 0] step=91, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 91 loss: nan iter time (s): 0.763 samples/sec: 1.310\n",
      "loss:nan\n",
      "num_steps:42\n",
      "[2025-07-26 06:07:11,439] [INFO] [logging.py:107:log_dist] [Rank 0] step=92, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 92 loss: nan iter time (s): 0.754 samples/sec: 1.327\n",
      "loss:nan\n",
      "num_steps:43\n",
      "[2025-07-26 06:07:12,215] [INFO] [logging.py:107:log_dist] [Rank 0] step=93, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 93 loss: nan iter time (s): 0.772 samples/sec: 1.296\n",
      "loss:nan\n",
      "num_steps:44\n",
      "[2025-07-26 06:07:12,991] [INFO] [logging.py:107:log_dist] [Rank 0] step=94, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 94 loss: nan iter time (s): 0.766 samples/sec: 1.306\n",
      "loss:nan\n",
      "num_steps:45\n",
      "[2025-07-26 06:07:13,758] [INFO] [logging.py:107:log_dist] [Rank 0] step=95, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 95 loss: nan iter time (s): 0.756 samples/sec: 1.323\n",
      "loss:nan\n",
      "num_steps:46\n",
      "[2025-07-26 06:07:14,527] [INFO] [logging.py:107:log_dist] [Rank 0] step=96, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 96 loss: nan iter time (s): 0.763 samples/sec: 1.311\n",
      "loss:nan\n",
      "num_steps:47\n",
      "[2025-07-26 06:07:15,280] [INFO] [logging.py:107:log_dist] [Rank 0] step=97, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 97 loss: nan iter time (s): 0.744 samples/sec: 1.343\n",
      "loss:nan\n",
      "num_steps:48\n",
      "[2025-07-26 06:07:16,038] [INFO] [logging.py:107:log_dist] [Rank 0] step=98, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 98 loss: nan iter time (s): 0.751 samples/sec: 1.331\n",
      "loss:nan\n",
      "num_steps:49\n",
      "Started new epoch: 3\n",
      "[2025-07-26 06:07:16,924] [INFO] [logging.py:107:log_dist] [Rank 0] step=99, skipped=0, lr=[1e-07], mom=[[0.9, 0.99]]\n",
      "steps: 99 loss: nan iter time (s): 0.779 samples/sec: 1.283\n",
      "loss:nan\n",
      "num_steps:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: this is state we need to save and resume when resuming from checkpoint. It only affects logging.\n",
    "epoch_loss = 0\n",
    "num_steps = 0\n",
    "while True:\n",
    "    #empty_cuda_cache()\n",
    "    model_engine.reset_activation_shape()\n",
    "    iterator = get_data_iterator_for_step(train_dataloader, model_engine)\n",
    "    loss = model_engine.train_batch(iterator).item()\n",
    "    print(f'loss:{loss}')\n",
    "    epoch_loss += loss\n",
    "    num_steps += 1\n",
    "    train_dataloader.sync_epoch()\n",
    "    print(f'num_steps:{num_steps}')\n",
    "    new_epoch, checkpointed, saved = saver.process_epoch(epoch, step)\n",
    "    finished_epoch = True if new_epoch != epoch else False\n",
    "\n",
    "    if is_main_process() and step % config['logging_steps'] == 0:\n",
    "        tb_writer.add_scalar(f'train/loss', loss, step)\n",
    "        if wandb_enable:\n",
    "            wandb.log({'train/loss': loss, 'step': step})\n",
    "        if optimizer.__class__.__name__ == 'Prodigy':\n",
    "            prodigy_d = get_prodigy_d(optimizer)\n",
    "            tb_writer.add_scalar(f'train/prodigy_d', prodigy_d, step)\n",
    "        if optimizer.__class__.__name__ == 'Automagic':\n",
    "            lrs, avg_lr = _get_automagic_lrs(optimizer)\n",
    "            tb_writer.add_histogram(f'train/automagic_lrs', lrs, step)\n",
    "            tb_writer.add_scalar(f'train/automagic_avg_lr', avg_lr, step)\n",
    "\n",
    "    if (config['eval_every_n_steps'] and step % config['eval_every_n_steps'] == 0) or (finished_epoch and config['eval_every_n_epochs'] and epoch % config['eval_every_n_epochs'] == 0):\n",
    "        evaluate(model, model_engine, eval_dataloaders, tb_writer, step, config['eval_gradient_accumulation_steps'], disable_block_swap_for_eval)\n",
    "\n",
    "    if finished_epoch:\n",
    "        if is_main_process():\n",
    "            tb_writer.add_scalar(f'train/epoch_loss', epoch_loss/num_steps, epoch)\n",
    "            if wandb_enable:\n",
    "                wandb.log({'train/epoch_loss': epoch_loss/num_steps, 'epoch': epoch})\n",
    "        epoch_loss = 0\n",
    "        num_steps = 0\n",
    "        epoch = new_epoch\n",
    "        if epoch is None:\n",
    "            break\n",
    "\n",
    "    saver.process_step(step)\n",
    "    step += 1\n",
    "    if step == 100: #수동\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save final training state checkpoint and model, unless we just saved them.\n",
    "if not checkpointed:\n",
    "    saver.save_checkpoint(step)\n",
    "if not saved:\n",
    "    saver.save_model(f'epoch{epoch}',state_dict)\n",
    "\n",
    "if is_main_process():\n",
    "    print('TRAINING COMPLETE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 06:07:17,019] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step99 is begin to save!\n",
      "[2025-07-26 06:07:24,859] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /workspace/processed_data/video_1753509189/lora/20250726_05-55-47/global_step99/mp_rank_00_model_states.pt\n"
     ]
    }
   ],
   "source": [
    "if not checkpointed:\n",
    "    saver.save_checkpoint(step)\n",
    "if not saved:\n",
    "    saver.save_full_model(f'epoch{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart\n"
     ]
    }
   ],
   "source": [
    "print('restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import dataset as dataset_util\n",
    "with open(config['dataset']) as f:\n",
    "    dataset_config = toml.load(f)\n",
    "radient_release = config['optimizer'].get('gradient_release', False)\n",
    "ds_config = {\n",
    "    'train_micro_batch_size_per_gpu': config.get('micro_batch_size_per_gpu', 1),\n",
    "    'gradient_accumulation_steps': config.get('gradient_accumulation_steps', 1),\n",
    "    # Can't do gradient clipping with gradient release, since there are no grads at the end of the step anymore.\n",
    "    # 'gradient_clipping': 0. if gradient_release else config.get('gradient_clipping', 1.0),\n",
    "    'steps_per_print': config.get('steps_per_print', 1),\n",
    "    \"bfloat16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "}\n",
    "caching_batch_size = config.get('caching_batch_size', 1)\n",
    "dataset_manager = dataset_util.DatasetManager(model, regenerate_cache=regenerate_cache, caching_batch_size=caching_batch_size)\n",
    "train_data = dataset_util.Dataset(dataset_config, model, skip_dataset_validation=args.i_know_what_i_am_doing)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineEngine(\n",
       "  (module): ManualPipelineModule(\n",
       "    (tied_modules): ModuleDict()\n",
       "    (0): InitialLayer(\n",
       "      (patch_embedding): Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "      (time_embedding): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (text_embedding): Sequential(\n",
       "        (0): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (1): GELU(approximate='tanh')\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (time_projection): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (24): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (25): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (26): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (27): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (28): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (29): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (30): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (31): FinalLayer(\n",
       "      (head): Head(\n",
       "        (norm): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (head): Linear(in_features=1536, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PipelineEngine.train_batch of PipelineEngine(\n",
       "  (module): ManualPipelineModule(\n",
       "    (tied_modules): ModuleDict()\n",
       "    (0): InitialLayer(\n",
       "      (patch_embedding): Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "      (time_embedding): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1536, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (text_embedding): Sequential(\n",
       "        (0): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (1): GELU(approximate='tanh')\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (time_projection): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (24): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (25): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (26): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (27): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (28): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (29): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (30): TransformerLayer(\n",
       "      (block): WanAttentionBlock(\n",
       "        (norm1): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (self_attn): WanSelfAttention(\n",
       "          (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (norm_q): WanRMSNorm()\n",
       "          (norm_k): WanRMSNorm()\n",
       "        )\n",
       "        (norm3): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "        (cross_attn): LoRACrossAttention(\n",
       "          (base): WanT2VCrossAttention(\n",
       "            (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (norm_q): WanRMSNorm()\n",
       "            (norm_k): WanRMSNorm()\n",
       "          )\n",
       "          (to_q_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_k_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_v_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "          (to_out_lora): LoRALinearLayer(\n",
       "            (down): Linear(in_features=1536, out_features=4, bias=False)\n",
       "            (up): Linear(in_features=4, out_features=1536, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (31): FinalLayer(\n",
       "      (head): Head(\n",
       "        (norm): WanLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "        (head): Linear(in_features=1536, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_engine.train_batch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
